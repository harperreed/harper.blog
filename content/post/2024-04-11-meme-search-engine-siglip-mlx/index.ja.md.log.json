{
  "input_file": "index.md",
  "output_file": "index.ja.md",
  "target_language": "japanese",
  "language_code": "ja",
  "model": "o3",
  "skip_edit": false,
  "do_critique": true,
  "critique_loops": 4,
  "has_frontmatter": true,
  "translation_context": "a blog post about vector search and mmemes",
  "token_usage": {
    "prompt_tokens": 83668,
    "completion_tokens": 27230,
    "total_tokens": 110898
  },
  "cost": "$1.9259",
  "prompts_and_responses": {
    "translation": {
      "model": "o3",
      "target_language": "japanese",
      "system_prompt": "\n        1. Read the provided text carefully, preserving all formatting, markdown, and structure exactly as they appear.\n        2. Identify any block quotes and code blocks.\n        3. Do not translate text in block quotes or in code blocks (including text within code blocks).\n        4. Translate everything else into japanese.\n        5. Maintain the original formatting, markdown, and structure in your output.\n        6. Provide a natural-sounding translation rather than a word-for-word one.\n        7. For idioms, colloquialisms, or slang, render them in an equivalent, natural way in japanese whenever possible.\n        8. If there isn't a direct or natural translation for a particular term or phrase, keep it in the original language and surround it with quotes if necessary.\n        9. Ensure that technical terms or jargon remain accurate; if there's no suitable translation, keep the original term.\n        10. Strive for fluid, native-sounding prose that retains the tone and intent of the original text.\n        11. If context about the text is provided, use it to inform your translation choices, especially regarding tone, style, and cultural adaptations.\n        ",
      "user_prompt": "# CONTEXT\na blog post about vector search and mmemes\n\n# TEXT TO TRANSLATE\n## Or: how to learn about clip/siglip and vector encoding images\n\n_tl;dr_: I built a meme search engine using siglip/CLIP and vector encoding images. It was fun and I learned a lot.\n\nI have been building a lot of applied AI tools for a while. One of the components that always seemed the most magical has always been vector embeddings. [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) and the like have straight blown my mind. It is like magic.\n\nI saw a [simple app on hacker news](https://news.ycombinator.com/item?id=39392582) that was [super impressive](https://mood-amber.vercel.app/). Someone crawled a bunch of Tumblr images and used [siglip](https://arxiv.org/abs/2303.15343) to get the embeddings and then made a simple \"click the image and see similar images\" app. It was like magic. I had no idea how to achieve this, but it seemed accessible.\n\nI decided to use my sudden motivation as an opportunity to learn how \"all this works.\"\n\n## wut\n\nIf you have never ran into vector embeddings, clip/siglip, vector databases, and the like - never fear.\n\nBefore I saw the hack on hn I really didn’t think much about vector embeddings, multi modal embeddings or vector datastores. I had used faiss (facebooks simple vector store), and Pinecone ($$) for some hacks, but didn’t really dig in. Just got it to work and then was like “yep. Tests pass.”\n\nI still barely know what vectors are. Lol. Before I dug in and built this, I really didn’t understand how I would use it outside of RAG or another LLM process.\n\nI learn by building. It helps if the results are really intriguing, and in this case kind of magical.\n\n### WTF terms\n\nI had a few friends read this over before publishing and a couple were like “wtf is X?” Here is a short list of terms that were largely new to me:\n\n- **Vector Embeddings** - Vector embeddings convert your text of images into numerical representations, allowing you to find similar pics and search your library effectively.\n- **Vector Database** - A vector database is a way to store and search through encoded items, enabling you to find similar items.\n- **Word2Vec** - Word2Vec is a groundbreaking technique that converts words into numerical vectors, enabling you to perform tasks like finding similar words and exploring relationships between them.\n- **CLIP** - CLIP is OpenAI's model that encodes images and text into numerical vectors.\n- **OpenCLIP** - OpenCLIP is an open-source implementation of OpenAI's CLIP model, allowing anyone to use and build upon this powerful image and text encoding technology without the need for special access or permissions.\n- **FAISS** - FAISS is an efficient library for managing and searching through large collections of image vectors, making it fast and easy to find the images you're looking for.\n- **ChromaDB** - ChromaDB is a database that stores and retrieves your image and text vectors, quickly returning similar results for your searches.\n\n## Keep it simple, harper.\n\nThis is a pretty straight forward hack. I am just fucking around so I wasn't super interested in making it scalable. I did have an interest in making it replicable. I wanted to make something that **you** could run without a lot of work.\n\nOne of my goals was to make sure everything runs locally to my laptop. We have these fancy Mac GPUs - let's heat them up.\n\nThe first step was building out a simple crawler that would crawl a directory of images. I use Apple Photos, so I didn't have a directory full of my photos laying around. I did, however, have a giant bucket of memes from my precious and very secret meme chat group (don't tell anyone). I exported the chat, moved the images to a directory and BAM - I had my test image set.\n\n### The Crawler\n\nI created the world's worst crawler. Well. I should be honest: Claude created the world's worst crawler with my instructions.\n\nIt is a bit complicated but here are the steps:\n\n1. It gets the file list of the target directory\n2. It stores the list in a msgpack file\n3. I reference the msgpack file and then iterate through every image and store it in a sqlite db. Grabing some metadata about the file\n    - hash\n    - filesize\n    - location\n4. I iterate through that sqlite db and then use CLIP to get the vector encoding of every image.\n5. Then I store those vectors back in the sqlite db\n6. Then I iterate through the sqlite db and insert the vectors and image path into chroma vector db\n7. Then we are done\n\nThis is a lot of wasted work. You could iterate through the images, grab the embeddings and slam it into chroma (I chose chroma cuz it is easy, free, and no infra).\n\nI have built it this way because:\n\n- After the memes, I crawled 140k images and wanted it to be resilient to crashing.\n- I needed it to be able to resume building out the databases in case it crashed, power went out, etc\n- I really like loops\n\nRegardless of the extra complexity, it worked flawlessly. I have crawled over 200k images without a blip.\n\n### An embedding system\n\nEncoding the images was fun.\n\nI started with siglip and created a [simple web service](https://github.com/harperreed/imbedding) where we could upload the image and get the vectors back. This ran on one of our GPU boxes at the studio and worked well. It wasn't fast, but it was way faster than running [open clip](https://github.com/mlfoundations/open_clip) locally.\n\nI still wanted to run it locally. I remembered that the [ml-explore](https://github.com/ml-explore/) repo from apple had some neat examples that could help. And BAM they had a [clip implementation](https://github.com/ml-explore/mlx-examples/tree/main/clip) that was fast af. Even using the larger model, it was faster than the 4090. Wildstyle.\n\nI just needed to make it easy to use in my script.\n\n### MLX_CLIP\n\nClaude and I were able to coerce the example script from apple into a fun lil python class that you can use locally on any of your machines. It will download the models if they don't exist, convert them, and then use them in flight with your script.\n\nYou can check it out here: https://github.com/harperreed/mlx_clip\n\nI am pretty chuffed with how well it turned out. I know most people know this, but the apple silicon is fast af.\n\nIt turned out to be rather simple to use:\n\n```python\nimport mlx_clip\n\n# Initialize the mlx_clip model with the given model name.\nclip = mlx_clip.mlx_clip(\"openai/clip-vit-base-patch32\")\n\n# Encode the image from the specified file path and obtain the image embeddings.\nimage_embeddings = clip.image_encoder(\"assets/cat.jpeg\")\n# Print the image embeddings to the console.\nprint(image_embeddings)\n\n# Encode the text description and obtain the text embeddings.\ntext_embeddings = clip.text_encoder(\"a photo of a cat\")\n# Print the text embeddings to the console.\nprint(text_embeddings)\n```\n\nI would love to get this to work with siglip, as I prefer that model (it is way better than CLIP). However, this is a POC more than a product I want to maintain. If anyone has any hints on how to get it working with siglip - [hmu](mailto:harper@modest.com). I don't want to reinvent open clip - which should theoretically run well on apple silicon, and is very good.\n\n### Now what\n\nNow that we had all the image vectors slammed into the vector datastore we could get started with the interface. I used the built in query functionality of chromadb to show similar images.\n\nGrab the vectors of the image you are starting with. Query those vectors with chromadb. Chromed returns a list of image ids that are similar in declining similarity.\n\nI then wrapped it all up in a tailwind/flask app.\nThis was incredible.\n\nI can't imagine the amount of work we would have done in 2015 to build this. I spent maybe 10 hours total on this and it was trivial.\n\nThe results are akin to magic.\n\n### Memes concept search\n\nNow remember, I used memes as my initial set of images. I had 12000 memes to search through.\n\nStart with this:\n\n{{< image src=\"images/posts/vector-memes-bowie.png\" caption=\"So true\" >}}\n\nEncode it, pass it to chroma to return similar results.\n\nAnd then similar images that return are like this:\n{{< image src=\"images/posts/vector-memes-bowie-results.png\" >}}\n\nAnother example:\n{{< image src=\"images/posts/vector-memes-star-trek.png\" >}}\n\nGives you results like:\n{{< image src=\"images/posts/vector-memes-star-trek-results.png\" >}}\n\nIt is really fun to click around.\n\n### Namespaces?\n\nThe magic isn't clicking on an image and getting a similar image. That is cool, but wasn't \"holy shit\" for me.\n\nWhat blew my mind was using the same model to encode the search text into vectors and finding images similar to the text.\n\nFor whatever reason, this screws up my brain. It is one thing to have a neat semantic like search for images based on another images. Being able to have a nice multi modal interface really made it like a magic trick.\n\nHere are some examples:\n\nSearching for **money**. I grab the encoding for money and pass the vectors to chroma. The results for **money** are:\n{{< image src=\"images/posts/vector-memes-money.png\" >}}\n\nSearching for **AI**\n{{< image src=\"images/posts/vector-memes-ai.png\" >}}\n\nSearching for **red** (a dozy! Is it a color? Is a lifestyle? Is it Russia?)\n{{< image src=\"images/posts/vector-memes-red.png\" >}}\n\nSo on and so forth. Forever. It is magical. You can find all sorts of gems you forgot about. Oh shit I need a meme about writing a blog post:\n{{< image src=\"images/posts/vector-memes-writing-meme.jpg\" >}}\n\n(I am self aware, I just don't care - lol)\n\n### How does it work with a photo library?\n\nIt works super well.\n\nI highly recommend running this against your photo library. To get started, I downloaded my google photos takeout archive. Extracted it onto an external disk. I had to run a few scripts against it to make it usable (Whoever designed the google photos takeout is very excited about duplicate data). I then pointed the script at that directory instead of my memes folder and let 'er rip.\n\nI had about 140k photos and it took about 6 hours to run through. Not so bad. The results are incredible.\n\n#### Here are some fun examples:\n\nObviously these are similar (I also have a dupe problem in google photos)\n{{< image src=\"images/posts/vector-memes-harper.png\" >}}\n\nWe have had a lot of poodles. Here are some\n{{< image src=\"images/posts/vector-memes-poodles.png\" >}}\n\nYou can search for landmarks. I had no idea I had taken a photo of fuji-san from a plane!\n{{< image src=\"images/posts/vector-memes-fuji-results.png\" >}}\n\nAnd then find similar images of Mt Fuji.\n{{< image src=\"images/posts/vector-memes-fuji-similar.png\" >}}\n\nIt is pretty easy to search for places.\n{{< image src=\"images/posts/vector-memes-chicago.png\" >}}\n\nOr emotions. I am apparently surprising so I have a lot of surprised photos.\n{{< image src=\"images/posts/vector-memes-surprised.png\" >}}\n\nAlso niche things like low riders. (These are from Shibuya!)\n{{< image src=\"images/posts/vector-memes-low-riders.png\" >}}\n\nAnd you can use it to find things that are not easy to find or search for. Like bokeh.\n{{< image src=\"images/posts/vector-memes-bokeh.png\" >}}\n\nIt's wonderful, because I can click through and find great images I had forgotten about. Like this great photo of Baratunde that I took in 2017:\n\n{{< image src=\"images/posts/vector-memes-baratunde.png\" >}}\n\n### This will be everywhere\n\nI imagine that we will see this tech rolled into all the various photo apps shortly. Google Photos probably already does this, but they have googled it so much that nobody notices.\n\nThis is too good to not roll into whatever photo app you use. If I had any large scale product that used photos or images, I would immediately set up a pipeline to start encoding the images to see what kind of weird features this unlocks.\n\n## YOU CAN USE THIS FOR THE LOW PRICE OF FREE\n\nI put the source here: [harperreed/photo-similarity-search](https://github.com/harperreed/photo-similarity-search).\n\nPlease check it out.\n\nIt is pretty straight forward to get going. It is a bit hacky. lol.\n\nI would use conda or something similar to keep things clean. The interface is simple tailwind. The web is flask. The code is python. I am your host, harper reed.\n\n## My challenge for you!\n\nPlease build an app that I can use to catalog my photo library in a nice way. I don't want to upload to another destination. I want to have a simple Mac app that I can point to my photo library and say \"crawl this.\" I imagine a lot of neat stuff could be added:\n\n- Llava/Moondream auto captioning\n- Keywords / Tags\n- Vector similarity\n- etc\n\nIt should run locally. Be a native app. Be simple, and effective. Maybe plug into Lightroom, capture one, or apple photos.\n\nI want this. Build it. Let's discover all the amazing photos we have taken through the magic of AI.\n\n## Extra Credit: Light Room Preview JPEG Recovery\n\nMy hacking buddy Ivan was around while I was building this. He immediately saw the magic of what a person could discover by using this on their photo library. He wanted to use it immediately.\n\nHis photo catalog is on an external hard drive - but he had his Lightroom preview file locally. He wrote a quick script to extract the thumbnails and metadata from the preview file and save it to an external disk.\n\nWe then ran the image vector crawler and BAM - he could see similar images and what not. Worked perfectly.\n\n#### Recover your Lightroom photos. Or at least the thumbnails.\n\nIvan's simple script to extract the images from the preview file is really awesome. If you have ever lost your real photo library (corrupt harddrive, or whatever) and you still have the lrpreview file - this script can help you extract at least the lower res version.\n\nA super handy script to keep around.\n\nYou can check it out here: [LR Preview JPEG Extractor](https://github.com/ibips/lrprev-extract).\n\n## Thanks for reading.\n\nAs always, [hmu](mailto:harper@modest.com) and let's hang out. I am thinking a lot about AI, Ecommerce, photos, hifi, hacking and other shit.\n\nIf you are in Chicago come hang out.",
      "context": "a blog post about vector search and mmemes",
      "response": "## つまり：clip/siglip と画像ベクトルエンコードを学ぶ方法\n\n_tl;dr_: siglip/CLIP を使ってミーム検索エンジンを作った。めちゃくちゃ楽しくて学びも多かった。\n\nしばらく前から、いろいろな実用系 AI ツールを作っている。その中でも “魔法” っぽさが際立っているのがベクトル埋め込みだ。[Word2Vec](https://en.wikipedia.org/wiki/Word2vec) なんかは本当に衝撃だった。まさに魔法。\n\n[Hacker News で見かけたシンプルなアプリ](https://news.ycombinator.com/item?id=39392582)が[かなりスゴかった](https://mood-amber.vercel.app/)。誰かが Tumblr の画像を大量にクロールし、[siglip](https://arxiv.org/abs/2303.15343) で埋め込みを取り、「画像をクリックすると類似画像が出てくる」アプリを作っていた。魔法みたいで、しかも手が届きそう。\n\nその勢いのまま、「これ全部どう動いてるんだろう？」を理解するチャンスにすることにした。\n\n## wut\n\nベクトル埋め込みや clip/siglip、ベクトル DB などに触れたことがなくても大丈夫。  \nHN であのハックを見るまでは、マルチモーダル埋め込みやベクトルストアについて深く考えたことがなかった。faiss（Facebook 製のシンプルなベクトルストア）や Pinecone（$$）を使ったことはあったけど、動けば OK 程度。テストが通れば「はい終了」。\n\n正直ベクトルが何かなんて今でもほとんど分かってない。笑  \nこれを作る前は、RAG や他の LLM 系プロセス以外でどう使うか想像も付かなかった。\n\n自分は「作って覚える」タイプ。結果が面白く、しかも今回はちょっとした“魔法”だったのでやる気も倍増。\n\n### WTF 用語集\n\n公開前に何人か友人に読んでもらったら「X って何？」と言われたので、当時ほぼ初見だった用語をざっくりまとめる。\n\n- **Vector Embeddings** - テキストや画像を数値ベクトルに変換し、類似画像検索などを可能にする。\n- **Vector Database** - エンコード済みのアイテムを保存・検索する DB。似ているものを素早く見つけられる。\n- **Word2Vec** - 単語を数値ベクトルに変換する画期的な手法。類似語や単語間の関係性を探索できる。\n- **CLIP** - OpenAI が公開した、画像とテキストを同じベクトル空間にエンコードするモデル。\n- **OpenCLIP** - OpenAI の CLIP を OSS 実装したもの。誰でも自由に使える。\n- **FAISS** - 大量の画像ベクトルを高速に検索・管理できるライブラリ。\n- **ChromaDB** - 画像やテキストのベクトルを保存し、類似検索を高速に返してくれる DB。\n\n## シンプルにいこうぜ、harper\n\nこれはかなりストレートなハック。遊びなのでスケールさせる気はあまりなく、むしろ再現性重視。**あなた**が少ない手間で動かせるようにしたかった。\n\n目標の一つは「全部ローカルのラップトップで動かす」こと。せっかくの Mac GPU、温めようじゃないか。\n\nまずは画像ディレクトリをクロールするシンプルなクローラを作成。Apple Photos を使っているのでフォトがバラバラのフォルダにはなっていない。でも秘密のミームチャット（内緒だよ）に溜め込んだ大量のミームがあったので、それをエクスポートしてテスト用画像セットにした。\n\n### クローラ\n\n世界一ひどいクローラを作った。いや正直に言うなら、指示を出して Claude に書いてもらった“世界一ひどいクローラ”だ。\n\n手順はこう：\n\n1. 対象ディレクトリのファイル一覧を取得  \n2. 一覧を msgpack ファイルに保存  \n3. その msgpack を参照しつつ全画像を走査し、sqlite DB にメタデータを保存  \n   - ハッシュ  \n   - ファイルサイズ  \n   - パス  \n4. sqlite DB を走査し、CLIP で各画像をエンコード  \n5. 得られたベクトルを再度 sqlite に保存  \n6. sqlite を走査し、ベクトルと画像パスを chroma ベクトル DB に投入  \n7. 完了\n\n無駄が多いのは承知。画像を読んで埋め込み取って即 chroma に突っ込めばいい。chroma を選んだ理由は「簡単・無料・インフラ不要」だから。\n\nこうしたのには理由があって：\n\n- ミームのあと 14 万枚の写真をクロールしたので、落ちても再開できるタフさが必要\n- 停電やクラッシュ時に途中から再開したい\n- ループが好き\n\n多少複雑になっても問題なく動いた。20 万枚以上クロールしてノートラブル。\n\n### 埋め込みシステム\n\n画像エンコードは楽しかった。\n\n最初は siglip を使い、[シンプルな Web サービス](https://github.com/harperreed/imbedding)を作成。スタジオの GPU マシンで動かしたらまあまあ速い。ローカルで [open clip](https://github.com/mlfoundations/open_clip) を回すより全然速い。\n\nそれでもローカルで動かしたかった。そこで思い出したのが Apple の [ml-explore](https://github.com/ml-explore/) リポジトリ。そこに[CLIP 実装](https://github.com/ml-explore/mlx-examples/tree/main/clip)があって、爆速。大きめモデルでも 4090 より速い。ワイルドだ。\n\nあとはスクリプトから簡単に呼べるようにするだけ。\n\n### MLX_CLIP\n\nClaude と一緒に Apple のサンプルをゴニョって、どのマシンでもローカルで動く小さな Python クラスに仕立てた。モデルが無ければ自動で DL & 変換、そのまま推論。\n\nここで公開している： https://github.com/harperreed/mlx_clip\n\n仕上がりにはかなり満足。知ってる人は多いけど、Apple Silicon は本当に速い。\n\n使い方は超シンプル：\n\n```python\nimport mlx_clip\n\n# Initialize the mlx_clip model with the given model name.\nclip = mlx_clip.mlx_clip(\"openai/clip-vit-base-patch32\")\n\n# Encode the image from the specified file path and obtain the image embeddings.\nimage_embeddings = clip.image_encoder(\"assets/cat.jpeg\")\n# Print the image embeddings to the console.\nprint(image_embeddings)\n\n# Encode the text description and obtain the text embeddings.\ntext_embeddings = clip.text_encoder(\"a photo of a cat\")\n# Print the text embeddings to the console.\nprint(text_embeddings)\n```\n\nsiglip でも動かしたいけど、自分は POC 目的なので保守まではしたくない。siglip 対応のヒントがあれば [hmu](mailto:harper@modest.com)。open clip を再発明したくはないし、Apple Silicon でも動くはずだしね。\n\n### さて次は\n\n画像ベクトルをベクトルストアにぶち込んだので、インターフェース作り。chroma のクエリ機能をそのまま使い、類似画像を表示。\n\n基準画像のベクトルを取り、それを chroma に投げる。chroma は類似度順に画像 ID を返す。\n\nフロントは tailwind、バックは flask でさくっとラップ。完成。\n\n2015 年ならこのレベルの仕組みを作るのにどれだけ工数がかかったか想像もつかない。総作業 10 時間くらいで出来ちゃった。マジで trivial。\n\n結果はほぼ魔法。\n\n### ミーム概念検索\n\n最初のデータセットはミーム。1.2 万枚ある。\n\nまずこれをエンコードして chroma で類似検索：\n\n{{< image src=\"images/posts/vector-memes-bowie.png\" caption=\"So true\" >}}\n\n類似結果はこんな感じ：\n{{< image src=\"images/posts/vector-memes-bowie-results.png\" >}}\n\nもう一例：\n{{< image src=\"images/posts/vector-memes-star-trek.png\" >}}\n\n結果：\n{{< image src=\"images/posts/vector-memes-star-trek-results.png\" >}}\n\nクリックするだけで延々遊べる。\n\n### Namespaces?\n\n「画像をクリックして類似画像を出す」は十分クールだけど、最大の驚きは同じモデルで検索テキストをエンコードし、画像を返せること。\n\nなぜか脳がバグる。画像→画像のセマンティック検索も面白いが、マルチモーダルだと完全にマジックトリック。\n\n例：\n\n**money** で検索。money をエンコードして chroma に渡すと：\n{{< image src=\"images/posts/vector-memes-money.png\" >}}\n\n**AI** で検索：\n{{< image src=\"images/posts/vector-memes-ai.png\" >}}\n\n**red** で検索（色？ライフスタイル？ロシア？）：\n{{< image src=\"images/posts/vector-memes-red.png\" >}}\n\n永遠に遊べる。忘れていた宝物ミームも見つけられる。  \n「ブログを書くミーム欲しいな」と思ったら：\n{{< image src=\"images/posts/vector-memes-writing-meme.jpg\" >}}\n\n（自覚はあるけど気にしない ― lol）\n\n### 写真ライブラリでの威力は？\n\nめちゃくちゃ良い。\n\n自分のフォトライブラリで試すのを強く推奨。始めるには Google Photos の Takeout を DL し、外付けディスクに展開。重複だらけなのでスクリプトで整理し、ミームフォルダの代わりにそのディレクトリをクロール。\n\nおよそ 14 万枚で 6 時間ほど。悪くない。結果は最高。\n\n#### 例いろいろ：\n\n当然似ている写真（Google Photos の重複問題…）\n{{< image src=\"images/posts/vector-memes-harper.png\" >}}\n\nうちのプードル集：\n{{< image src=\"images/posts/vector-memes-poodles.png\" >}}\n\nランドマーク検索も。飛行機から富士山を撮ってたなんて知らなかった！\n{{< image src=\"images/posts/vector-memes-fuji-results.png\" >}}\n\nそこから類似の富士山写真：\n{{< image src=\"images/posts/vector-memes-fuji-similar.png\" >}}\n\n場所検索も簡単：\n{{< image src=\"images/posts/vector-memes-chicago.png\" >}}\n\n感情も。自分は「驚き顔」が多いらしい：\n{{< image src=\"images/posts/vector-memes-surprised.png\" >}}\n\nニッチに「ローライダー」（渋谷で撮影！）：\n{{< image src=\"images/posts/vector-memes-low-riders.png\" >}}\n\n検索しづらい「ボケ味」も発掘：\n{{< image src=\"images/posts/vector-memes-bokeh.png\" >}}\n\n忘れていた名ショットも掘り起こせる。2017 年に撮った Baratunde のこの写真とかね：\n\n{{< image src=\"images/posts/vector-memes-baratunde.png\" >}}\n\n### これはあちこちに搭載されるはず\n\nそのうち主要フォトアプリには全部入ると思う。Google Photos はもうやってるかもしれないけど “Google っぽさ” が強くて気付かれないだけかも。\n\n画像を扱う大規模サービスなら、今すぐパイプラインを組んでエンコードを始めたくなるレベルで強力。\n\n## なんと無料で使えます\n\nソースはここに置いた：[harperreed/photo-similarity-search](https://github.com/harperreed/photo-similarity-search)。\n\nぜひ触ってみて。かなり手作り感あるけど動くよ。笑\n\n環境は conda などで分けると楽。UI は tailwind、Web は flask、コードは python。ホストは私、harper reed でした。\n\n## キミへのチャレンジ！\n\n自分のフォトライブラリを快適にカタログ化できるアプリを作ってほしい。クラウドにアップロードはしたくない。Mac ネイティブで、ライブラリを指定したら「クロール開始」が押せるだけ、みたいな。\n\n- Llava/Moondream で自動キャプション  \n- キーワード / タグ付与  \n- ベクトル類似検索  \n- などなど\n\nすべてローカルで。ネイティブで。シンプルかつ強力。Lightroom、Capture One、Apple Photos と連携できたら最高。\n\n欲しいので誰か作って。AI の魔法で眠れる写真を掘り起こそう。\n\n## 追加課題：Lightroom プレビュー JPEG 復旧\n\nハック仲間の Ivan もこの魔法に即反応。彼のフォトカタログは外付け HDD にあるが、Lightroom のプレビューファイルはローカル。そこでサムネとメタデータを抽出して外付けに保存するスクリプトを書いてくれた。\n\nその後イメージベクトルクローラを回したら即類似検索できた。完璧。\n\n#### Lightroom 写真を救出 ― サムネだけでも\n\nIvan のシンプルなスクリプトは超便利。もし本番ライブラリが飛んでプレビューファイルだけ残った場合でも、低解像度版を救出できる。\n\n備えておくと安心。\n\nこちらに公開： [LR Preview JPEG Extractor](https://github.com/ibips/lrprev-extract)\n\n## 読んでくれてありがとう\n\nいつでも [hmu](mailto:harper@modest.com)。AI、Ecommerce、写真、ハイファイ、ハックその他いろいろ語りましょう。\n\nシカゴにいるならぜひ会いに来て。",
      "usage": {
        "prompt_tokens": 3509,
        "completion_tokens": 3398,
        "total_tokens": 6907
      },
      "streaming": true
    },
    "editing": {
      "model": "o3",
      "target_language": "japanese",
      "system_prompt": "\n        1. Carefully read the translated text alongside the original text in its entirety.\n        2. Compare both texts to ensure the translation accurately reflects the original meaning.\n        3. Correct any grammatical errors you find in the japanese text.\n        4. Adjust phrasing to make it sound natural and fluent for japanese speakers, making sure idioms and expressions are culturally appropriate.\n        5. Preserve the original tone, nuance, and style, including any formatting, markdown, and structure.\n        6. Avoid adding new information or altering the core meaning.\n        7. Ensure the final result doesn't feel machine-translated but remains faithful to the source.\n        8. Make only changes that genuinely improve the text's quality in japanese.\n        9. Don't be too literal. If there isn't a direct translation, provide a natural-sounding translation.\n        10. If the text contains idioms or colloquialisms, translate them into the target language while maintaining their original meaning.\n        11. If the text contains technical terms or jargon, ensure that the translation is accurate and appropriate for the target audience, if there isn't a natural translation, keep it in the original language.\n        12. If there is not natural translation, keep it in the original language.\n        ",
      "user_prompt": "# ORIGINAL TEXT\n## Or: how to learn about clip/siglip and vector encoding images\n\n_tl;dr_: I built a meme search engine using siglip/CLIP and vector encoding images. It was fun and I learned a lot.\n\nI have been building a lot of applied AI tools for a while. One of the components that always seemed the most magical has always been vector embeddings. [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) and the like have straight blown my mind. It is like magic.\n\nI saw a [simple app on hacker news](https://news.ycombinator.com/item?id=39392582) that was [super impressive](https://mood-amber.vercel.app/). Someone crawled a bunch of Tumblr images and used [siglip](https://arxiv.org/abs/2303.15343) to get the embeddings and then made a simple \"click the image and see similar images\" app. It was like magic. I had no idea how to achieve this, but it seemed accessible.\n\nI decided to use my sudden motivation as an opportunity to learn how \"all this works.\"\n\n## wut\n\nIf you have never ran into vector embeddings, clip/siglip, vector databases, and the like - never fear.\n\nBefore I saw the hack on hn I really didn’t think much about vector embeddings, multi modal embeddings or vector datastores. I had used faiss (facebooks simple vector store), and Pinecone ($$) for some hacks, but didn’t really dig in. Just got it to work and then was like “yep. Tests pass.”\n\nI still barely know what vectors are. Lol. Before I dug in and built this, I really didn’t understand how I would use it outside of RAG or another LLM process.\n\nI learn by building. It helps if the results are really intriguing, and in this case kind of magical.\n\n### WTF terms\n\nI had a few friends read this over before publishing and a couple were like “wtf is X?” Here is a short list of terms that were largely new to me:\n\n- **Vector Embeddings** - Vector embeddings convert your text of images into numerical representations, allowing you to find similar pics and search your library effectively.\n- **Vector Database** - A vector database is a way to store and search through encoded items, enabling you to find similar items.\n- **Word2Vec** - Word2Vec is a groundbreaking technique that converts words into numerical vectors, enabling you to perform tasks like finding similar words and exploring relationships between them.\n- **CLIP** - CLIP is OpenAI's model that encodes images and text into numerical vectors.\n- **OpenCLIP** - OpenCLIP is an open-source implementation of OpenAI's CLIP model, allowing anyone to use and build upon this powerful image and text encoding technology without the need for special access or permissions.\n- **FAISS** - FAISS is an efficient library for managing and searching through large collections of image vectors, making it fast and easy to find the images you're looking for.\n- **ChromaDB** - ChromaDB is a database that stores and retrieves your image and text vectors, quickly returning similar results for your searches.\n\n## Keep it simple, harper.\n\nThis is a pretty straight forward hack. I am just fucking around so I wasn't super interested in making it scalable. I did have an interest in making it replicable. I wanted to make something that **you** could run without a lot of work.\n\nOne of my goals was to make sure everything runs locally to my laptop. We have these fancy Mac GPUs - let's heat them up.\n\nThe first step was building out a simple crawler that would crawl a directory of images. I use Apple Photos, so I didn't have a directory full of my photos laying around. I did, however, have a giant bucket of memes from my precious and very secret meme chat group (don't tell anyone). I exported the chat, moved the images to a directory and BAM - I had my test image set.\n\n### The Crawler\n\nI created the world's worst crawler. Well. I should be honest: Claude created the world's worst crawler with my instructions.\n\nIt is a bit complicated but here are the steps:\n\n1. It gets the file list of the target directory\n2. It stores the list in a msgpack file\n3. I reference the msgpack file and then iterate through every image and store it in a sqlite db. Grabing some metadata about the file\n    - hash\n    - filesize\n    - location\n4. I iterate through that sqlite db and then use CLIP to get the vector encoding of every image.\n5. Then I store those vectors back in the sqlite db\n6. Then I iterate through the sqlite db and insert the vectors and image path into chroma vector db\n7. Then we are done\n\nThis is a lot of wasted work. You could iterate through the images, grab the embeddings and slam it into chroma (I chose chroma cuz it is easy, free, and no infra).\n\nI have built it this way because:\n\n- After the memes, I crawled 140k images and wanted it to be resilient to crashing.\n- I needed it to be able to resume building out the databases in case it crashed, power went out, etc\n- I really like loops\n\nRegardless of the extra complexity, it worked flawlessly. I have crawled over 200k images without a blip.\n\n### An embedding system\n\nEncoding the images was fun.\n\nI started with siglip and created a [simple web service](https://github.com/harperreed/imbedding) where we could upload the image and get the vectors back. This ran on one of our GPU boxes at the studio and worked well. It wasn't fast, but it was way faster than running [open clip](https://github.com/mlfoundations/open_clip) locally.\n\nI still wanted to run it locally. I remembered that the [ml-explore](https://github.com/ml-explore/) repo from apple had some neat examples that could help. And BAM they had a [clip implementation](https://github.com/ml-explore/mlx-examples/tree/main/clip) that was fast af. Even using the larger model, it was faster than the 4090. Wildstyle.\n\nI just needed to make it easy to use in my script.\n\n### MLX_CLIP\n\nClaude and I were able to coerce the example script from apple into a fun lil python class that you can use locally on any of your machines. It will download the models if they don't exist, convert them, and then use them in flight with your script.\n\nYou can check it out here: https://github.com/harperreed/mlx_clip\n\nI am pretty chuffed with how well it turned out. I know most people know this, but the apple silicon is fast af.\n\nIt turned out to be rather simple to use:\n\n```python\nimport mlx_clip\n\n# Initialize the mlx_clip model with the given model name.\nclip = mlx_clip.mlx_clip(\"openai/clip-vit-base-patch32\")\n\n# Encode the image from the specified file path and obtain the image embeddings.\nimage_embeddings = clip.image_encoder(\"assets/cat.jpeg\")\n# Print the image embeddings to the console.\nprint(image_embeddings)\n\n# Encode the text description and obtain the text embeddings.\ntext_embeddings = clip.text_encoder(\"a photo of a cat\")\n# Print the text embeddings to the console.\nprint(text_embeddings)\n```\n\nI would love to get this to work with siglip, as I prefer that model (it is way better than CLIP). However, this is a POC more than a product I want to maintain. If anyone has any hints on how to get it working with siglip - [hmu](mailto:harper@modest.com). I don't want to reinvent open clip - which should theoretically run well on apple silicon, and is very good.\n\n### Now what\n\nNow that we had all the image vectors slammed into the vector datastore we could get started with the interface. I used the built in query functionality of chromadb to show similar images.\n\nGrab the vectors of the image you are starting with. Query those vectors with chromadb. Chromed returns a list of image ids that are similar in declining similarity.\n\nI then wrapped it all up in a tailwind/flask app.\nThis was incredible.\n\nI can't imagine the amount of work we would have done in 2015 to build this. I spent maybe 10 hours total on this and it was trivial.\n\nThe results are akin to magic.\n\n### Memes concept search\n\nNow remember, I used memes as my initial set of images. I had 12000 memes to search through.\n\nStart with this:\n\n{{< image src=\"images/posts/vector-memes-bowie.png\" caption=\"So true\" >}}\n\nEncode it, pass it to chroma to return similar results.\n\nAnd then similar images that return are like this:\n{{< image src=\"images/posts/vector-memes-bowie-results.png\" >}}\n\nAnother example:\n{{< image src=\"images/posts/vector-memes-star-trek.png\" >}}\n\nGives you results like:\n{{< image src=\"images/posts/vector-memes-star-trek-results.png\" >}}\n\nIt is really fun to click around.\n\n### Namespaces?\n\nThe magic isn't clicking on an image and getting a similar image. That is cool, but wasn't \"holy shit\" for me.\n\nWhat blew my mind was using the same model to encode the search text into vectors and finding images similar to the text.\n\nFor whatever reason, this screws up my brain. It is one thing to have a neat semantic like search for images based on another images. Being able to have a nice multi modal interface really made it like a magic trick.\n\nHere are some examples:\n\nSearching for **money**. I grab the encoding for money and pass the vectors to chroma. The results for **money** are:\n{{< image src=\"images/posts/vector-memes-money.png\" >}}\n\nSearching for **AI**\n{{< image src=\"images/posts/vector-memes-ai.png\" >}}\n\nSearching for **red** (a dozy! Is it a color? Is a lifestyle? Is it Russia?)\n{{< image src=\"images/posts/vector-memes-red.png\" >}}\n\nSo on and so forth. Forever. It is magical. You can find all sorts of gems you forgot about. Oh shit I need a meme about writing a blog post:\n{{< image src=\"images/posts/vector-memes-writing-meme.jpg\" >}}\n\n(I am self aware, I just don't care - lol)\n\n### How does it work with a photo library?\n\nIt works super well.\n\nI highly recommend running this against your photo library. To get started, I downloaded my google photos takeout archive. Extracted it onto an external disk. I had to run a few scripts against it to make it usable (Whoever designed the google photos takeout is very excited about duplicate data). I then pointed the script at that directory instead of my memes folder and let 'er rip.\n\nI had about 140k photos and it took about 6 hours to run through. Not so bad. The results are incredible.\n\n#### Here are some fun examples:\n\nObviously these are similar (I also have a dupe problem in google photos)\n{{< image src=\"images/posts/vector-memes-harper.png\" >}}\n\nWe have had a lot of poodles. Here are some\n{{< image src=\"images/posts/vector-memes-poodles.png\" >}}\n\nYou can search for landmarks. I had no idea I had taken a photo of fuji-san from a plane!\n{{< image src=\"images/posts/vector-memes-fuji-results.png\" >}}\n\nAnd then find similar images of Mt Fuji.\n{{< image src=\"images/posts/vector-memes-fuji-similar.png\" >}}\n\nIt is pretty easy to search for places.\n{{< image src=\"images/posts/vector-memes-chicago.png\" >}}\n\nOr emotions. I am apparently surprising so I have a lot of surprised photos.\n{{< image src=\"images/posts/vector-memes-surprised.png\" >}}\n\nAlso niche things like low riders. (These are from Shibuya!)\n{{< image src=\"images/posts/vector-memes-low-riders.png\" >}}\n\nAnd you can use it to find things that are not easy to find or search for. Like bokeh.\n{{< image src=\"images/posts/vector-memes-bokeh.png\" >}}\n\nIt's wonderful, because I can click through and find great images I had forgotten about. Like this great photo of Baratunde that I took in 2017:\n\n{{< image src=\"images/posts/vector-memes-baratunde.png\" >}}\n\n### This will be everywhere\n\nI imagine that we will see this tech rolled into all the various photo apps shortly. Google Photos probably already does this, but they have googled it so much that nobody notices.\n\nThis is too good to not roll into whatever photo app you use. If I had any large scale product that used photos or images, I would immediately set up a pipeline to start encoding the images to see what kind of weird features this unlocks.\n\n## YOU CAN USE THIS FOR THE LOW PRICE OF FREE\n\nI put the source here: [harperreed/photo-similarity-search](https://github.com/harperreed/photo-similarity-search).\n\nPlease check it out.\n\nIt is pretty straight forward to get going. It is a bit hacky. lol.\n\nI would use conda or something similar to keep things clean. The interface is simple tailwind. The web is flask. The code is python. I am your host, harper reed.\n\n## My challenge for you!\n\nPlease build an app that I can use to catalog my photo library in a nice way. I don't want to upload to another destination. I want to have a simple Mac app that I can point to my photo library and say \"crawl this.\" I imagine a lot of neat stuff could be added:\n\n- Llava/Moondream auto captioning\n- Keywords / Tags\n- Vector similarity\n- etc\n\nIt should run locally. Be a native app. Be simple, and effective. Maybe plug into Lightroom, capture one, or apple photos.\n\nI want this. Build it. Let's discover all the amazing photos we have taken through the magic of AI.\n\n## Extra Credit: Light Room Preview JPEG Recovery\n\nMy hacking buddy Ivan was around while I was building this. He immediately saw the magic of what a person could discover by using this on their photo library. He wanted to use it immediately.\n\nHis photo catalog is on an external hard drive - but he had his Lightroom preview file locally. He wrote a quick script to extract the thumbnails and metadata from the preview file and save it to an external disk.\n\nWe then ran the image vector crawler and BAM - he could see similar images and what not. Worked perfectly.\n\n#### Recover your Lightroom photos. Or at least the thumbnails.\n\nIvan's simple script to extract the images from the preview file is really awesome. If you have ever lost your real photo library (corrupt harddrive, or whatever) and you still have the lrpreview file - this script can help you extract at least the lower res version.\n\nA super handy script to keep around.\n\nYou can check it out here: [LR Preview JPEG Extractor](https://github.com/ibips/lrprev-extract).\n\n## Thanks for reading.\n\nAs always, [hmu](mailto:harper@modest.com) and let's hang out. I am thinking a lot about AI, Ecommerce, photos, hifi, hacking and other shit.\n\nIf you are in Chicago come hang out.\n\n# TRANSLATED TEXT\n## つまり：clip/siglip と画像ベクトルエンコードを学ぶ方法\n\n_tl;dr_: siglip/CLIP を使ってミーム検索エンジンを作った。めちゃくちゃ楽しくて学びも多かった。\n\nしばらく前から、いろいろな実用系 AI ツールを作っている。その中でも “魔法” っぽさが際立っているのがベクトル埋め込みだ。[Word2Vec](https://en.wikipedia.org/wiki/Word2vec) なんかは本当に衝撃だった。まさに魔法。\n\n[Hacker News で見かけたシンプルなアプリ](https://news.ycombinator.com/item?id=39392582)が[かなりスゴかった](https://mood-amber.vercel.app/)。誰かが Tumblr の画像を大量にクロールし、[siglip](https://arxiv.org/abs/2303.15343) で埋め込みを取り、「画像をクリックすると類似画像が出てくる」アプリを作っていた。魔法みたいで、しかも手が届きそう。\n\nその勢いのまま、「これ全部どう動いてるんだろう？」を理解するチャンスにすることにした。\n\n## wut\n\nベクトル埋め込みや clip/siglip、ベクトル DB などに触れたことがなくても大丈夫。  \nHN であのハックを見るまでは、マルチモーダル埋め込みやベクトルストアについて深く考えたことがなかった。faiss（Facebook 製のシンプルなベクトルストア）や Pinecone（$$）を使ったことはあったけど、動けば OK 程度。テストが通れば「はい終了」。\n\n正直ベクトルが何かなんて今でもほとんど分かってない。笑  \nこれを作る前は、RAG や他の LLM 系プロセス以外でどう使うか想像も付かなかった。\n\n自分は「作って覚える」タイプ。結果が面白く、しかも今回はちょっとした“魔法”だったのでやる気も倍増。\n\n### WTF 用語集\n\n公開前に何人か友人に読んでもらったら「X って何？」と言われたので、当時ほぼ初見だった用語をざっくりまとめる。\n\n- **Vector Embeddings** - テキストや画像を数値ベクトルに変換し、類似画像検索などを可能にする。\n- **Vector Database** - エンコード済みのアイテムを保存・検索する DB。似ているものを素早く見つけられる。\n- **Word2Vec** - 単語を数値ベクトルに変換する画期的な手法。類似語や単語間の関係性を探索できる。\n- **CLIP** - OpenAI が公開した、画像とテキストを同じベクトル空間にエンコードするモデル。\n- **OpenCLIP** - OpenAI の CLIP を OSS 実装したもの。誰でも自由に使える。\n- **FAISS** - 大量の画像ベクトルを高速に検索・管理できるライブラリ。\n- **ChromaDB** - 画像やテキストのベクトルを保存し、類似検索を高速に返してくれる DB。\n\n## シンプルにいこうぜ、harper\n\nこれはかなりストレートなハック。遊びなのでスケールさせる気はあまりなく、むしろ再現性重視。**あなた**が少ない手間で動かせるようにしたかった。\n\n目標の一つは「全部ローカルのラップトップで動かす」こと。せっかくの Mac GPU、温めようじゃないか。\n\nまずは画像ディレクトリをクロールするシンプルなクローラを作成。Apple Photos を使っているのでフォトがバラバラのフォルダにはなっていない。でも秘密のミームチャット（内緒だよ）に溜め込んだ大量のミームがあったので、それをエクスポートしてテスト用画像セットにした。\n\n### クローラ\n\n世界一ひどいクローラを作った。いや正直に言うなら、指示を出して Claude に書いてもらった“世界一ひどいクローラ”だ。\n\n手順はこう：\n\n1. 対象ディレクトリのファイル一覧を取得  \n2. 一覧を msgpack ファイルに保存  \n3. その msgpack を参照しつつ全画像を走査し、sqlite DB にメタデータを保存  \n   - ハッシュ  \n   - ファイルサイズ  \n   - パス  \n4. sqlite DB を走査し、CLIP で各画像をエンコード  \n5. 得られたベクトルを再度 sqlite に保存  \n6. sqlite を走査し、ベクトルと画像パスを chroma ベクトル DB に投入  \n7. 完了\n\n無駄が多いのは承知。画像を読んで埋め込み取って即 chroma に突っ込めばいい。chroma を選んだ理由は「簡単・無料・インフラ不要」だから。\n\nこうしたのには理由があって：\n\n- ミームのあと 14 万枚の写真をクロールしたので、落ちても再開できるタフさが必要\n- 停電やクラッシュ時に途中から再開したい\n- ループが好き\n\n多少複雑になっても問題なく動いた。20 万枚以上クロールしてノートラブル。\n\n### 埋め込みシステム\n\n画像エンコードは楽しかった。\n\n最初は siglip を使い、[シンプルな Web サービス](https://github.com/harperreed/imbedding)を作成。スタジオの GPU マシンで動かしたらまあまあ速い。ローカルで [open clip](https://github.com/mlfoundations/open_clip) を回すより全然速い。\n\nそれでもローカルで動かしたかった。そこで思い出したのが Apple の [ml-explore](https://github.com/ml-explore/) リポジトリ。そこに[CLIP 実装](https://github.com/ml-explore/mlx-examples/tree/main/clip)があって、爆速。大きめモデルでも 4090 より速い。ワイルドだ。\n\nあとはスクリプトから簡単に呼べるようにするだけ。\n\n### MLX_CLIP\n\nClaude と一緒に Apple のサンプルをゴニョって、どのマシンでもローカルで動く小さな Python クラスに仕立てた。モデルが無ければ自動で DL & 変換、そのまま推論。\n\nここで公開している： https://github.com/harperreed/mlx_clip\n\n仕上がりにはかなり満足。知ってる人は多いけど、Apple Silicon は本当に速い。\n\n使い方は超シンプル：\n\n```python\nimport mlx_clip\n\n# Initialize the mlx_clip model with the given model name.\nclip = mlx_clip.mlx_clip(\"openai/clip-vit-base-patch32\")\n\n# Encode the image from the specified file path and obtain the image embeddings.\nimage_embeddings = clip.image_encoder(\"assets/cat.jpeg\")\n# Print the image embeddings to the console.\nprint(image_embeddings)\n\n# Encode the text description and obtain the text embeddings.\ntext_embeddings = clip.text_encoder(\"a photo of a cat\")\n# Print the text embeddings to the console.\nprint(text_embeddings)\n```\n\nsiglip でも動かしたいけど、自分は POC 目的なので保守まではしたくない。siglip 対応のヒントがあれば [hmu](mailto:harper@modest.com)。open clip を再発明したくはないし、Apple Silicon でも動くはずだしね。\n\n### さて次は\n\n画像ベクトルをベクトルストアにぶち込んだので、インターフェース作り。chroma のクエリ機能をそのまま使い、類似画像を表示。\n\n基準画像のベクトルを取り、それを chroma に投げる。chroma は類似度順に画像 ID を返す。\n\nフロントは tailwind、バックは flask でさくっとラップ。完成。\n\n2015 年ならこのレベルの仕組みを作るのにどれだけ工数がかかったか想像もつかない。総作業 10 時間くらいで出来ちゃった。マジで trivial。\n\n結果はほぼ魔法。\n\n### ミーム概念検索\n\n最初のデータセットはミーム。1.2 万枚ある。\n\nまずこれをエンコードして chroma で類似検索：\n\n{{< image src=\"images/posts/vector-memes-bowie.png\" caption=\"So true\" >}}\n\n類似結果はこんな感じ：\n{{< image src=\"images/posts/vector-memes-bowie-results.png\" >}}\n\nもう一例：\n{{< image src=\"images/posts/vector-memes-star-trek.png\" >}}\n\n結果：\n{{< image src=\"images/posts/vector-memes-star-trek-results.png\" >}}\n\nクリックするだけで延々遊べる。\n\n### Namespaces?\n\n「画像をクリックして類似画像を出す」は十分クールだけど、最大の驚きは同じモデルで検索テキストをエンコードし、画像を返せること。\n\nなぜか脳がバグる。画像→画像のセマンティック検索も面白いが、マルチモーダルだと完全にマジックトリック。\n\n例：\n\n**money** で検索。money をエンコードして chroma に渡すと：\n{{< image src=\"images/posts/vector-memes-money.png\" >}}\n\n**AI** で検索：\n{{< image src=\"images/posts/vector-memes-ai.png\" >}}\n\n**red** で検索（色？ライフスタイル？ロシア？）：\n{{< image src=\"images/posts/vector-memes-red.png\" >}}\n\n永遠に遊べる。忘れていた宝物ミームも見つけられる。  \n「ブログを書くミーム欲しいな」と思ったら：\n{{< image src=\"images/posts/vector-memes-writing-meme.jpg\" >}}\n\n（自覚はあるけど気にしない ― lol）\n\n### 写真ライブラリでの威力は？\n\nめちゃくちゃ良い。\n\n自分のフォトライブラリで試すのを強く推奨。始めるには Google Photos の Takeout を DL し、外付けディスクに展開。重複だらけなのでスクリプトで整理し、ミームフォルダの代わりにそのディレクトリをクロール。\n\nおよそ 14 万枚で 6 時間ほど。悪くない。結果は最高。\n\n#### 例いろいろ：\n\n当然似ている写真（Google Photos の重複問題…）\n{{< image src=\"images/posts/vector-memes-harper.png\" >}}\n\nうちのプードル集：\n{{< image src=\"images/posts/vector-memes-poodles.png\" >}}\n\nランドマーク検索も。飛行機から富士山を撮ってたなんて知らなかった！\n{{< image src=\"images/posts/vector-memes-fuji-results.png\" >}}\n\nそこから類似の富士山写真：\n{{< image src=\"images/posts/vector-memes-fuji-similar.png\" >}}\n\n場所検索も簡単：\n{{< image src=\"images/posts/vector-memes-chicago.png\" >}}\n\n感情も。自分は「驚き顔」が多いらしい：\n{{< image src=\"images/posts/vector-memes-surprised.png\" >}}\n\nニッチに「ローライダー」（渋谷で撮影！）：\n{{< image src=\"images/posts/vector-memes-low-riders.png\" >}}\n\n検索しづらい「ボケ味」も発掘：\n{{< image src=\"images/posts/vector-memes-bokeh.png\" >}}\n\n忘れていた名ショットも掘り起こせる。2017 年に撮った Baratunde のこの写真とかね：\n\n{{< image src=\"images/posts/vector-memes-baratunde.png\" >}}\n\n### これはあちこちに搭載されるはず\n\nそのうち主要フォトアプリには全部入ると思う。Google Photos はもうやってるかもしれないけど “Google っぽさ” が強くて気付かれないだけかも。\n\n画像を扱う大規模サービスなら、今すぐパイプラインを組んでエンコードを始めたくなるレベルで強力。\n\n## なんと無料で使えます\n\nソースはここに置いた：[harperreed/photo-similarity-search](https://github.com/harperreed/photo-similarity-search)。\n\nぜひ触ってみて。かなり手作り感あるけど動くよ。笑\n\n環境は conda などで分けると楽。UI は tailwind、Web は flask、コードは python。ホストは私、harper reed でした。\n\n## キミへのチャレンジ！\n\n自分のフォトライブラリを快適にカタログ化できるアプリを作ってほしい。クラウドにアップロードはしたくない。Mac ネイティブで、ライブラリを指定したら「クロール開始」が押せるだけ、みたいな。\n\n- Llava/Moondream で自動キャプション  \n- キーワード / タグ付与  \n- ベクトル類似検索  \n- などなど\n\nすべてローカルで。ネイティブで。シンプルかつ強力。Lightroom、Capture One、Apple Photos と連携できたら最高。\n\n欲しいので誰か作って。AI の魔法で眠れる写真を掘り起こそう。\n\n## 追加課題：Lightroom プレビュー JPEG 復旧\n\nハック仲間の Ivan もこの魔法に即反応。彼のフォトカタログは外付け HDD にあるが、Lightroom のプレビューファイルはローカル。そこでサムネとメタデータを抽出して外付けに保存するスクリプトを書いてくれた。\n\nその後イメージベクトルクローラを回したら即類似検索できた。完璧。\n\n#### Lightroom 写真を救出 ― サムネだけでも\n\nIvan のシンプルなスクリプトは超便利。もし本番ライブラリが飛んでプレビューファイルだけ残った場合でも、低解像度版を救出できる。\n\n備えておくと安心。\n\nこちらに公開： [LR Preview JPEG Extractor](https://github.com/ibips/lrprev-extract)\n\n## 読んでくれてありがとう\n\nいつでも [hmu](mailto:harper@modest.com)。AI、Ecommerce、写真、ハイファイ、ハックその他いろいろ語りましょう。\n\nシカゴにいるならぜひ会いに来て。\n\nPlease review and improve the translated text to make it natural and accurate in japanese.\nReturn ONLY the improved translated text without explanations or comments.",
      "response": "## つまり：clip/siglip と画像ベクトルエンコードを学ぶ方法\n\n_tl;dr_: siglip/CLIP を使ってミーム検索エンジンを作った。めちゃくちゃ楽しくて、学びも多かった。\n\nしばらく前から実用系の AI ツールをいろいろ作っている。その中でも“魔法”感が強いのがベクトル埋め込みだ。[Word2Vec](https://en.wikipedia.org/wiki/Word2vec) を初めて触ったときは本当に衝撃で、まさに魔法だった。\n\n[Hacker News で見かけたシンプルなアプリ](https://news.ycombinator.com/item?id=39392582)が[とにかくスゴかった](https://mood-amber.vercel.app/)。誰かが Tumblr の画像を大量にクロールし、[siglip](https://arxiv.org/abs/2303.15343) で埋め込みを生成して、「画像をクリックすると類似画像が表示される」アプリを作っていた。まさに魔法で、しかも自分にも手が届きそうな感じだった。\n\nその勢いのまま、「これ全部どう動いてるんだろう？」を理解するチャンスにすることにした。\n\n## wut\n\nベクトル埋め込みや CLIP/siglip、ベクトル DB に触れたことがなくても心配ご無用。  \nHN であのハックを見るまでは、マルチモーダル埋め込みやベクトルストアについて深く考えたことがなかった。faiss（Facebook 製のシンプルなベクトルストア）や Pinecone（$$）をいじったことはあったけど、動けば OK。テストが通れば「はい終了」って感じだった。\n\n正直、ベクトルが何なのか今でもほとんど分かっていない。笑  \nこれを作る前は、RAG や他の LLM 系プロセス以外でどう使うか想像もつかなかった。\n\n自分は「作って覚える」派。結果が面白く、しかも今回はちょっとした“魔法”だったのでやる気も倍増した。\n\n### WTF 用語集\n\n公開前に数人の友人に読んでもらったら「X って何？」と言われたので、当時ほぼ初見だった用語をざっくりまとめる。\n\n- **Vector Embeddings** - テキストや画像を数値ベクトルに変換し、類似画像検索などを可能にする技術。  \n- **Vector Database** - エンコード済みのアイテムを保存・検索できる DB。似ているものを高速で見つけられる。  \n- **Word2Vec** - 単語を数値ベクトルに変換する画期的な手法。類似語や単語間の関係を探索できる。  \n- **CLIP** - OpenAI が公開した、画像とテキストを同じベクトル空間にエンコードするモデル。  \n- **OpenCLIP** - OpenAI の CLIP を OSS 化した実装。誰でも自由に使える。  \n- **FAISS** - 大量のベクトルを高速に検索・管理できるライブラリ。  \n- **ChromaDB** - 画像やテキストのベクトルを保存し、類似検索を高速に返してくれる DB。  \n\n## シンプルにいこうぜ、harper\n\nこれはかなりストレートなハック。遊びなのでスケールさせる気はあまりなく、むしろ再現性重視。**あなた**が少ない手間で動かせるようにしたかった。\n\n目標の一つは「全部ローカルのラップトップで動かす」こと。せっかくの Mac GPU、温めようじゃないか。\n\nまずは画像ディレクトリをクロールするシンプルなクローラを作成。Apple Photos を使っているのでフォトがバラバラのフォルダにはなっていない。でも秘密のミームチャット（内緒だよ）に溜め込んでいた大量のミームがあったので、それをエクスポートしてテスト用画像セットにした。\n\n### クローラ\n\n世界一ひどいクローラを作った。正直に言うなら、指示を出して Claude に書いてもらった“世界一ひどいクローラ”。\n\n手順はこう：\n\n1. 対象ディレクトリのファイル一覧を取得  \n2. 一覧を msgpack に保存  \n3. その msgpack を参照しつつ全画像を走査し、SQLite にメタデータを保存  \n   - ハッシュ  \n   - ファイルサイズ  \n   - パス  \n4. SQLite を走査し、CLIP で各画像をエンコード  \n5. 得られたベクトルを再び SQLite に保存  \n6. SQLite を走査し、ベクトルと画像パスを Chroma のベクトル DB に投入  \n7. 完了  \n\n無駄が多いのは承知。画像を読んで埋め込みを取って、そのまま Chroma に突っ込めばいい。Chroma を選んだ理由は「簡単・無料・インフラ不要」だから。\n\nこうしたのには理由があって：\n\n- ミームのあと 14 万枚の写真をクロールしたので、途中で落ちても再開できるタフさが必要  \n- 停電やクラッシュ時に途中から再開したい  \n- ループが好き  \n\n多少複雑になっても問題なく動いた。20 万枚以上クロールしてノートラブル。\n\n### 埋め込みシステム\n\n画像エンコードは楽しかった。\n\n最初は siglip を使い、[シンプルな Web サービス](https://github.com/harperreed/imbedding)を作成。スタジオの GPU マシンで動かしたらまあまあ速い。ローカルで [open clip](https://github.com/mlfoundations/open_clip) を回すより全然速い。\n\nそれでもローカルで動かしたかった。そこで思い出したのが Apple の [ml-explore](https://github.com/ml-explore/) リポジトリ。そこに [CLIP の実装](https://github.com/ml-explore/mlx-examples/tree/main/clip)があって、爆速。大きめモデルでも 4090 より速い。ワイルドだ。\n\nあとはスクリプトから簡単に呼べるようにするだけ。\n\n### MLX_CLIP\n\nClaude と一緒に Apple のサンプルをいじって、どのマシンでもローカルで動く小さな Python クラスに仕立てた。モデルがなければ自動でダウンロード＆変換、そのまま推論。\n\n公開リポジトリ： <https://github.com/harperreed/mlx_clip>\n\n仕上がりにはかなり満足。知ってる人は多いけど、Apple Silicon は本当に速い。\n\n使い方は超シンプル：\n\n```python\nimport mlx_clip\n\n# モデルを初期化\nclip = mlx_clip.mlx_clip(\"openai/clip-vit-base-patch32\")\n\n# 画像をエンコード\nimage_embeddings = clip.image_encoder(\"assets/cat.jpeg\")\nprint(image_embeddings)\n\n# テキストをエンコード\ntext_embeddings = clip.text_encoder(\"a photo of a cat\")\nprint(text_embeddings)\n```\n\nsiglip でも動かしたいけど、今回は POC なので保守まではしたくない。siglip 対応のヒントがあれば [hmu](mailto:harper@modest.com)。open clip を再発明したくないし、Apple Silicon でも動くはずだしね。\n\n### さて次は\n\n画像ベクトルをベクトルストアに突っ込んだので、インターフェース作り。Chroma のクエリ機能を使って類似画像を表示。\n\n基準画像のベクトルを取得して Chroma に投げる。Chroma は類似度順に画像 ID を返す。\n\nフロントは Tailwind、バックは Flask でサクッとラップ。完成。\n\n2015 年ならこのレベルの仕組みを作るのにどれだけ工数がかかったか想像もつかない。総作業 10 時間くらいで出来ちゃった。マジで trivial。\n\n結果はほぼ魔法。\n\n### ミーム概念検索\n\n最初のデータセットはミーム。1.2 万枚ある。\n\nまずこれをエンコードして Chroma で類似検索：\n\n{{< image src=\"images/posts/vector-memes-bowie.png\" caption=\"So true\" >}}\n\n類似結果はこんな感じ：  \n{{< image src=\"images/posts/vector-memes-bowie-results.png\" >}}\n\nもう一例：  \n{{< image src=\"images/posts/vector-memes-star-trek.png\" >}}\n\n結果：  \n{{< image src=\"images/posts/vector-memes-star-trek-results.png\" >}}\n\nクリックするだけで延々遊べる。\n\n### Namespaces?\n\n「画像をクリックして類似画像を出す」は十分クールだけど、最大の驚きは同じモデルで検索テキストをエンコードし、画像を返せること。\n\nなぜか脳がバグる。画像→画像のセマンティック検索も面白いが、マルチモーダルだと完全にマジックトリック。\n\n例：\n\n**money** で検索。money をエンコードして Chroma に渡すと：  \n{{< image src=\"images/posts/vector-memes-money.png\" >}}\n\n**AI** で検索：  \n{{< image src=\"images/posts/vector-memes-ai.png\" >}}\n\n**red** で検索（色？ ライフスタイル？ ロシア？）：  \n{{< image src=\"images/posts/vector-memes-red.png\" >}}\n\n永遠に遊べる。忘れていたお宝ミームも見つかる。  \n「ブログ書くときのミーム欲しいな」と思ったら：  \n{{< image src=\"images/posts/vector-memes-writing-meme.jpg\" >}}\n\n（自覚はあるけど気にしない ― lol）\n\n### 写真ライブラリでの威力は？\n\nめちゃくちゃ良い。\n\n自分のフォトライブラリで試すのを強く推奨。まず Google Photos の Takeout をダウンロードし、外付けディスクに展開。重複だらけなのでスクリプトで整理し、ミームフォルダの代わりにそのディレクトリをクロール。\n\nおよそ 14 万枚で 6 時間ほど。悪くない。結果は最高。\n\n#### 例いろいろ：\n\n当然似ている写真（Google Photos の重複問題…）  \n{{< image src=\"images/posts/vector-memes-harper.png\" >}}\n\nうちのプードル集：  \n{{< image src=\"images/posts/vector-memes-poodles.png\" >}}\n\nランドマーク検索も。飛行機から富士山を撮ってたなんて知らなかった！  \n{{< image src=\"images/posts/vector-memes-fuji-results.png\" >}}\n\nそこから類似の富士山写真：  \n{{< image src=\"images/posts/vector-memes-fuji-similar.png\" >}}\n\n場所検索も簡単：  \n{{< image src=\"images/posts/vector-memes-chicago.png\" >}}\n\n感情も。自分は「驚き顔」が多いらしい：  \n{{< image src=\"images/posts/vector-memes-surprised.png\" >}}\n\nニッチに「ローライダー」（渋谷で撮影！）：  \n{{< image src=\"images/posts/vector-memes-low-riders.png\" >}}\n\n検索しづらい「ボケ味」も発掘：  \n{{< image src=\"images/posts/vector-memes-bokeh.png\" >}}\n\n忘れていた名ショットも掘り起こせる。2017 年に撮った Baratunde のこの写真とか：  \n\n{{< image src=\"images/posts/vector-memes-baratunde.png\" >}}\n\n### これはあちこちに搭載されるはず\n\nそのうち主要フォトアプリには全部入ると思う。Google Photos はすでにやっているかもしれないけど、“Google っぽさ” が強すぎて気付かれていないだけかも。\n\n画像を扱う大規模サービスなら、今すぐパイプラインを組んでエンコードを始めたくなるレベルで強力。\n\n## なんと無料で使えます\n\nソースはこちら：[harperreed/photo-similarity-search](https://github.com/harperreed/photo-similarity-search)\n\nぜひ触ってみて。かなり手作り感あるけど動くよ。笑\n\n環境は conda などで分けると楽。UI は Tailwind、Web は Flask、コードは Python。ホストは私、harper reed でした。\n\n## キミへのチャレンジ！\n\n自分のフォトライブラリを快適にカタログ化できる Mac ネイティブアプリを作ってほしい。クラウドにアップロードはしたくない。ライブラリを指定して「クロール開始」を押すだけ、みたいなやつ。\n\n- Llava/Moondream で自動キャプション  \n- キーワード／タグ付与  \n- ベクトル類似検索  \n- などなど  \n\nすべてローカルで。シンプルかつ強力。Lightroom、Capture One、Apple Photos と連携できたら最高。\n\n欲しいので誰か作って。AI の魔法で眠れる写真を掘り起こそう。\n\n## 追加課題：Lightroom プレビュー JPEG 復旧\n\nハック仲間の Ivan もこの魔法を見て即反応。彼のフォトカタログは外付け HDD にあるが、Lightroom のプレビューファイルはローカル。そこでサムネイルとメタデータを抽出して外付けに保存するスクリプトを書いてくれた。\n\nその後イメージベクトルクローラを回したら、類似検索もバッチリ。完璧。\n\n#### Lightroom の写真を救出 ― サムネだけでも\n\nIvan のシンプルなスクリプトは超便利。もし本番ライブラリが飛んでプレビューファイルだけ残った場合でも、低解像度版を救出できる。\n\n備えておくと安心だ。\n\n公開リポジトリ： [LR Preview JPEG Extractor](https://github.com/ibips/lrprev-extract)\n\n## 読んでくれてありがとう\n\nいつでも [hmu](mailto:harper@modest.com)。AI、Ecommerce、写真、ハイファイ、ハックその他いろいろ語りましょう。\n\nシカゴにいるならぜひ会いに来て。",
      "usage": {
        "prompt_tokens": 7979,
        "completion_tokens": 3402,
        "total_tokens": 11381
      },
      "streaming": true
    },
    "critique": {
      "model": "o3",
      "target_language": "japanese",
      "system_prompt": "You are a highly critical professional translator and linguistic expert specializing in japanese.\nYour task is to ruthlessly critique the translation by:\n\n1. Meticulously comparing the translated text with the original, identifying ANY inaccuracies, mistranslations, or omissions\n2. Highlighting nuances, cultural references, or idioms that were lost or mistranslated\n3. Scrutinizing for grammatical errors, awkward phrasing, or unnatural expressions in japanese\n4. Checking for inconsistencies in tone, style, or register compared to the original\n5. Verifying that technical terms are translated accurately and consistently\n6. Ensuring no content was accidentally skipped or added\n7. Finding places where the translation sounds machine-like or overly literal\n\nBe extremely thorough and critical in your assessment. Do not accept mediocre translations.\nList specific issues and suggestions for improvement, organized by severity and category.\nYour critique should be detailed enough for another translator to address all the issues.\n\nYour goal is to help create a perfect translation that reads as if originally written in japanese while being 100% faithful to the source.\n",
      "user_prompt": "# ORIGINAL TEXT\n## Or: how to learn about clip/siglip and vector encoding images\n\n_tl;dr_: I built a meme search engine using siglip/CLIP and vector encoding images. It was fun and I learned a lot.\n\nI have been building a lot of applied AI tools for a while. One of the components that always seemed the most magical has always been vector embeddings. [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) and the like have straight blown my mind. It is like magic.\n\nI saw a [simple app on hacker news](https://news.ycombinator.com/item?id=39392582) that was [super impressive](https://mood-amber.vercel.app/). Someone crawled a bunch of Tumblr images and used [siglip](https://arxiv.org/abs/2303.15343) to get the embeddings and then made a simple \"click the image and see similar images\" app. It was like magic. I had no idea how to achieve this, but it seemed accessible.\n\nI decided to use my sudden motivation as an opportunity to learn how \"all this works.\"\n\n## wut\n\nIf you have never ran into vector embeddings, clip/siglip, vector databases, and the like - never fear.\n\nBefore I saw the hack on hn I really didn’t think much about vector embeddings, multi modal embeddings or vector datastores. I had used faiss (facebooks simple vector store), and Pinecone ($$) for some hacks, but didn’t really dig in. Just got it to work and then was like “yep. Tests pass.”\n\nI still barely know what vectors are. Lol. Before I dug in and built this, I really didn’t understand how I would use it outside of RAG or another LLM process.\n\nI learn by building. It helps if the results are really intriguing, and in this case kind of magical.\n\n### WTF terms\n\nI had a few friends read this over before publishing and a couple were like “wtf is X?” Here is a short list of terms that were largely new to me:\n\n- **Vector Embeddings** - Vector embeddings convert your text of images into numerical representations, allowing you to find similar pics and search your library effectively.\n- **Vector Database** - A vector database is a way to store and search through encoded items, enabling you to find similar items.\n- **Word2Vec** - Word2Vec is a groundbreaking technique that converts words into numerical vectors, enabling you to perform tasks like finding similar words and exploring relationships between them.\n- **CLIP** - CLIP is OpenAI's model that encodes images and text into numerical vectors.\n- **OpenCLIP** - OpenCLIP is an open-source implementation of OpenAI's CLIP model, allowing anyone to use and build upon this powerful image and text encoding technology without the need for special access or permissions.\n- **FAISS** - FAISS is an efficient library for managing and searching through large collections of image vectors, making it fast and easy to find the images you're looking for.\n- **ChromaDB** - ChromaDB is a database that stores and retrieves your image and text vectors, quickly returning similar results for your searches.\n\n## Keep it simple, harper.\n\nThis is a pretty straight forward hack. I am just fucking around so I wasn't super interested in making it scalable. I did have an interest in making it replicable. I wanted to make something that **you** could run without a lot of work.\n\nOne of my goals was to make sure everything runs locally to my laptop. We have these fancy Mac GPUs - let's heat them up.\n\nThe first step was building out a simple crawler that would crawl a directory of images. I use Apple Photos, so I didn't have a directory full of my photos laying around. I did, however, have a giant bucket of memes from my precious and very secret meme chat group (don't tell anyone). I exported the chat, moved the images to a directory and BAM - I had my test image set.\n\n### The Crawler\n\nI created the world's worst crawler. Well. I should be honest: Claude created the world's worst crawler with my instructions.\n\nIt is a bit complicated but here are the steps:\n\n1. It gets the file list of the target directory\n2. It stores the list in a msgpack file\n3. I reference the msgpack file and then iterate through every image and store it in a sqlite db. Grabing some metadata about the file\n    - hash\n    - filesize\n    - location\n4. I iterate through that sqlite db and then use CLIP to get the vector encoding of every image.\n5. Then I store those vectors back in the sqlite db\n6. Then I iterate through the sqlite db and insert the vectors and image path into chroma vector db\n7. Then we are done\n\nThis is a lot of wasted work. You could iterate through the images, grab the embeddings and slam it into chroma (I chose chroma cuz it is easy, free, and no infra).\n\nI have built it this way because:\n\n- After the memes, I crawled 140k images and wanted it to be resilient to crashing.\n- I needed it to be able to resume building out the databases in case it crashed, power went out, etc\n- I really like loops\n\nRegardless of the extra complexity, it worked flawlessly. I have crawled over 200k images without a blip.\n\n### An embedding system\n\nEncoding the images was fun.\n\nI started with siglip and created a [simple web service](https://github.com/harperreed/imbedding) where we could upload the image and get the vectors back. This ran on one of our GPU boxes at the studio and worked well. It wasn't fast, but it was way faster than running [open clip](https://github.com/mlfoundations/open_clip) locally.\n\nI still wanted to run it locally. I remembered that the [ml-explore](https://github.com/ml-explore/) repo from apple had some neat examples that could help. And BAM they had a [clip implementation](https://github.com/ml-explore/mlx-examples/tree/main/clip) that was fast af. Even using the larger model, it was faster than the 4090. Wildstyle.\n\nI just needed to make it easy to use in my script.\n\n### MLX_CLIP\n\nClaude and I were able to coerce the example script from apple into a fun lil python class that you can use locally on any of your machines. It will download the models if they don't exist, convert them, and then use them in flight with your script.\n\nYou can check it out here: https://github.com/harperreed/mlx_clip\n\nI am pretty chuffed with how well it turned out. I know most people know this, but the apple silicon is fast af.\n\nIt turned out to be rather simple to use:\n\n```python\nimport mlx_clip\n\n# Initialize the mlx_clip model with the given model name.\nclip = mlx_clip.mlx_clip(\"openai/clip-vit-base-patch32\")\n\n# Encode the image from the specified file path and obtain the image embeddings.\nimage_embeddings = clip.image_encoder(\"assets/cat.jpeg\")\n# Print the image embeddings to the console.\nprint(image_embeddings)\n\n# Encode the text description and obtain the text embeddings.\ntext_embeddings = clip.text_encoder(\"a photo of a cat\")\n# Print the text embeddings to the console.\nprint(text_embeddings)\n```\n\nI would love to get this to work with siglip, as I prefer that model (it is way better than CLIP). However, this is a POC more than a product I want to maintain. If anyone has any hints on how to get it working with siglip - [hmu](mailto:harper@modest.com). I don't want to reinvent open clip - which should theoretically run well on apple silicon, and is very good.\n\n### Now what\n\nNow that we had all the image vectors slammed into the vector datastore we could get started with the interface. I used the built in query functionality of chromadb to show similar images.\n\nGrab the vectors of the image you are starting with. Query those vectors with chromadb. Chromed returns a list of image ids that are similar in declining similarity.\n\nI then wrapped it all up in a tailwind/flask app.\nThis was incredible.\n\nI can't imagine the amount of work we would have done in 2015 to build this. I spent maybe 10 hours total on this and it was trivial.\n\nThe results are akin to magic.\n\n### Memes concept search\n\nNow remember, I used memes as my initial set of images. I had 12000 memes to search through.\n\nStart with this:\n\n{{< image src=\"images/posts/vector-memes-bowie.png\" caption=\"So true\" >}}\n\nEncode it, pass it to chroma to return similar results.\n\nAnd then similar images that return are like this:\n{{< image src=\"images/posts/vector-memes-bowie-results.png\" >}}\n\nAnother example:\n{{< image src=\"images/posts/vector-memes-star-trek.png\" >}}\n\nGives you results like:\n{{< image src=\"images/posts/vector-memes-star-trek-results.png\" >}}\n\nIt is really fun to click around.\n\n### Namespaces?\n\nThe magic isn't clicking on an image and getting a similar image. That is cool, but wasn't \"holy shit\" for me.\n\nWhat blew my mind was using the same model to encode the search text into vectors and finding images similar to the text.\n\nFor whatever reason, this screws up my brain. It is one thing to have a neat semantic like search for images based on another images. Being able to have a nice multi modal interface really made it like a magic trick.\n\nHere are some examples:\n\nSearching for **money**. I grab the encoding for money and pass the vectors to chroma. The results for **money** are:\n{{< image src=\"images/posts/vector-memes-money.png\" >}}\n\nSearching for **AI**\n{{< image src=\"images/posts/vector-memes-ai.png\" >}}\n\nSearching for **red** (a dozy! Is it a color? Is a lifestyle? Is it Russia?)\n{{< image src=\"images/posts/vector-memes-red.png\" >}}\n\nSo on and so forth. Forever. It is magical. You can find all sorts of gems you forgot about. Oh shit I need a meme about writing a blog post:\n{{< image src=\"images/posts/vector-memes-writing-meme.jpg\" >}}\n\n(I am self aware, I just don't care - lol)\n\n### How does it work with a photo library?\n\nIt works super well.\n\nI highly recommend running this against your photo library. To get started, I downloaded my google photos takeout archive. Extracted it onto an external disk. I had to run a few scripts against it to make it usable (Whoever designed the google photos takeout is very excited about duplicate data). I then pointed the script at that directory instead of my memes folder and let 'er rip.\n\nI had about 140k photos and it took about 6 hours to run through. Not so bad. The results are incredible.\n\n#### Here are some fun examples:\n\nObviously these are similar (I also have a dupe problem in google photos)\n{{< image src=\"images/posts/vector-memes-harper.png\" >}}\n\nWe have had a lot of poodles. Here are some\n{{< image src=\"images/posts/vector-memes-poodles.png\" >}}\n\nYou can search for landmarks. I had no idea I had taken a photo of fuji-san from a plane!\n{{< image src=\"images/posts/vector-memes-fuji-results.png\" >}}\n\nAnd then find similar images of Mt Fuji.\n{{< image src=\"images/posts/vector-memes-fuji-similar.png\" >}}\n\nIt is pretty easy to search for places.\n{{< image src=\"images/posts/vector-memes-chicago.png\" >}}\n\nOr emotions. I am apparently surprising so I have a lot of surprised photos.\n{{< image src=\"images/posts/vector-memes-surprised.png\" >}}\n\nAlso niche things like low riders. (These are from Shibuya!)\n{{< image src=\"images/posts/vector-memes-low-riders.png\" >}}\n\nAnd you can use it to find things that are not easy to find or search for. Like bokeh.\n{{< image src=\"images/posts/vector-memes-bokeh.png\" >}}\n\nIt's wonderful, because I can click through and find great images I had forgotten about. Like this great photo of Baratunde that I took in 2017:\n\n{{< image src=\"images/posts/vector-memes-baratunde.png\" >}}\n\n### This will be everywhere\n\nI imagine that we will see this tech rolled into all the various photo apps shortly. Google Photos probably already does this, but they have googled it so much that nobody notices.\n\nThis is too good to not roll into whatever photo app you use. If I had any large scale product that used photos or images, I would immediately set up a pipeline to start encoding the images to see what kind of weird features this unlocks.\n\n## YOU CAN USE THIS FOR THE LOW PRICE OF FREE\n\nI put the source here: [harperreed/photo-similarity-search](https://github.com/harperreed/photo-similarity-search).\n\nPlease check it out.\n\nIt is pretty straight forward to get going. It is a bit hacky. lol.\n\nI would use conda or something similar to keep things clean. The interface is simple tailwind. The web is flask. The code is python. I am your host, harper reed.\n\n## My challenge for you!\n\nPlease build an app that I can use to catalog my photo library in a nice way. I don't want to upload to another destination. I want to have a simple Mac app that I can point to my photo library and say \"crawl this.\" I imagine a lot of neat stuff could be added:\n\n- Llava/Moondream auto captioning\n- Keywords / Tags\n- Vector similarity\n- etc\n\nIt should run locally. Be a native app. Be simple, and effective. Maybe plug into Lightroom, capture one, or apple photos.\n\nI want this. Build it. Let's discover all the amazing photos we have taken through the magic of AI.\n\n## Extra Credit: Light Room Preview JPEG Recovery\n\nMy hacking buddy Ivan was around while I was building this. He immediately saw the magic of what a person could discover by using this on their photo library. He wanted to use it immediately.\n\nHis photo catalog is on an external hard drive - but he had his Lightroom preview file locally. He wrote a quick script to extract the thumbnails and metadata from the preview file and save it to an external disk.\n\nWe then ran the image vector crawler and BAM - he could see similar images and what not. Worked perfectly.\n\n#### Recover your Lightroom photos. Or at least the thumbnails.\n\nIvan's simple script to extract the images from the preview file is really awesome. If you have ever lost your real photo library (corrupt harddrive, or whatever) and you still have the lrpreview file - this script can help you extract at least the lower res version.\n\nA super handy script to keep around.\n\nYou can check it out here: [LR Preview JPEG Extractor](https://github.com/ibips/lrprev-extract).\n\n## Thanks for reading.\n\nAs always, [hmu](mailto:harper@modest.com) and let's hang out. I am thinking a lot about AI, Ecommerce, photos, hifi, hacking and other shit.\n\nIf you are in Chicago come hang out.\n\n# CURRENT TRANSLATION\n## CLIP/SigLIP と画像ベクトルエンコーディング入門\n\n_tl;dr_: SigLIP／CLIP を使ってミーム検索エンジンを作った。クソ楽しかったし、得るものも山ほどあった。\n\nしばらく実用寄りの AI ツールを作り散らかしているが、その中でもいちばん “魔法” っぽいのがベクトル埋め込みだ。[Word2Vec](https://en.wikipedia.org/wiki/Word2vec) を初めて触ったときはほんとにブッ飛んだ。\n\n先日、[Hacker News で見かけたシンプルなアプリ](https://news.ycombinator.com/item?id=39392582)が[マジで凄かった](https://mood-amber.vercel.app/)。誰かが Tumblr の画像をゴッソリかき集め、[SigLIP](https://arxiv.org/abs/2303.15343) で埋め込みを作り、「画像をクリックすると似た画像が並ぶ」だけのアプリを出していた。まるで魔法。しかも手が届きそうに見えた。\n\nこの勢いで「全部理解してやるか」と走り始めた。\n\n## wut（何それ？）\n\nベクトル埋め込みとか CLIP／SigLIP、マルチモーダル埋め込み、ベクトルデータベース……初見でもビビる必要なし。\n\nあの HN のハックを見るまでは、俺もベクトルのことなんて深く考えたことなかった。FAISS（Facebook 製のシンプルなベクトル DB）や Pinecone（$$）を “動けば OK” で使った程度。テスト通ったら「はい終わり」。\n\nぶっちゃけ今でも「ベクトルって何だよ」って思ってる。Lol.　RAG とか LLM 以外でどう使うかも、今回作るまでピンと来てなかった。\n\n俺は「作りながら学ぶ」派だ。結果が面白ければ燃えるし、今回はほんと魔法だった。\n\n### WTF 用語集\n\n公開前に友人に読んでもらったら「X って何？」が多発したので、当時ほぼ初見だったキーワードをざっくりまとめておく。\n\n- **Vector Embeddings（ベクトル埋め込み）** — テキストや画像を数値ベクトルに変換し、似たもの検索を爆速にする仕組み。  \n- **Vector Database（ベクトルデータベース）** — エンコード済みベクトルを保存・検索できる DB。  \n- **Word2Vec** — 単語をベクトル化し、類似語や関係性を探れる革命的手法。  \n- **CLIP** — OpenAI のモデル。画像とテキストを同じベクトル空間に押し込む。  \n- **OpenCLIP** — CLIP の OSS 実装。誰でも使い放題。  \n- **FAISS** — 大量ベクトルを管理して近傍検索を激速にするライブラリ。  \n- **ChromaDB** — 画像・テキストのベクトルを保存し、似た結果を秒速で返す DB。  \n- **MessagePack** — 軽量バイナリ形式。ここではファイル一覧の保存に使用。\n\n## シンプルにいこうぜ、harper\n\nこれはただの簡単ハックだ。テキトーにいじってるだけだからスケールなんて気にしない。でも **あなた** が再現できることは重視した。\n\n目標その 1：ぜんぶ Mac ローカルで回す。せっかくの Apple シリコン GPU をフル回転させよう。\n\nまずは画像ディレクトリをクロールするクソ雑なクローラを書く。Apple Photos 派なので手元に写真フォルダがない。そこで秘密のミームチャットから 1 万枚超のミームをエクスポートしてテスト画像にした。\n\n### クローラ\n\n世界一ひどいクローラを書いた。いや正確には、俺の指示で Claude が書いた “世界一ひどいクローラ” だ。\n\n流れはこんな感じ：\n\n1. 対象ディレクトリのファイル一覧を取得  \n2. その一覧を MessagePack に保存  \n3. MessagePack を読みつつ全画像を走査し、SQLite にメタデータを突っ込む  \n   - ハッシュ  \n   - ファイルサイズ  \n   - パス（location）  \n4. SQLite を回しながら CLIP で各画像をエンコード  \n5. 得たベクトルをまた SQLite に書き戻す  \n6. さらに SQLite を回し、ベクトル＋画像パスを ChromaDB に投入  \n7. 完了  \n\nムダ多すぎるのは百も承知。画像→埋め込み→ChromaDB に一発で突っ込めばいい。でもこんな構成にした理由は：\n\n- ミームのあと 140k 枚クロールしたから、途中で落ちても再開できる堅牢さが欲しかった  \n- 停電やクラッシュでも途中から走らせたかった  \n- ループ回すの大好き  \n\n多少ゴチャついてても結果は優秀。20 万枚超えてもトラブルなしでクロールできた。\n\n### 埋め込みシステム\n\n画像エンコードは楽しい。\n\nまず SigLIP で [シンプルな Web サービス](https://github.com/harperreed/imbedding) を立てた。スタジオの GPU マシンで回したら速いとは言えないが、ローカルの [OpenCLIP](https://github.com/mlfoundations/open_clip) よりははるかに速かった。\n\nそれでもローカル完結させたい。そこで Apple の [ml-explore](https://github.com/ml-explore/) リポジトリを思い出す。中にある [CLIP 実装](https://github.com/ml-explore/mlx-examples/tree/main/clip) が爆速。大きめモデルでも RTX 4090 より速い。ワイルドすぎるだろ。\n\nあとはスクリプトから簡単に呼び出せれば OK。\n\n### MLX_CLIP\n\nClaude と一緒にサンプルをいじって、どの Mac でも動く小さな Python クラスにした。モデルがなければ自動で DL＆変換、即推論。\n\nリポジトリはこちら: <https://github.com/harperreed/mlx_clip>\n\n出来栄えにけっこう満足。Apple シリコン、マジで速い。\n\n使い方はこんな感じ：\n\n```python\nimport mlx_clip\n\n# モデルを初期化\nclip = mlx_clip.mlx_clip(\"openai/clip-vit-base-patch32\")\n\n# 画像をエンコード\nimage_embeddings = clip.image_encoder(\"assets/cat.jpeg\")\nprint(image_embeddings)\n\n# テキストをエンコード\ntext_embeddings = clip.text_encoder(\"a photo of a cat\")\nprint(text_embeddings)\n```\n\nSigLIP 版も動かしたいけど、これは POC だし保守する気はない。SigLIP で動かすヒントあったら [hmu](mailto:harper@modest.com)。OpenCLIP を再発明したくはないし、理論上 Apple シリコンでも走るはずなんだよね。\n\n### さて次は\n\n画像ベクトルを ChromaDB にブチ込んだら、あとは UI。ChromaDB の query を叩けば類似画像が取れる。\n\n基準画像のベクトルを抜いて ChromaDB に投げる → 類似度順に画像 ID が返る。\n\nフロントは Tailwind、サーバは Flask でさくっと包んで完成。  \n2015 年なら何人月かかったかわからんが、今回はトータル 10 時間。笑うしかない。\n\n結果はガチで魔法。\n\n### ミーム概念検索\n\n最初のデータセットはミーム。約 12,000 枚。\n\nまずこれ：\n\n{{< image src=\"images/posts/vector-memes-bowie.png\" caption=\"So true\" >}}\n\nエンコードして ChromaDB に投げると……\n\n{{< image src=\"images/posts/vector-memes-bowie-results.png\" >}}\n\nもう一例：\n\n{{< image src=\"images/posts/vector-memes-star-trek.png\" >}}\n\n返ってくるのがこれ：\n\n{{< image src=\"images/posts/vector-memes-star-trek-results.png\" >}}\n\nクリックしてるだけで無限に遊べる。\n\n### 名前空間って？\n\n「画像クリック→類似画像」はクールだけど、俺の脳を吹き飛ばしたのは “同じモデルで検索テキストもベクトル化し、似た画像を返せる” ことだ。テキスト→画像のマルチモーダル検索は完全に手品。\n\nいくつか例を挙げよう。\n\n**money** で検索：  \n{{< image src=\"images/posts/vector-memes-money.png\" >}}\n\n**AI** で検索：  \n{{< image src=\"images/posts/vector-memes-ai.png\" >}}\n\n**red** で検索（色？ ライフスタイル？ それともロシア？）：  \n{{< image src=\"images/posts/vector-memes-red.png\" >}}\n\n延々と遊べる。忘れてたお宝ミームもザクザク。  \n「ブログ執筆のミーム欲しいな」と思ったら——  \n{{< image src=\"images/posts/vector-memes-writing-meme.jpg\" >}}\n\n（自覚あるけど気にしない w）\n\n### 写真ライブラリで試すと？\n\nめちゃくちゃいい感じに動く。\n\n自分のフォトライブラリで試すのを激しくオススメする。俺は Google Photos の Takeout を落として外付けディスクに展開。重複だらけだからスクリプトで整理し、ミームフォルダの代わりにそのディレクトリをクロール。\n\n写真 140k 枚で約 6 時間。悪くない。結果は最高。\n\n#### 例いろいろ\n\nそっくり写真（Google Photos の重複問題はご愛敬）  \n{{< image src=\"images/posts/vector-memes-harper.png\" >}}\n\nうちのプードルたち  \n{{< image src=\"images/posts/vector-memes-poodles.png\" >}}\n\nランドマーク検索：飛行機から撮った富士山に自分でも気づいてなかった  \n{{< image src=\"images/posts/vector-memes-fuji-results.png\" >}}\n\nそこから類似の富士山写真  \n{{< image src=\"images/posts/vector-memes-fuji-similar.png\" >}}\n\n場所もラクラク  \n{{< image src=\"images/posts/vector-memes-chicago.png\" >}}\n\n感情系。「驚き顔」がこんなにあったとは  \n{{< image src=\"images/posts/vector-memes-surprised.png\" >}}\n\nニッチなテーマ、ローライダー（渋谷で撮影）  \n{{< image src=\"images/posts/vector-memes-low-riders.png\" >}}\n\n検索しづらい「ボケ味」も一発  \n{{< image src=\"images/posts/vector-memes-bokeh.png\" >}}\n\n忘れてた名ショットも救出。2017 年に撮った Baratunde：  \n{{< image src=\"images/posts/vector-memes-baratunde.png\" >}}\n\n### これはそのうち全部のアプリに入る\n\n近いうちに主要フォトアプリは絶対この機能を積む。Google Photos にはもう入ってるかもしれないけど、Google がゴチャゴチャ手を入れすぎて誰も気づいてないだけかも。\n\n画像を扱うプロダクトを持ってるなら、今すぐパイプライン組んでエンコードを始めた方がいい。\n\n## YOU CAN USE THIS FOR THE LOW PRICE OF FREE\n\nソースはここ: <https://github.com/harperreed/photo-similarity-search>\n\nぜひ触ってみてくれ。ちょっとハック感あるけど動く。  \nconda などで環境を分けると楽。UI は Tailwind、サーバは Flask、コードは Python。ホストは harper reed でした。\n\n## キミへのチャレンジ！\n\n俺のフォトライブラリを快適にカタログ化できる Mac ネイティブアプリを作ってくれ。クラウドに上げるのはイヤ。ライブラリを指定して「クロール開始」押すだけ、みたいなのが理想。\n\n- Llava／Moondream で自動キャプション  \n- キーワード／タグ付け  \n- ベクトル類似検索  \n- etc.  \n\n全部ローカルで動いて、シンプルで強力。Lightroom、Capture One、Apple Photos と連携できたら最高だ。\n\nこれ、マジで欲しい。誰か作って！\n\n## 追加課題：Lightroom プレビュー JPEG 復旧\n\nハック仲間の Ivan もこの魔法を見て即参戦。彼の写真は外付け HDD にあるが、Lightroom のプレビューファイルはローカルにあった。そこでサムネとメタデータを抜き出して外付けに保存するスクリプトを書いた。\n\nそのあと画像ベクトルクローラを回したら類似検索も問題なく動いた。BAM。\n\n#### Lightroom の写真を救出 — サムネだけでも\n\nIvan のシンプルスクリプトは超便利。もし本体ライブラリが吹っ飛んでも .lrprev が残っていれば低解像度版だけでも救出できる。\n\nリポジトリ: <https://github.com/ibips/lrprev-extract>\n\n## Thanks for reading.\n\nいつでも [hmu](mailto:harper@modest.com)（DM ちょうだい）。AI、EC、写真、Hi-Fi、ハック、何でも語ろう。  \nシカゴにいるならぜひ遊びに来て。\n\nPlease critique this translation mercilessly and provide detailed feedback on what needs to be improved.\nFormat your critique as a structured list of issues, organized by severity and category.\nInclude specific suggestions for how to fix each issue.",
      "response": "【総評】  \n大筋では原文の意味とノリをよく再現できているものの、①情報を“追加”してしまった箇所、②細かな抜け・意味のずれ、③トーンがやや行き過ぎ／足りない箇所、④日本語としてのぎこちなさ──が点在する。とくに「追加情報」は忠実性の観点でクリティカル。以下、重大度順に列挙する。\n\n────────────────\n■Critical（意味改変・追加／欠落）\n────────────────\n1. 追加情報「MessagePack」  \n   • 原文の用語一覧には MessagePack はない。訳文では独自に項目を立て説明を付け足している。  \n   →削除するか、脚注扱いで「訳注: 原文にはないが…」と明示。\n\n2. “it is way better than CLIP” の欠落  \n   • 原文: “I prefer that model (it is way better than CLIP).”  \n   • 訳文: “SigLIP 版も動かしたいけど～保守する気はない。”  \n   →「CLIP よりはるかに良いと思っている」という評価を補う。\n\n3. “This was incredible.” の欠落  \n   • 原文: “I then wrapped it all up in a tailwind/flask app. This was incredible.”  \n   →「これには自分でも驚いた」等を追加。\n\n4. “I had no idea how to achieve this” の弱体化  \n   • 訳文では「手が届きそうに見えた」だけで、“全くやり方が分からなかった” という驚きが薄い。  \n   →「どうやればいいか皆目見当もつかなかったけど～」と補強。\n\n5. Apple-silicon 速度記述の一部欠落  \n   • “I know most people know this, but…” が訳出されていない。  \n   →「言わずもがなだけど～」を入れる。\n\n────────────────\n■Major（トーン・スタイル・語法）\n────────────────\n6. 一人称「俺」・罵語「クソ」等の強さ調整  \n   • 原文の “fucking”, “shit” を訳文ではほぼ「クソ」で統一しており、やや連発気味。  \n   →緩急を付けて「クソ→めちゃ→死ぬほど」など言い換え、リズムを整える。\n\n7. 「作り散らかしている」  \n   • 口語としてやや不自然。  \n   →「作りまくっている／量産している」など。\n\n8. 「ワイルドすぎるだろ」  \n   • “Wildstyle.” をどう訳すかは悩ましいが、ここでは「ワイルドすぎる」より「意味がわからんほど速い」などの方が自然。\n\n9. 「名前空間って？」  \n   • 原文 “Namespaces?” は半ば冗談。直訳だと誤解を招く。  \n   →「『Namespaces?』って何だよ、みたいな話」等で冗談感を残す。\n\n10. 見出しの “Or:” の処理  \n    • 原文タイトルの “Or: how to …” という肩書き的ニュアンスが消えた。  \n    →「別名：CLIP/SigLIP と…」などで拾う。\n\n────────────────\n■Minor（表記ゆれ・リズム・細部）\n────────────────\n11. SigLIP の大小表記ゆれ  \n    • 訳文では “SigLIP／SigLIP 版” と混在。全て “SigLIP” 統一を推奨。\n\n12. 半角全角スペース  \n    • “Lol.　RAG” など、句点後に全角空白が残る箇所あり →削除。\n\n13. “ベクトル埋め込みとか CLIP／SigLIP、マルチモーダル埋め込み…”  \n    • “とか” が重複しリズムが悪い →「ベクトル埋め込み、CLIP／SigLIP などのマルチモーダル埋め込み…」等。\n\n14. 「BAM—」のダッシュ  \n    • 全角ダッシュ “—” が環境によっては化ける。短い破線 “–” か “──” 等に統一。\n\n15. 「ローライダー」  \n    • “low riders” は複数形なので「ローライダーたち／ローライダーの車」など文脈合わせ。\n\n16. 「hmu」を毎回英語のまま  \n    • 初回に（hit me up）と訳注、一部は「連絡ちょうだい」でいい。\n\n17. Light Room → Lightroom  \n    • 原文のスペルミスを訳で直すのは OK。ただ訳注で「原文では Light Room となっているが実際は Lightroom」等を添えると親切。\n\n18. “YOU CAN USE THIS FOR THE LOW PRICE OF FREE” を英語のまま残置  \n    • カタカナ or 日本語に＋原文見出し併記例：「無料で使えるヨ！」（原文見出し: “YOU CAN USE…”）など。\n\n────────────────\n■Stylistic / 流れの改善提案\n────────────────\n・見出し階層が多少ズレている（例：### ミーム概念検索 → “Memes concept search” の訳）。段落番号を合わせて可読性を保つ。  \n・箇条書き内で「—」区切りと「:」が混在。どちらかに統一。  \n・Fコードなどのコメント行を日本語もしくは原文併記で簡単に説明すると読者フレンドリー。\n\n────────────────\n■まとめ\n────────────────\n1) 追加してしまった MessagePack 項目を削除（または訳注）。  \n2) 欠落した評価コメント・驚き表現を補完。  \n3) 俗語の使い方を抑揚／語彙の多様化で自然に。  \n4) タイトル・見出しのニュアンスを原文どおりに。  \n5) 細かな表記ゆれ・スペース・破線を修正。\n\n以上を直せば、「原文らしい勢い」と「日本語としての滑らかさ」のバランスが取れた完訳になる。",
      "usage": {
        "prompt_tokens": 7724,
        "completion_tokens": 1469,
        "total_tokens": 9193
      },
      "streaming": true
    },
    "feedback": {
      "model": "o3",
      "target_language": "japanese",
      "system_prompt": "You are a master translator and editor specializing in japanese.\nYour task is to improve a translation based on detailed critique feedback.\n\n1. Carefully read the original text, current translation, and the critique feedback\n2. Address ALL issues identified in the critique\n3. Apply the specific suggestions for improvement\n4. Ensure the translation is accurate, natural-sounding, and faithful to the original\n5. Preserve all formatting, markdown, and structure of the original text\n6. Make sure the final text reads as if it were originally written in japanese\n\nDo not ignore any of the critique points. Every issue identified must be addressed in your improved version.\n",
      "user_prompt": "# ORIGINAL TEXT\n## Or: how to learn about clip/siglip and vector encoding images\n\n_tl;dr_: I built a meme search engine using siglip/CLIP and vector encoding images. It was fun and I learned a lot.\n\nI have been building a lot of applied AI tools for a while. One of the components that always seemed the most magical has always been vector embeddings. [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) and the like have straight blown my mind. It is like magic.\n\nI saw a [simple app on hacker news](https://news.ycombinator.com/item?id=39392582) that was [super impressive](https://mood-amber.vercel.app/). Someone crawled a bunch of Tumblr images and used [siglip](https://arxiv.org/abs/2303.15343) to get the embeddings and then made a simple \"click the image and see similar images\" app. It was like magic. I had no idea how to achieve this, but it seemed accessible.\n\nI decided to use my sudden motivation as an opportunity to learn how \"all this works.\"\n\n## wut\n\nIf you have never ran into vector embeddings, clip/siglip, vector databases, and the like - never fear.\n\nBefore I saw the hack on hn I really didn’t think much about vector embeddings, multi modal embeddings or vector datastores. I had used faiss (facebooks simple vector store), and Pinecone ($$) for some hacks, but didn’t really dig in. Just got it to work and then was like “yep. Tests pass.”\n\nI still barely know what vectors are. Lol. Before I dug in and built this, I really didn’t understand how I would use it outside of RAG or another LLM process.\n\nI learn by building. It helps if the results are really intriguing, and in this case kind of magical.\n\n### WTF terms\n\nI had a few friends read this over before publishing and a couple were like “wtf is X?” Here is a short list of terms that were largely new to me:\n\n- **Vector Embeddings** - Vector embeddings convert your text of images into numerical representations, allowing you to find similar pics and search your library effectively.\n- **Vector Database** - A vector database is a way to store and search through encoded items, enabling you to find similar items.\n- **Word2Vec** - Word2Vec is a groundbreaking technique that converts words into numerical vectors, enabling you to perform tasks like finding similar words and exploring relationships between them.\n- **CLIP** - CLIP is OpenAI's model that encodes images and text into numerical vectors.\n- **OpenCLIP** - OpenCLIP is an open-source implementation of OpenAI's CLIP model, allowing anyone to use and build upon this powerful image and text encoding technology without the need for special access or permissions.\n- **FAISS** - FAISS is an efficient library for managing and searching through large collections of image vectors, making it fast and easy to find the images you're looking for.\n- **ChromaDB** - ChromaDB is a database that stores and retrieves your image and text vectors, quickly returning similar results for your searches.\n\n## Keep it simple, harper.\n\nThis is a pretty straight forward hack. I am just fucking around so I wasn't super interested in making it scalable. I did have an interest in making it replicable. I wanted to make something that **you** could run without a lot of work.\n\nOne of my goals was to make sure everything runs locally to my laptop. We have these fancy Mac GPUs - let's heat them up.\n\nThe first step was building out a simple crawler that would crawl a directory of images. I use Apple Photos, so I didn't have a directory full of my photos laying around. I did, however, have a giant bucket of memes from my precious and very secret meme chat group (don't tell anyone). I exported the chat, moved the images to a directory and BAM - I had my test image set.\n\n### The Crawler\n\nI created the world's worst crawler. Well. I should be honest: Claude created the world's worst crawler with my instructions.\n\nIt is a bit complicated but here are the steps:\n\n1. It gets the file list of the target directory\n2. It stores the list in a msgpack file\n3. I reference the msgpack file and then iterate through every image and store it in a sqlite db. Grabing some metadata about the file\n    - hash\n    - filesize\n    - location\n4. I iterate through that sqlite db and then use CLIP to get the vector encoding of every image.\n5. Then I store those vectors back in the sqlite db\n6. Then I iterate through the sqlite db and insert the vectors and image path into chroma vector db\n7. Then we are done\n\nThis is a lot of wasted work. You could iterate through the images, grab the embeddings and slam it into chroma (I chose chroma cuz it is easy, free, and no infra).\n\nI have built it this way because:\n\n- After the memes, I crawled 140k images and wanted it to be resilient to crashing.\n- I needed it to be able to resume building out the databases in case it crashed, power went out, etc\n- I really like loops\n\nRegardless of the extra complexity, it worked flawlessly. I have crawled over 200k images without a blip.\n\n### An embedding system\n\nEncoding the images was fun.\n\nI started with siglip and created a [simple web service](https://github.com/harperreed/imbedding) where we could upload the image and get the vectors back. This ran on one of our GPU boxes at the studio and worked well. It wasn't fast, but it was way faster than running [open clip](https://github.com/mlfoundations/open_clip) locally.\n\nI still wanted to run it locally. I remembered that the [ml-explore](https://github.com/ml-explore/) repo from apple had some neat examples that could help. And BAM they had a [clip implementation](https://github.com/ml-explore/mlx-examples/tree/main/clip) that was fast af. Even using the larger model, it was faster than the 4090. Wildstyle.\n\nI just needed to make it easy to use in my script.\n\n### MLX_CLIP\n\nClaude and I were able to coerce the example script from apple into a fun lil python class that you can use locally on any of your machines. It will download the models if they don't exist, convert them, and then use them in flight with your script.\n\nYou can check it out here: https://github.com/harperreed/mlx_clip\n\nI am pretty chuffed with how well it turned out. I know most people know this, but the apple silicon is fast af.\n\nIt turned out to be rather simple to use:\n\n```python\nimport mlx_clip\n\n# Initialize the mlx_clip model with the given model name.\nclip = mlx_clip.mlx_clip(\"openai/clip-vit-base-patch32\")\n\n# Encode the image from the specified file path and obtain the image embeddings.\nimage_embeddings = clip.image_encoder(\"assets/cat.jpeg\")\n# Print the image embeddings to the console.\nprint(image_embeddings)\n\n# Encode the text description and obtain the text embeddings.\ntext_embeddings = clip.text_encoder(\"a photo of a cat\")\n# Print the text embeddings to the console.\nprint(text_embeddings)\n```\n\nI would love to get this to work with siglip, as I prefer that model (it is way better than CLIP). However, this is a POC more than a product I want to maintain. If anyone has any hints on how to get it working with siglip - [hmu](mailto:harper@modest.com). I don't want to reinvent open clip - which should theoretically run well on apple silicon, and is very good.\n\n### Now what\n\nNow that we had all the image vectors slammed into the vector datastore we could get started with the interface. I used the built in query functionality of chromadb to show similar images.\n\nGrab the vectors of the image you are starting with. Query those vectors with chromadb. Chromed returns a list of image ids that are similar in declining similarity.\n\nI then wrapped it all up in a tailwind/flask app.\nThis was incredible.\n\nI can't imagine the amount of work we would have done in 2015 to build this. I spent maybe 10 hours total on this and it was trivial.\n\nThe results are akin to magic.\n\n### Memes concept search\n\nNow remember, I used memes as my initial set of images. I had 12000 memes to search through.\n\nStart with this:\n\n{{< image src=\"images/posts/vector-memes-bowie.png\" caption=\"So true\" >}}\n\nEncode it, pass it to chroma to return similar results.\n\nAnd then similar images that return are like this:\n{{< image src=\"images/posts/vector-memes-bowie-results.png\" >}}\n\nAnother example:\n{{< image src=\"images/posts/vector-memes-star-trek.png\" >}}\n\nGives you results like:\n{{< image src=\"images/posts/vector-memes-star-trek-results.png\" >}}\n\nIt is really fun to click around.\n\n### Namespaces?\n\nThe magic isn't clicking on an image and getting a similar image. That is cool, but wasn't \"holy shit\" for me.\n\nWhat blew my mind was using the same model to encode the search text into vectors and finding images similar to the text.\n\nFor whatever reason, this screws up my brain. It is one thing to have a neat semantic like search for images based on another images. Being able to have a nice multi modal interface really made it like a magic trick.\n\nHere are some examples:\n\nSearching for **money**. I grab the encoding for money and pass the vectors to chroma. The results for **money** are:\n{{< image src=\"images/posts/vector-memes-money.png\" >}}\n\nSearching for **AI**\n{{< image src=\"images/posts/vector-memes-ai.png\" >}}\n\nSearching for **red** (a dozy! Is it a color? Is a lifestyle? Is it Russia?)\n{{< image src=\"images/posts/vector-memes-red.png\" >}}\n\nSo on and so forth. Forever. It is magical. You can find all sorts of gems you forgot about. Oh shit I need a meme about writing a blog post:\n{{< image src=\"images/posts/vector-memes-writing-meme.jpg\" >}}\n\n(I am self aware, I just don't care - lol)\n\n### How does it work with a photo library?\n\nIt works super well.\n\nI highly recommend running this against your photo library. To get started, I downloaded my google photos takeout archive. Extracted it onto an external disk. I had to run a few scripts against it to make it usable (Whoever designed the google photos takeout is very excited about duplicate data). I then pointed the script at that directory instead of my memes folder and let 'er rip.\n\nI had about 140k photos and it took about 6 hours to run through. Not so bad. The results are incredible.\n\n#### Here are some fun examples:\n\nObviously these are similar (I also have a dupe problem in google photos)\n{{< image src=\"images/posts/vector-memes-harper.png\" >}}\n\nWe have had a lot of poodles. Here are some\n{{< image src=\"images/posts/vector-memes-poodles.png\" >}}\n\nYou can search for landmarks. I had no idea I had taken a photo of fuji-san from a plane!\n{{< image src=\"images/posts/vector-memes-fuji-results.png\" >}}\n\nAnd then find similar images of Mt Fuji.\n{{< image src=\"images/posts/vector-memes-fuji-similar.png\" >}}\n\nIt is pretty easy to search for places.\n{{< image src=\"images/posts/vector-memes-chicago.png\" >}}\n\nOr emotions. I am apparently surprising so I have a lot of surprised photos.\n{{< image src=\"images/posts/vector-memes-surprised.png\" >}}\n\nAlso niche things like low riders. (These are from Shibuya!)\n{{< image src=\"images/posts/vector-memes-low-riders.png\" >}}\n\nAnd you can use it to find things that are not easy to find or search for. Like bokeh.\n{{< image src=\"images/posts/vector-memes-bokeh.png\" >}}\n\nIt's wonderful, because I can click through and find great images I had forgotten about. Like this great photo of Baratunde that I took in 2017:\n\n{{< image src=\"images/posts/vector-memes-baratunde.png\" >}}\n\n### This will be everywhere\n\nI imagine that we will see this tech rolled into all the various photo apps shortly. Google Photos probably already does this, but they have googled it so much that nobody notices.\n\nThis is too good to not roll into whatever photo app you use. If I had any large scale product that used photos or images, I would immediately set up a pipeline to start encoding the images to see what kind of weird features this unlocks.\n\n## YOU CAN USE THIS FOR THE LOW PRICE OF FREE\n\nI put the source here: [harperreed/photo-similarity-search](https://github.com/harperreed/photo-similarity-search).\n\nPlease check it out.\n\nIt is pretty straight forward to get going. It is a bit hacky. lol.\n\nI would use conda or something similar to keep things clean. The interface is simple tailwind. The web is flask. The code is python. I am your host, harper reed.\n\n## My challenge for you!\n\nPlease build an app that I can use to catalog my photo library in a nice way. I don't want to upload to another destination. I want to have a simple Mac app that I can point to my photo library and say \"crawl this.\" I imagine a lot of neat stuff could be added:\n\n- Llava/Moondream auto captioning\n- Keywords / Tags\n- Vector similarity\n- etc\n\nIt should run locally. Be a native app. Be simple, and effective. Maybe plug into Lightroom, capture one, or apple photos.\n\nI want this. Build it. Let's discover all the amazing photos we have taken through the magic of AI.\n\n## Extra Credit: Light Room Preview JPEG Recovery\n\nMy hacking buddy Ivan was around while I was building this. He immediately saw the magic of what a person could discover by using this on their photo library. He wanted to use it immediately.\n\nHis photo catalog is on an external hard drive - but he had his Lightroom preview file locally. He wrote a quick script to extract the thumbnails and metadata from the preview file and save it to an external disk.\n\nWe then ran the image vector crawler and BAM - he could see similar images and what not. Worked perfectly.\n\n#### Recover your Lightroom photos. Or at least the thumbnails.\n\nIvan's simple script to extract the images from the preview file is really awesome. If you have ever lost your real photo library (corrupt harddrive, or whatever) and you still have the lrpreview file - this script can help you extract at least the lower res version.\n\nA super handy script to keep around.\n\nYou can check it out here: [LR Preview JPEG Extractor](https://github.com/ibips/lrprev-extract).\n\n## Thanks for reading.\n\nAs always, [hmu](mailto:harper@modest.com) and let's hang out. I am thinking a lot about AI, Ecommerce, photos, hifi, hacking and other shit.\n\nIf you are in Chicago come hang out.\n\n# CURRENT TRANSLATION\n## CLIP/SigLIP と画像ベクトルエンコーディング入門\n\n_tl;dr_: SigLIP／CLIP を使ってミーム検索エンジンを作った。クソ楽しかったし、得るものも山ほどあった。\n\nしばらく実用寄りの AI ツールを作り散らかしているが、その中でもいちばん “魔法” っぽいのがベクトル埋め込みだ。[Word2Vec](https://en.wikipedia.org/wiki/Word2vec) を初めて触ったときはほんとにブッ飛んだ。\n\n先日、[Hacker News で見かけたシンプルなアプリ](https://news.ycombinator.com/item?id=39392582)が[マジで凄かった](https://mood-amber.vercel.app/)。誰かが Tumblr の画像をゴッソリかき集め、[SigLIP](https://arxiv.org/abs/2303.15343) で埋め込みを作り、「画像をクリックすると似た画像が並ぶ」だけのアプリを出していた。まるで魔法。しかも手が届きそうに見えた。\n\nこの勢いで「全部理解してやるか」と走り始めた。\n\n## wut（何それ？）\n\nベクトル埋め込みとか CLIP／SigLIP、マルチモーダル埋め込み、ベクトルデータベース……初見でもビビる必要なし。\n\nあの HN のハックを見るまでは、俺もベクトルのことなんて深く考えたことなかった。FAISS（Facebook 製のシンプルなベクトル DB）や Pinecone（$$）を “動けば OK” で使った程度。テスト通ったら「はい終わり」。\n\nぶっちゃけ今でも「ベクトルって何だよ」って思ってる。Lol.　RAG とか LLM 以外でどう使うかも、今回作るまでピンと来てなかった。\n\n俺は「作りながら学ぶ」派だ。結果が面白ければ燃えるし、今回はほんと魔法だった。\n\n### WTF 用語集\n\n公開前に友人に読んでもらったら「X って何？」が多発したので、当時ほぼ初見だったキーワードをざっくりまとめておく。\n\n- **Vector Embeddings（ベクトル埋め込み）** — テキストや画像を数値ベクトルに変換し、似たもの検索を爆速にする仕組み。  \n- **Vector Database（ベクトルデータベース）** — エンコード済みベクトルを保存・検索できる DB。  \n- **Word2Vec** — 単語をベクトル化し、類似語や関係性を探れる革命的手法。  \n- **CLIP** — OpenAI のモデル。画像とテキストを同じベクトル空間に押し込む。  \n- **OpenCLIP** — CLIP の OSS 実装。誰でも使い放題。  \n- **FAISS** — 大量ベクトルを管理して近傍検索を激速にするライブラリ。  \n- **ChromaDB** — 画像・テキストのベクトルを保存し、似た結果を秒速で返す DB。  \n- **MessagePack** — 軽量バイナリ形式。ここではファイル一覧の保存に使用。\n\n## シンプルにいこうぜ、harper\n\nこれはただの簡単ハックだ。テキトーにいじってるだけだからスケールなんて気にしない。でも **あなた** が再現できることは重視した。\n\n目標その 1：ぜんぶ Mac ローカルで回す。せっかくの Apple シリコン GPU をフル回転させよう。\n\nまずは画像ディレクトリをクロールするクソ雑なクローラを書く。Apple Photos 派なので手元に写真フォルダがない。そこで秘密のミームチャットから 1 万枚超のミームをエクスポートしてテスト画像にした。\n\n### クローラ\n\n世界一ひどいクローラを書いた。いや正確には、俺の指示で Claude が書いた “世界一ひどいクローラ” だ。\n\n流れはこんな感じ：\n\n1. 対象ディレクトリのファイル一覧を取得  \n2. その一覧を MessagePack に保存  \n3. MessagePack を読みつつ全画像を走査し、SQLite にメタデータを突っ込む  \n   - ハッシュ  \n   - ファイルサイズ  \n   - パス（location）  \n4. SQLite を回しながら CLIP で各画像をエンコード  \n5. 得たベクトルをまた SQLite に書き戻す  \n6. さらに SQLite を回し、ベクトル＋画像パスを ChromaDB に投入  \n7. 完了  \n\nムダ多すぎるのは百も承知。画像→埋め込み→ChromaDB に一発で突っ込めばいい。でもこんな構成にした理由は：\n\n- ミームのあと 140k 枚クロールしたから、途中で落ちても再開できる堅牢さが欲しかった  \n- 停電やクラッシュでも途中から走らせたかった  \n- ループ回すの大好き  \n\n多少ゴチャついてても結果は優秀。20 万枚超えてもトラブルなしでクロールできた。\n\n### 埋め込みシステム\n\n画像エンコードは楽しい。\n\nまず SigLIP で [シンプルな Web サービス](https://github.com/harperreed/imbedding) を立てた。スタジオの GPU マシンで回したら速いとは言えないが、ローカルの [OpenCLIP](https://github.com/mlfoundations/open_clip) よりははるかに速かった。\n\nそれでもローカル完結させたい。そこで Apple の [ml-explore](https://github.com/ml-explore/) リポジトリを思い出す。中にある [CLIP 実装](https://github.com/ml-explore/mlx-examples/tree/main/clip) が爆速。大きめモデルでも RTX 4090 より速い。ワイルドすぎるだろ。\n\nあとはスクリプトから簡単に呼び出せれば OK。\n\n### MLX_CLIP\n\nClaude と一緒にサンプルをいじって、どの Mac でも動く小さな Python クラスにした。モデルがなければ自動で DL＆変換、即推論。\n\nリポジトリはこちら: <https://github.com/harperreed/mlx_clip>\n\n出来栄えにけっこう満足。Apple シリコン、マジで速い。\n\n使い方はこんな感じ：\n\n```python\nimport mlx_clip\n\n# モデルを初期化\nclip = mlx_clip.mlx_clip(\"openai/clip-vit-base-patch32\")\n\n# 画像をエンコード\nimage_embeddings = clip.image_encoder(\"assets/cat.jpeg\")\nprint(image_embeddings)\n\n# テキストをエンコード\ntext_embeddings = clip.text_encoder(\"a photo of a cat\")\nprint(text_embeddings)\n```\n\nSigLIP 版も動かしたいけど、これは POC だし保守する気はない。SigLIP で動かすヒントあったら [hmu](mailto:harper@modest.com)。OpenCLIP を再発明したくはないし、理論上 Apple シリコンでも走るはずなんだよね。\n\n### さて次は\n\n画像ベクトルを ChromaDB にブチ込んだら、あとは UI。ChromaDB の query を叩けば類似画像が取れる。\n\n基準画像のベクトルを抜いて ChromaDB に投げる → 類似度順に画像 ID が返る。\n\nフロントは Tailwind、サーバは Flask でさくっと包んで完成。  \n2015 年なら何人月かかったかわからんが、今回はトータル 10 時間。笑うしかない。\n\n結果はガチで魔法。\n\n### ミーム概念検索\n\n最初のデータセットはミーム。約 12,000 枚。\n\nまずこれ：\n\n{{< image src=\"images/posts/vector-memes-bowie.png\" caption=\"So true\" >}}\n\nエンコードして ChromaDB に投げると……\n\n{{< image src=\"images/posts/vector-memes-bowie-results.png\" >}}\n\nもう一例：\n\n{{< image src=\"images/posts/vector-memes-star-trek.png\" >}}\n\n返ってくるのがこれ：\n\n{{< image src=\"images/posts/vector-memes-star-trek-results.png\" >}}\n\nクリックしてるだけで無限に遊べる。\n\n### 名前空間って？\n\n「画像クリック→類似画像」はクールだけど、俺の脳を吹き飛ばしたのは “同じモデルで検索テキストもベクトル化し、似た画像を返せる” ことだ。テキスト→画像のマルチモーダル検索は完全に手品。\n\nいくつか例を挙げよう。\n\n**money** で検索：  \n{{< image src=\"images/posts/vector-memes-money.png\" >}}\n\n**AI** で検索：  \n{{< image src=\"images/posts/vector-memes-ai.png\" >}}\n\n**red** で検索（色？ ライフスタイル？ それともロシア？）：  \n{{< image src=\"images/posts/vector-memes-red.png\" >}}\n\n延々と遊べる。忘れてたお宝ミームもザクザク。  \n「ブログ執筆のミーム欲しいな」と思ったら——  \n{{< image src=\"images/posts/vector-memes-writing-meme.jpg\" >}}\n\n（自覚あるけど気にしない w）\n\n### 写真ライブラリで試すと？\n\nめちゃくちゃいい感じに動く。\n\n自分のフォトライブラリで試すのを激しくオススメする。俺は Google Photos の Takeout を落として外付けディスクに展開。重複だらけだからスクリプトで整理し、ミームフォルダの代わりにそのディレクトリをクロール。\n\n写真 140k 枚で約 6 時間。悪くない。結果は最高。\n\n#### 例いろいろ\n\nそっくり写真（Google Photos の重複問題はご愛敬）  \n{{< image src=\"images/posts/vector-memes-harper.png\" >}}\n\nうちのプードルたち  \n{{< image src=\"images/posts/vector-memes-poodles.png\" >}}\n\nランドマーク検索：飛行機から撮った富士山に自分でも気づいてなかった  \n{{< image src=\"images/posts/vector-memes-fuji-results.png\" >}}\n\nそこから類似の富士山写真  \n{{< image src=\"images/posts/vector-memes-fuji-similar.png\" >}}\n\n場所もラクラク  \n{{< image src=\"images/posts/vector-memes-chicago.png\" >}}\n\n感情系。「驚き顔」がこんなにあったとは  \n{{< image src=\"images/posts/vector-memes-surprised.png\" >}}\n\nニッチなテーマ、ローライダー（渋谷で撮影）  \n{{< image src=\"images/posts/vector-memes-low-riders.png\" >}}\n\n検索しづらい「ボケ味」も一発  \n{{< image src=\"images/posts/vector-memes-bokeh.png\" >}}\n\n忘れてた名ショットも救出。2017 年に撮った Baratunde：  \n{{< image src=\"images/posts/vector-memes-baratunde.png\" >}}\n\n### これはそのうち全部のアプリに入る\n\n近いうちに主要フォトアプリは絶対この機能を積む。Google Photos にはもう入ってるかもしれないけど、Google がゴチャゴチャ手を入れすぎて誰も気づいてないだけかも。\n\n画像を扱うプロダクトを持ってるなら、今すぐパイプライン組んでエンコードを始めた方がいい。\n\n## YOU CAN USE THIS FOR THE LOW PRICE OF FREE\n\nソースはここ: <https://github.com/harperreed/photo-similarity-search>\n\nぜひ触ってみてくれ。ちょっとハック感あるけど動く。  \nconda などで環境を分けると楽。UI は Tailwind、サーバは Flask、コードは Python。ホストは harper reed でした。\n\n## キミへのチャレンジ！\n\n俺のフォトライブラリを快適にカタログ化できる Mac ネイティブアプリを作ってくれ。クラウドに上げるのはイヤ。ライブラリを指定して「クロール開始」押すだけ、みたいなのが理想。\n\n- Llava／Moondream で自動キャプション  \n- キーワード／タグ付け  \n- ベクトル類似検索  \n- etc.  \n\n全部ローカルで動いて、シンプルで強力。Lightroom、Capture One、Apple Photos と連携できたら最高だ。\n\nこれ、マジで欲しい。誰か作って！\n\n## 追加課題：Lightroom プレビュー JPEG 復旧\n\nハック仲間の Ivan もこの魔法を見て即参戦。彼の写真は外付け HDD にあるが、Lightroom のプレビューファイルはローカルにあった。そこでサムネとメタデータを抜き出して外付けに保存するスクリプトを書いた。\n\nそのあと画像ベクトルクローラを回したら類似検索も問題なく動いた。BAM。\n\n#### Lightroom の写真を救出 — サムネだけでも\n\nIvan のシンプルスクリプトは超便利。もし本体ライブラリが吹っ飛んでも .lrprev が残っていれば低解像度版だけでも救出できる。\n\nリポジトリ: <https://github.com/ibips/lrprev-extract>\n\n## Thanks for reading.\n\nいつでも [hmu](mailto:harper@modest.com)（DM ちょうだい）。AI、EC、写真、Hi-Fi、ハック、何でも語ろう。  \nシカゴにいるならぜひ遊びに来て。\n\n# CRITIQUE FEEDBACK\n【総評】  \n大筋では原文の意味とノリをよく再現できているものの、①情報を“追加”してしまった箇所、②細かな抜け・意味のずれ、③トーンがやや行き過ぎ／足りない箇所、④日本語としてのぎこちなさ──が点在する。とくに「追加情報」は忠実性の観点でクリティカル。以下、重大度順に列挙する。\n\n────────────────\n■Critical（意味改変・追加／欠落）\n────────────────\n1. 追加情報「MessagePack」  \n   • 原文の用語一覧には MessagePack はない。訳文では独自に項目を立て説明を付け足している。  \n   →削除するか、脚注扱いで「訳注: 原文にはないが…」と明示。\n\n2. “it is way better than CLIP” の欠落  \n   • 原文: “I prefer that model (it is way better than CLIP).”  \n   • 訳文: “SigLIP 版も動かしたいけど～保守する気はない。”  \n   →「CLIP よりはるかに良いと思っている」という評価を補う。\n\n3. “This was incredible.” の欠落  \n   • 原文: “I then wrapped it all up in a tailwind/flask app. This was incredible.”  \n   →「これには自分でも驚いた」等を追加。\n\n4. “I had no idea how to achieve this” の弱体化  \n   • 訳文では「手が届きそうに見えた」だけで、“全くやり方が分からなかった” という驚きが薄い。  \n   →「どうやればいいか皆目見当もつかなかったけど～」と補強。\n\n5. Apple-silicon 速度記述の一部欠落  \n   • “I know most people know this, but…” が訳出されていない。  \n   →「言わずもがなだけど～」を入れる。\n\n────────────────\n■Major（トーン・スタイル・語法）\n────────────────\n6. 一人称「俺」・罵語「クソ」等の強さ調整  \n   • 原文の “fucking”, “shit” を訳文ではほぼ「クソ」で統一しており、やや連発気味。  \n   →緩急を付けて「クソ→めちゃ→死ぬほど」など言い換え、リズムを整える。\n\n7. 「作り散らかしている」  \n   • 口語としてやや不自然。  \n   →「作りまくっている／量産している」など。\n\n8. 「ワイルドすぎるだろ」  \n   • “Wildstyle.” をどう訳すかは悩ましいが、ここでは「ワイルドすぎる」より「意味がわからんほど速い」などの方が自然。\n\n9. 「名前空間って？」  \n   • 原文 “Namespaces?” は半ば冗談。直訳だと誤解を招く。  \n   →「『Namespaces?』って何だよ、みたいな話」等で冗談感を残す。\n\n10. 見出しの “Or:” の処理  \n    • 原文タイトルの “Or: how to …” という肩書き的ニュアンスが消えた。  \n    →「別名：CLIP/SigLIP と…」などで拾う。\n\n────────────────\n■Minor（表記ゆれ・リズム・細部）\n────────────────\n11. SigLIP の大小表記ゆれ  \n    • 訳文では “SigLIP／SigLIP 版” と混在。全て “SigLIP” 統一を推奨。\n\n12. 半角全角スペース  \n    • “Lol.　RAG” など、句点後に全角空白が残る箇所あり →削除。\n\n13. “ベクトル埋め込みとか CLIP／SigLIP、マルチモーダル埋め込み…”  \n    • “とか” が重複しリズムが悪い →「ベクトル埋め込み、CLIP／SigLIP などのマルチモーダル埋め込み…」等。\n\n14. 「BAM—」のダッシュ  \n    • 全角ダッシュ “—” が環境によっては化ける。短い破線 “–” か “──” 等に統一。\n\n15. 「ローライダー」  \n    • “low riders” は複数形なので「ローライダーたち／ローライダーの車」など文脈合わせ。\n\n16. 「hmu」を毎回英語のまま  \n    • 初回に（hit me up）と訳注、一部は「連絡ちょうだい」でいい。\n\n17. Light Room → Lightroom  \n    • 原文のスペルミスを訳で直すのは OK。ただ訳注で「原文では Light Room となっているが実際は Lightroom」等を添えると親切。\n\n18. “YOU CAN USE THIS FOR THE LOW PRICE OF FREE” を英語のまま残置  \n    • カタカナ or 日本語に＋原文見出し併記例：「無料で使えるヨ！」（原文見出し: “YOU CAN USE…”）など。\n\n────────────────\n■Stylistic / 流れの改善提案\n────────────────\n・見出し階層が多少ズレている（例：### ミーム概念検索 → “Memes concept search” の訳）。段落番号を合わせて可読性を保つ。  \n・箇条書き内で「—」区切りと「:」が混在。どちらかに統一。  \n・Fコードなどのコメント行を日本語もしくは原文併記で簡単に説明すると読者フレンドリー。\n\n────────────────\n■まとめ\n────────────────\n1) 追加してしまった MessagePack 項目を削除（または訳注）。  \n2) 欠落した評価コメント・驚き表現を補完。  \n3) 俗語の使い方を抑揚／語彙の多様化で自然に。  \n4) タイトル・見出しのニュアンスを原文どおりに。  \n5) 細かな表記ゆれ・スペース・破線を修正。\n\n以上を直せば、「原文らしい勢い」と「日本語としての滑らかさ」のバランスが取れた完訳になる。\n\nPlease address ALL issues identified in the critique and provide an improved translation.\nReturn ONLY the improved translated text without explanations or comments.",
      "response": "## 別名：CLIP/SigLIP と画像ベクトルエンコーディングを学ぶ方法\n\n_tl;dr_: SigLIP／CLIP を使ってミーム検索エンジンを作った。めちゃ楽しかったし、学びも山盛りだった。\n\nここしばらく実用寄りの AI ツールを作りまくっているが、その中でもいちばん “魔法” 感があるのがベクトル埋め込みだ。[Word2Vec](https://en.wikipedia.org/wiki/Word2vec) を初めて触ったときなんて本当にブッ飛んだ。\n\n先日、[Hacker News で見かけたシンプルなアプリ](https://news.ycombinator.com/item?id=39392582)が[マジで凄かった](https://mood-amber.vercel.app/)。誰かが Tumblr の画像をかき集めて [SigLIP](https://arxiv.org/abs/2303.15343) で埋め込みを作り、「画像をクリックすると似た画像が並ぶ」だけのアプリを公開していた。どうやればいいか皆目見当もつかなかったけど、手が届きそうにも見えた。\n\nこの勢いで「全部理解してやるか」と走り始めた。\n\n## wut（何それ？）\n\nベクトル埋め込み、CLIP／SigLIP などのマルチモーダル埋め込み、ベクトルデータベース……初見でもビビる必要なし。\n\nあの HN のハックを見るまでは、俺もベクトルのことなんて深く考えたことがなかった。FAISS（Facebook 製のシンプルなベクトル DB）や Pinecone（$$）を「動けば OK」で使った程度。テストが通ったら「はい終わり」。\n\nぶっちゃけ今でも「ベクトルって何だよ」って思ってる。Lol.  RAG とか LLM 以外でどう使うかも、今回作るまでピンと来ていなかった。\n\n俺は「作りながら学ぶ」派だ。結果が面白ければ燃えるし、今回はほんと魔法だった。\n\n### WTF 用語集\n\n公開前に友人に読んでもらったら「X って何？」が多発したので、当時ほぼ初見だったキーワードをざっくりまとめておく。\n\n- **Vector Embeddings（ベクトル埋め込み）** — テキストや画像を数値ベクトルに変換し、似たもの検索を爆速にする仕組み。  \n- **Vector Database（ベクトルデータベース）** — エンコード済みベクトルを保存・検索できる DB。  \n- **Word2Vec** — 単語をベクトル化し、類似語や関係性を探れる革命的手法。  \n- **CLIP** — OpenAI のモデル。画像とテキストを同じベクトル空間に押し込む。  \n- **OpenCLIP** — CLIP の OSS 実装。誰でも使い放題。  \n- **FAISS** — 大量ベクトルを管理して近傍検索を激速にするライブラリ。  \n- **ChromaDB** — 画像・テキストのベクトルを保存し、似た結果を秒速で返す DB。  \n\n## シンプルにいこうぜ、harper\n\nこれはただの簡単ハックだ。テキトーにいじってるだけだからスケールなんて気にしない。でも **あなた** が再現できることは重視した。\n\n目標その 1：ぜんぶ Mac ローカルで回す。せっかくの Apple シリコン GPU をフル回転させよう。\n\nまずは画像ディレクトリをクロールするクソ雑なクローラを書く。Apple Photos 派なので手元に写真フォルダがない。そこで秘密のミームチャットから 1 万枚超のミームをエクスポートしてテスト画像にした。\n\n### クローラ\n\n世界一ひどいクローラを書いた。いや正確には、俺の指示で Claude が書いた “世界一ひどいクローラ” だ。\n\n流れはこんな感じ：\n\n1. 対象ディレクトリのファイル一覧を取得  \n2. その一覧を msgpack に保存  \n3. msgpack を読みつつ全画像を走査し、SQLite にメタデータを突っ込む  \n   - ハッシュ  \n   - ファイルサイズ  \n   - パス（location）  \n4. SQLite を回しながら CLIP で各画像をエンコード  \n5. 得たベクトルをまた SQLite に書き戻す  \n6. さらに SQLite を回し、ベクトル＋画像パスを ChromaDB に投入  \n7. 完了  \n\nムダ多すぎるのは百も承知。画像→埋め込み→ChromaDB に一発で突っ込めばいい。でもこんな構成にした理由は：\n\n- ミームのあと 140k 枚クロールしたから、途中で落ちても再開できる堅牢さが欲しかった  \n- 停電やクラッシュでも途中から走らせたかった  \n- ループ回すの大好き  \n\n多少ゴチャついてても結果は優秀。20 万枚超えてもトラブルなしでクロールできた。\n\n### 埋め込みシステム\n\n画像エンコードは楽しい。\n\nまず SigLIP で [シンプルな Web サービス](https://github.com/harperreed/imbedding) を立てた。スタジオの GPU マシンで回したら速いとは言えないが、ローカルの [OpenCLIP](https://github.com/mlfoundations/open_clip) よりははるかに速かった。\n\nそれでもローカル完結させたい。そこで Apple の [ml-explore](https://github.com/ml-explore/) リポジトリを思い出す。中にある [CLIP 実装](https://github.com/ml-explore/mlx-examples/tree/main/clip) が爆速。大きめモデルでも RTX 4090 より速い。意味がわからんほど速い。\n\nあとはスクリプトから簡単に呼び出せれば OK。\n\n### MLX_CLIP\n\nClaude と一緒にサンプルをいじって、どの Mac でも動く小さな Python クラスにした。モデルがなければ自動で DL＆変換、即推論。\n\nリポジトリはこちら: <https://github.com/harperreed/mlx_clip>\n\n出来栄えにけっこう満足。言わずもがなだけど、Apple シリコンはマジで速い。\n\n使い方はこんな感じ：\n\n```python\nimport mlx_clip\n\n# モデルを初期化\nclip = mlx_clip.mlx_clip(\"openai/clip-vit-base-patch32\")\n\n# 画像をエンコード\nimage_embeddings = clip.image_encoder(\"assets/cat.jpeg\")\nprint(image_embeddings)\n\n# テキストをエンコード\ntext_embeddings = clip.text_encoder(\"a photo of a cat\")\nprint(text_embeddings)\n```\n\nSigLIP 版も動かしたい（CLIP よりはるかに良いと思っている）が、これは POC だし保守する気はない。SigLIP で動かすヒントがあったら連絡ちょうだい（hmu）。OpenCLIP を再発明したくはないし、理論上 Apple シリコンでも走るはずなんだよね。\n\n### さて次は\n\n画像ベクトルを ChromaDB にブチ込んだら、あとは UI。基準画像のベクトルを抜いて ChromaDB に投げる → 類似度順に画像 ID が返る。\n\nこれらを Tailwind と Flask で包んで一気に Web 化した。これが自分でも驚くほどイケてた。  \n2015 年なら何人月かかったかわからんが、今回はトータル 10 時間。笑うしかない。\n\n結果はガチで魔法。\n\n### ミーム概念検索\n\n最初のデータセットはミーム。約 12,000 枚。\n\nまずこれ：\n\n{{< image src=\"images/posts/vector-memes-bowie.png\" caption=\"So true\" >}}\n\nエンコードして ChromaDB に投げると……\n\n{{< image src=\"images/posts/vector-memes-bowie-results.png\" >}}\n\nもう一例：\n\n{{< image src=\"images/posts/vector-memes-star-trek.png\" >}}\n\n返ってくるのがこれ：\n\n{{< image src=\"images/posts/vector-memes-star-trek-results.png\" >}}\n\nクリックしてるだけで無限に遊べる。\n\n### 「Namespaces?」って何だよ、みたいな話\n\n「画像クリック→類似画像」はクールだけど、俺の脳を吹き飛ばしたのは “同じモデルで検索テキストもベクトル化し、似た画像を返せる” ことだ。テキスト→画像のマルチモーダル検索は完全に手品。\n\nいくつか例を挙げよう。\n\n**money** で検索：  \n{{< image src=\"images/posts/vector-memes-money.png\" >}}\n\n**AI** で検索：  \n{{< image src=\"images/posts/vector-memes-ai.png\" >}}\n\n**red** で検索（色？ ライフスタイル？ それともロシア？）：  \n{{< image src=\"images/posts/vector-memes-red.png\" >}}\n\n延々と遊べる。忘れてたお宝ミームもザクザク。  \n「ブログ執筆のミーム欲しいな」と思ったら──  \n{{< image src=\"images/posts/vector-memes-writing-meme.jpg\" >}}\n\n（自覚はあるけど気にしない w）\n\n### 写真ライブラリで試すと？\n\nめちゃくちゃいい感じに動く。\n\n自分のフォトライブラリで試すのを激しくオススメする。俺は Google Photos の Takeout を落として外付けディスクに展開。重複だらけだからスクリプトで整理し、ミームフォルダの代わりにそのディレクトリをクロール。\n\n写真 140k 枚で約 6 時間。悪くない。結果は最高。\n\n#### 例いろいろ\n\nそっくり写真（Google Photos の重複問題はご愛敬）  \n{{< image src=\"images/posts/vector-memes-harper.png\" >}}\n\nうちのプードルたち  \n{{< image src=\"images/posts/vector-memes-poodles.png\" >}}\n\nランドマーク検索：飛行機から撮った富士山に自分でも気づいてなかった  \n{{< image src=\"images/posts/vector-memes-fuji-results.png\" >}}\n\nそこから類似の富士山写真  \n{{< image src=\"images/posts/vector-memes-fuji-similar.png\" >}}\n\n場所もラクラク  \n{{< image src=\"images/posts/vector-memes-chicago.png\" >}}\n\n感情系。「驚き顔」がこんなにあったとは  \n{{< image src=\"images/posts/vector-memes-surprised.png\" >}}\n\nニッチなテーマ、ローライダーのクルマ（渋谷で撮影）  \n{{< image src=\"images/posts/vector-memes-low-riders.png\" >}}\n\n検索しづらい「ボケ味」も一発  \n{{< image src=\"images/posts/vector-memes-bokeh.png\" >}}\n\n忘れてた名ショットも救出。2017 年に撮った Baratunde：  \n{{< image src=\"images/posts/vector-memes-baratunde.png\" >}}\n\n### これはそのうち全部のアプリに入る\n\n近いうちに主要フォトアプリは絶対この機能を積む。Google Photos にはもう入ってるかもしれないけど、Google がゴチャゴチャ手を入れすぎて誰も気づいてないだけかも。\n\n画像を扱うプロダクトを持ってるなら、今すぐパイプライン組んでエンコードを始めた方がいい。\n\n## 無料で使えるヨ！（原文見出し: “YOU CAN USE THIS FOR THE LOW PRICE OF FREE”）\n\nソースはここ: <https://github.com/harperreed/photo-similarity-search>\n\nぜひ触ってみてくれ。ちょっとハック感あるけど動く。  \nconda などで環境を分けると楽。UI は Tailwind、サーバは Flask、コードは Python。ホストは harper reed でした。\n\n## キミへのチャレンジ！\n\n俺のフォトライブラリを快適にカタログ化できる Mac ネイティブアプリを作ってくれ。クラウドに上げるのはイヤ。ライブラリを指定して「クロール開始」押すだけ、みたいなのが理想。\n\n- Llava／Moondream で自動キャプション  \n- キーワード／タグ付け  \n- ベクトル類似検索  \n- etc.  \n\n全部ローカルで動いて、シンプルで強力。Lightroom、Capture One、Apple Photos と連携できたら最高だ。\n\nこれ、マジで欲しい。誰か作って！\n\n## 追加課題：Lightroom プレビュー JPEG 復旧\n\nハック仲間の Ivan もこの魔法を見て即参戦。彼の写真は外付け HDD にあるが、Lightroom のプレビューファイルはローカルにあった。そこでサムネとメタデータを抜き出して外付けに保存するスクリプトを書いた。\n\nそのあと画像ベクトルクローラを回したら類似検索も問題なく動いた。BAM──完璧。\n\n#### Lightroom の写真を救出 — サムネだけでも\n\nIvan のシンプルスクリプトは超便利。もし本体ライブラリが吹っ飛んでも .lrprev が残っていれば低解像度版だけでも救出できる。  \nリポジトリ: <https://github.com/ibips/lrprev-extract>  \n（訳注: 原文では “Light Room” と表記されているが、正しくは “Lightroom”。）\n\n## Thanks for reading.\n\nいつでも連絡ちょうだい（hmu: harper@modest.com）―AI、EC、写真、Hi-Fi、ハック、何でも語ろう。  \nシカゴにいるならぜひ遊びに来て。",
      "usage": {
        "prompt_tokens": 9556,
        "completion_tokens": 3293,
        "total_tokens": 12849
      },
      "streaming": true
    },
    "frontmatter": {
      "model": "o3",
      "target_language": "japanese",
      "system_prompt": "You are a professional translator. Translate the following frontmatter fields to japanese.\nEach field is in the format \"field_name: content\". Translate ONLY the content, not the field names.\nReturn the translated content in the exact same format, preserving all field names.",
      "user_prompt": "title: I accidentally built a meme search engine\n\ndescription: I built a magical meme search engine using siglip/CLIP and vector encoding images. It was a fun way to learn about this powerful technology. I'm sharing the code so you can build your own and discover forgotten gems in your photo library. Let's unleash the power of AI on our images!\n\n",
      "response": "title: うっかりミーム検索エンジンを作ってしまった\ndescription: siglip/CLIP と画像のベクトルエンコーディングを使って、魔法のようなミーム検索エンジンを作りました。この強力な技術を学ぶのに最適で、とても楽しかったです。あなたも自分で構築して写真ライブラリに眠る忘れられた逸品を見つけられるよう、コードを共有しています。さあ、画像に AI の力を解き放ちましょう！",
      "usage": {
        "prompt_tokens": 124,
        "completion_tokens": 130,
        "total_tokens": 254
      },
      "fields": [
        "title",
        "description"
      ],
      "streaming": true
    },
    "all_critiques": [
      "【総評】  \n大筋では意味を保った訳になっているものの、原文の“毒気のあるカジュアルさ”や細かいニュアンス・語彙の一貫性がかなり薄まり、細部の訳抜け／訳し違いも点在します。完成度を上げるには、意味上の欠落を埋めるだけでなく、語調・スタイルを統一しつつ自然な日本語に磨き直す必要があります。以下、重大度別に具体的な問題点を列挙します。\n\n────────────────\n■Critical（意味の誤り・欠落・誤解を招くもの）\n────────────────\n1. タイトル\n   - 原文: “Or: how to learn about clip/siglip and vector encoding images”\n   - 訳: 「つまり：clip/siglip と画像ベクトルエンコードを学ぶ方法」\n   - 問題: “Or:” は「別題」や「言い換え」のニュアンスで “つまり” ではない。“画像ベクトルエンコード” も不自然。\n   - 改善案: 「別名：CLIP/SigLIP と画像のベクトルエンコードを学ぶには」\n\n2. “vector embeddings, multi modal embeddings or vector datastores” の訳抜け\n   - 訳文では “vector embeddings” が落ちている。\n   - 例: 「…ベクトル埋め込みやマルチモーダル埋め込み、ベクトルストアについて…」など補完。\n\n3. “I still barely know what vectors are.” の語調\n   - 原文は「未だにベクトルが何だかロクにわかっていない」という自嘲。訳では語気が若干柔らかいが許容範囲。ただし「ほとんど分かっていない」→「いまだによく分かってない」で口語感を強めたい。\n\n4. Bullet: “Vector Embeddings convert your text of images”  \n   - 原文は “text or images”。訳は “テキストや画像” としており誤りではないが、後段「画像検索“などを可能にする”」と機能を限定。検索だけでなく「意味的な比較」等も含むので明示するとより正確。\n\n5. Crawler 手順 3 の metadata\n   - 原文 “location” = パス/ロケーション。訳では「パス」と記述箇所があるが、箇条書き冒頭は「ハッシュ / ファイルサイズ / パス」などに統一を。\n\n6. “Chromed returns …”  \n   - “Chromed” はタイプミスで “Chroma’d” 意味だが、訳では「Chroma は」と修正しており OK。注釈を付けるか「原文の誤字」メモを入れると親切。\n\n────────────────\n■Major（ニュアンス喪失・文体/用語の不統一・読みにくさ）\n────────────────\n1. スラング／罵り語の希薄化\n   - “just fucking around”“holy shit”“oh shit” などを軟化。「遊び」「十分クール」などに置き換えているため、原文の “やや荒っぽいフランクさ” が薄い。  \n   - 方針として残すなら「ただの遊びでやってんだぜ」「マジでヤバい」「うわクソ、ブログ用ミームが要る」等でトーン維持を。\n\n2. “wut” 見出し\n   - 原文の崩し (“wut”) をそのまま残したが、日本語本文では浮く。  \n   - 例：「wut（要するに何？）」のようにカタカナ補足や注釈付きで“崩し”を演出。\n\n3. 用語の揺れ\n   - “ベクトルストア / ベクトル DB / Vector Database” が混在。\n   - “CLIP/OpenCLIP/open clip” も大小表記がまちまち。\n   - 一貫して「ベクトルデータベース」「OpenCLIP」などに統一する。\n\n4. 英語直訳・カタカナ過多による機械っぽさ\n   - 「ベクトルをベクトルストアに突っ込む」「爆速」「BAM」などは口語調を再現しつつも濫用すると稚拙に感じる。  \n   - 「ワイルドだ」は “wildstyle” の訳として違和感。例：「ブッ飛んでる」等。\n\n5. “fast af”“magic” などの繰り返し\n   - 訳では「爆速」「魔法」だけで単調。バリエーション（「鬼速」「チート級」「魔法みたい」等）でリズムを再現。\n\n6. Numeral consistency\n   - “140k/200k” → 「14万/20万」で統一したが、“1.2万” と “12000” が混在。漢数字 or アラビア数字に揃える。\n\n7. Caption の括弧\n   - “So true” などのキャプションは訳出漏れや英語のまま。「ほんとそれ」等訳を入れるか注記。\n\n8. 段落切りが長い\n   - 原文は短い文で畳みかけるリズム。日本語でも 1 文が長くなりがちなので、小刻みに区切って勢いを維持。\n\n────────────────\n■Minor（文法・表記・細部の自然さ）\n────────────────\n1. 「フォトがバラバラのフォルダにはなっていない」  \n   - 「写真が一箇所のフォルダにまとまっていない」など自然に。\n\n2. “温めようじゃないか”  \n   - GPU を「温める」は直訳臭。「ブン回そう」「フル稼働させよう」に。\n\n3. 「ループが好き」  \n   - やや唐突。「ループ処理フェチなんだよね」など補完すると馴染む。\n\n4. 「マジで trivial」  \n   - “trivial” をそのまま。→「ほんとに大したことない」「笑っちゃうほど簡単」等。\n\n5. “hmu” の訳\n   - 何度かメール表記だけ。最初の登場で「hmu（hit me up：気軽に連絡ちょうだい）の意」など注釈を。\n\n6. リポジトリ URL\n   - 全て <> で括る、またはインラインコードにして統一。\n\n7. 全角／半角の混乱\n   - “CLIP/siglip” などスラッシュ前後のスペース揺れ。全体確認。\n\n────────────────\n■Terminology（技術用語の精度・統一）\n────────────────\n| 原語 | 推奨訳 | 備考 |\n|------|--------|------|\n| embedding | 埋め込み | “エンベディング” に寄せる場合は全編統一 |\n| vector store | ベクトルデータベース | “ベクトルストア” でも可・統一必須 |\n| multimodal | マルチモーダル | 訳抜け注意 |\n| Apple Silicon | Apple シリコン | カタカナ表記推奨 |\n| encode / encoding | エンコード／ベクトル化 | 行為と結果で使い分け |\n\n────────────────\n■Tone & Register（口語感の維持）\n────────────────\n原文は「技術オタクの独白ブログ」という強い一人称カジュアル。訳文は若干フォーマル寄りで、罵り語を丸めすぎると原作者のキャラが薄れる。  \n・“lol”“af”“fuck”“shit” などは下品さを残しつつ、日本語スラングで再現（「草」「クソ速い」「クッソ楽しい」など）  \n・不要に英単語を混ぜすぎると逆に機械翻訳っぽくなるので、意図的に使う場合はカギ括弧や斜体で演出。\n\n────────────────\n■まとめ\n1. タイトル・訳抜けなどCritical項目を最優先で修正。  \n2. 口語スラングを和製スラングに置き換え、著者の“荒っぽいノリ”を再生。  \n3. 用語・数字・表記ルールをドキュメント化し、全文で統一。  \n4. 長文を切り、語彙のバリエーションでリズムを再現。  \n5. hmu 等ネット略語の注釈で読者フォロー。  \n\n以上を反映すれば、原文の勢いを保ちつつ自然で読みやすい日本語になるはずです。",
      "【総評】  \n原文のカジュアルで罵り言葉混じりのノリはよく再現できている一方、①不必要な意訳・脚色、②細部説明の抜け落ち、③訳者自身のコメント混入、④専門用語のニュアンスずれ、⑤語法ミス／不自然表現 が散見される。原文と同程度の荒いトーンを維持しつつも、意味の忠実さ・情報量・日本語としての自然さをもう一段上げられる。\n\n────────────────────\nA. 重大（意味変化・情報欠落）\n────────────────────\n1. “location” の誤解釈  \n　原文 step3 “…Grabing some metadata … location” は撮影地ないしディレクトリ位置の両義があり得る。訳では「パス」と断定しており、地理情報を指す可能性を潰している。  \n　修正案: 「ロケーション（撮影場所／ファイルパスなど）」と両義保持。\n\n2. Bullet 定義の省略による意味の欠落  \n　・Vector Embeddings – 原文 “find similar pics and search your library effectively” の “効果的に …” 部分が訳文ではごっそり削られ、ユースケース説明が弱まった。  \n　・Vector DB, Word2Vec も同様に原文の “exploring relationships” 等を落としている。  \n　修正案: 端折らずに「〜できる」「〜を探れる」だけでなく、原文どおり “〜を探索・可視化できる” 等まで補完。\n\n3. 原文にない訳者コメントの混入  \n　「（原文は “Chromed” ってタイプミスしてるけどね）」は事実でも本文情報ではない。読者を混乱させる上、忠実訳の原則から外れる。  \n　修正案: 脚注等に分離するか削除。\n\n4. “never fear.” の訳抜け  \n　原文冒頭 “never fear” が訳文では落ちており、読者安心を促すニュアンスが消えている。  \n　修正案: 「心配ご無用。」などを追加。\n\n────────────────────\nB. 中程度（スタイル・語調・用語統一）\n────────────────────\n1. タイトル “別名”  \n　“Or:” は「あるいは」「または」の方が自然。“別名” は別タイトルという誤読誘発。  \n　修正案: 「または：CLIP／SigLIP と画像ベクトル化の学び方」。\n\n2. “ブッ込んで”“クソ速い” の多用  \n　原文にも F-word が頻出するが、日本語での強さはワンランク上に聞こえる。「クソ〜」「激ヤバ」連発は読者によっては稚拙に感じる恐れ。数回に留め、他は「めちゃ」「バカ速い」等で抑揚を付けるとバランス良。  \n\n3. 一人称「俺」の固定  \n　原文 “I” は性別非依存。カジュアルでも「私/ぼく」を混ぜるか、「筆者」にする方が汎用性が高い。  \n\n4. 英語残存の揺れ  \n　“hmu”“BAM” などは原文通り残してよいが、読み下し注を統一して付す（*hmu = 気軽に連絡して* のように毎回簡潔に）。  \n\n5. 用語表記ゆらぎ  \n　CLIP → **大文字**、SigLIP → **小文字 g** を守る等一貫させる。ChromaDB⇔Chroma も同じ。\n\n────────────────────\nC. 軽微（自然さ・文法・タイポ）\n────────────────────\n1. 「覚える方法」  \n　ここは “learn about” であり「学ぶ方法」「理解する方法」の方が自然。  \n\n2. “ガチ推奨”“ヤバいくらい良い”  \n　意味は通じるが口語度が高い。ブログ翻訳と割り切るなら問題小、より広い読者向けには「強くおすすめ」「驚くほど良い」等に調整。  \n\n3. “クソ楽しかったし、めちゃくちゃ学びも多かった。”  \n　“学び” は名詞化がやや硬直。「学べたことも山ほどあった」などリズムを整える。  \n\n4. 半角/全角統一  \n　数字＋単位、“14 万枚” vs “140k” 等、全角／半角が混在。日本語記事としてはアラビア数字半角で統一した方が読みやすい。  \n\n5. “笑っちゃうほど簡単。”  \n　末尾句点がある箇所とない箇所が混在。文末の体裁を揃える。\n\n────────────────────\nD. 具体的修正文例（抜粋）\n────────────────────\n【例1：タイトル】  \n誤）別名：CLIP／SigLIP と画像のベクトルエンコードを覚える方法  \n正）または：CLIP／SigLIPで画像をベクトル化する方法を学ぶには\n\n【例2：Vector Embeddings 定義】  \n誤）テキストや画像を数値ベクトルに変換して意味的な比較や検索を可能にする。  \n正）テキストや画像を数値ベクトルに変換し、似た画像を素早く見つけたりライブラリ内を効率的に検索したりできるようにする。\n\n【例3：crawler step3】  \n誤）- パス  \n正）- ロケーション（ファイルパスまたは撮影地点）\n\n【例4：不要コメント削除】  \n誤）Chroma は類似度順に画像 ID を返す（原文は “Chromed” ってタイプミスしてるけどね）。  \n正）Chroma は類似度順に画像 ID を返す。\n\n────────────────────\nE. まとめ／改訂指針\n────────────────────\n1. 情報欠落を補い、意訳で削った機能説明は戻す。  \n2. 「原作者の口癖」と「訳者の脚色」の境界を明確にし、訳者コメントは脚注扱いか削除。  \n3. 罵り語は原文比で強すぎないレベルに抑えてバリエーションを付ける。  \n4. 用語・表記を一貫させ、日本語としての読みやすさ（語尾・句点・半角）を整える。  \n\n上記を反映すれば、原文のラフな雰囲気を保ちつつ正確さと読みやすさを両立した訳になるはず。",
      "【総評】  \n全体としては読みやすく、技術内容も概ね正確ですが、①意味が変わる／弱まる箇所、②原文のカジュアルで下品なトーン（罵り語・スラング）の希薄化、③細部の省略・追加、④語法・語調のばらつきが散見されます。以下、重大度順に項目別で指摘します。\n\n────────────────────\n■Critical（意味を損なう・誤訳・脱落）\n\n1. “It wasn't fast, but it was way faster …” の逆転  \n　訳:「そこそこ速く～ずっとマシだった」  \n　問題: 原文は「決して速くはないが、OpenCLIP よりはずっと速い」。訳は｢そこそこ速い｣と肯定的ニュアンスへ反転。  \n　改善: 「速いとは言えないが、ローカルの OpenCLIP よりははるかに速かった」等。\n\n2. タイトル “Or: …”  \n　訳:「または：CLIP／SigLIPと画像ベクトル化を学ぶ方法」  \n　問題: “Or:” はブログ副題のノリで「～入門」「～の巻」に近い。訳の「または」は不自然で意味が通らない。  \n　改善: 「CLIP/SigLIP と画像ベクトルエンコーディング入門」などに変更。\n\n3. 原文の強い口語・罵り語のトーン欠落  \n　例: “I am just fucking around” → 「遊びなので」  \n　問題: 口調が和らぎ、著者のキャラクターが抑制される。  \n　改善: 「テキトーにいじってるだけだから～」などラフさを残す。\n\n4. “Wildstyle.” の削除  \n　原文の勢いを示す一語を抹消。  \n　改善: 「ワイルドすぎる。」等で反映。\n\n5. “Google Photos probably already does this, but they have googled it so much that nobody notices.”  \n　訳:「Google 流に埋もれているだけかもしれない。」  \n　問題: “googled it” ＝ Google 的改悪／ゴテゴテ化 の皮肉が弱い。  \n　改善: 「Google がゴチャゴチャ手を入れすぎて誰も気づかないだけかもしれない」等。\n\n6. 軽微だが “I still barely know what vectors are. Lol.” の “Lol.” が落ちている。  \n　改善: 「ベクトルが何か正直まだピンときてない。笑」など。\n\n────────────────────\n■Major（ニュアンス・トーン・レジスター／文体・表記揺れ）\n\nA. トーンの統一  \n　・「です/ます」と「だ/である」が混在（例: 「これはストレートなハックだ。遊びなので…」→その後「おすすめする」など）。  \n　・原文は終始くだけた一人称＆罵り語。邦訳もラフさで統一するか、丁寧体に改めるか決めて徹底。\n\nB. スラング・略語の処理  \n　・“hmu” に注記を付けつつカタカナ化しているが、本文では英字のまま。「hmu（DM ちょうだい）」など一度だけ脚注扱いし、以後は原綴き保持でよい。  \n　・“kinda”、 “shit” なども全て穏便表現に置換され、温度差がある。\n\nC. 技術用語の表記ブレ  \n　・SigLIP / siglip / Siglip など統一されていない。  \n　・FAISS → 全文大文字が正式。  \n　・ChromaDB → 「Chroma」表記も混在。どちらかに統一し注記を付ける。\n\nD. 不自然な外来語直訳  \n　・「ループ処理が好き」→ 読点かカジュアル訳「ループ回すの大好き」  \n　・「温めよう」(GPU を heat up) はやや不自然。「フル回転させよう」などへ。\n\nE. 見出し “Namespaces?”  \n　訳はそのまま。読者には意味不明。本文で「名前空間が魔法じゃない」などの種明かしがないため、訳注で「※ここでは単なる区切りタイトル」等を入れるか、日本語に置き換える。\n\n────────────────────\n■Minor（文法・スタイル・細部欠落）\n\n1. “140 k” → 「140k」か「14万」。スペース不要。  \n2. “msgpack” 大文字小文字を統一（例: MessagePack）。  \n3. “20 万枚を超えてもノートラブル” → “ノートラブルで” より “トラブルなしで” の方が自然。  \n4. “ニッチにローライダー” → 「ニッチなテーマとしてローライダー」とする方が通じやすい。  \n5. “自分のフォトライブラリで試すのを強くおすすめする” → “おすすめ” と “お勧め” ブレ。  \n6. 句読点のゆれ（全角／半角カンマ、ピリオド併用）。統一推奨。\n\n────────────────────\n■Omissions／Additions（内容追加・抜け）\n\n・“multi modal embeddings”を「マルチモーダル埋め込み」と入れたが、用語解説一覧からは漏れている。  \n・“I really like loops” は訳出済みだが、直前の「I needed it to be able to resume …」など理由が若干要約され弱い。  \n・“A super handy script to keep around.” → 「常備しておくと超便利」と補うと原意保持。  \n・“I want this. Build it.” の二重命令が一文に圧縮され、原文の勢いが薄い。 →「これ、マジで欲しい。誰か作って！」等で再現。\n\n────────────────────\n■機械翻訳っぽい直訳箇所\n\n・「これはストレートなハックだ。」→ “straightforward hack” の直訳感。「ただの簡単ハック」など。  \n・「ムダが多いのは承知している。」→ “This is a lot of wasted work.” 直訳臭。  \n・「驚くほど良い。」→ “works super well” は「めちゃくちゃいい感じに動く」などの方が口語。  \n\n────────────────────\n■改善後のサンプル（抜粋）\n\n原: It wasn't fast, but it was way faster than running open clip locally.  \n改: ぶっちゃけ速いとは言えないけど、ローカルで OpenCLIP を回すよりは断然速かった。\n\n原: I am just fucking around so I wasn't super interested in making it scalable.  \n改: こっちは遊び半分でやってるだけだから、スケールさせることには正直こだわってない。\n\n原: Wildstyle.  \n改: ワイルドすぎるだろ。\n\n────────────────────\n■まとめ\n\n・意味変化箇所（速度の評価など）は必ず修正。  \n・スラング／罵り語／口語のレベルを揃え、原文の勢いを保持。  \n・用語表記を統一 (SigLIP, CLIP, FAISS, ChromaDB, MessagePack)。  \n・見出し「Namespaces?」など不明瞭部分は訳注または言い換えで読者配慮。  \n・句読点・敬体／常体の揺れを一本化。\n\n以上を反映すれば、原文の“オタクっぽいノリと魔法感”を損なわず、自然な日本語で100%忠実な訳文になります。",
      "【総評】  \n大筋では原文の意味とノリをよく再現できているものの、①情報を“追加”してしまった箇所、②細かな抜け・意味のずれ、③トーンがやや行き過ぎ／足りない箇所、④日本語としてのぎこちなさ──が点在する。とくに「追加情報」は忠実性の観点でクリティカル。以下、重大度順に列挙する。\n\n────────────────\n■Critical（意味改変・追加／欠落）\n────────────────\n1. 追加情報「MessagePack」  \n   • 原文の用語一覧には MessagePack はない。訳文では独自に項目を立て説明を付け足している。  \n   →削除するか、脚注扱いで「訳注: 原文にはないが…」と明示。\n\n2. “it is way better than CLIP” の欠落  \n   • 原文: “I prefer that model (it is way better than CLIP).”  \n   • 訳文: “SigLIP 版も動かしたいけど～保守する気はない。”  \n   →「CLIP よりはるかに良いと思っている」という評価を補う。\n\n3. “This was incredible.” の欠落  \n   • 原文: “I then wrapped it all up in a tailwind/flask app. This was incredible.”  \n   →「これには自分でも驚いた」等を追加。\n\n4. “I had no idea how to achieve this” の弱体化  \n   • 訳文では「手が届きそうに見えた」だけで、“全くやり方が分からなかった” という驚きが薄い。  \n   →「どうやればいいか皆目見当もつかなかったけど～」と補強。\n\n5. Apple-silicon 速度記述の一部欠落  \n   • “I know most people know this, but…” が訳出されていない。  \n   →「言わずもがなだけど～」を入れる。\n\n────────────────\n■Major（トーン・スタイル・語法）\n────────────────\n6. 一人称「俺」・罵語「クソ」等の強さ調整  \n   • 原文の “fucking”, “shit” を訳文ではほぼ「クソ」で統一しており、やや連発気味。  \n   →緩急を付けて「クソ→めちゃ→死ぬほど」など言い換え、リズムを整える。\n\n7. 「作り散らかしている」  \n   • 口語としてやや不自然。  \n   →「作りまくっている／量産している」など。\n\n8. 「ワイルドすぎるだろ」  \n   • “Wildstyle.” をどう訳すかは悩ましいが、ここでは「ワイルドすぎる」より「意味がわからんほど速い」などの方が自然。\n\n9. 「名前空間って？」  \n   • 原文 “Namespaces?” は半ば冗談。直訳だと誤解を招く。  \n   →「『Namespaces?』って何だよ、みたいな話」等で冗談感を残す。\n\n10. 見出しの “Or:” の処理  \n    • 原文タイトルの “Or: how to …” という肩書き的ニュアンスが消えた。  \n    →「別名：CLIP/SigLIP と…」などで拾う。\n\n────────────────\n■Minor（表記ゆれ・リズム・細部）\n────────────────\n11. SigLIP の大小表記ゆれ  \n    • 訳文では “SigLIP／SigLIP 版” と混在。全て “SigLIP” 統一を推奨。\n\n12. 半角全角スペース  \n    • “Lol.　RAG” など、句点後に全角空白が残る箇所あり →削除。\n\n13. “ベクトル埋め込みとか CLIP／SigLIP、マルチモーダル埋め込み…”  \n    • “とか” が重複しリズムが悪い →「ベクトル埋め込み、CLIP／SigLIP などのマルチモーダル埋め込み…」等。\n\n14. 「BAM—」のダッシュ  \n    • 全角ダッシュ “—” が環境によっては化ける。短い破線 “–” か “──” 等に統一。\n\n15. 「ローライダー」  \n    • “low riders” は複数形なので「ローライダーたち／ローライダーの車」など文脈合わせ。\n\n16. 「hmu」を毎回英語のまま  \n    • 初回に（hit me up）と訳注、一部は「連絡ちょうだい」でいい。\n\n17. Light Room → Lightroom  \n    • 原文のスペルミスを訳で直すのは OK。ただ訳注で「原文では Light Room となっているが実際は Lightroom」等を添えると親切。\n\n18. “YOU CAN USE THIS FOR THE LOW PRICE OF FREE” を英語のまま残置  \n    • カタカナ or 日本語に＋原文見出し併記例：「無料で使えるヨ！」（原文見出し: “YOU CAN USE…”）など。\n\n────────────────\n■Stylistic / 流れの改善提案\n────────────────\n・見出し階層が多少ズレている（例：### ミーム概念検索 → “Memes concept search” の訳）。段落番号を合わせて可読性を保つ。  \n・箇条書き内で「—」区切りと「:」が混在。どちらかに統一。  \n・Fコードなどのコメント行を日本語もしくは原文併記で簡単に説明すると読者フレンドリー。\n\n────────────────\n■まとめ\n────────────────\n1) 追加してしまった MessagePack 項目を削除（または訳注）。  \n2) 欠落した評価コメント・驚き表現を補完。  \n3) 俗語の使い方を抑揚／語彙の多様化で自然に。  \n4) タイトル・見出しのニュアンスを原文どおりに。  \n5) 細かな表記ゆれ・スペース・破線を修正。\n\n以上を直せば、「原文らしい勢い」と「日本語としての滑らかさ」のバランスが取れた完訳になる。"
    ]
  },
  "critique_loop_details": {
    "critique_usages": [
      {
        "prompt_tokens": 7951,
        "completion_tokens": 2013,
        "total_tokens": 9964
      },
      {
        "prompt_tokens": 7938,
        "completion_tokens": 1583,
        "total_tokens": 9521
      },
      {
        "prompt_tokens": 8042,
        "completion_tokens": 1834,
        "total_tokens": 9876
      },
      {
        "prompt_tokens": 7724,
        "completion_tokens": 1469,
        "total_tokens": 9193
      }
    ],
    "feedback_usages": [
      {
        "prompt_tokens": 10548,
        "completion_tokens": 3407,
        "total_tokens": 13955
      },
      {
        "prompt_tokens": 9965,
        "completion_tokens": 3468,
        "total_tokens": 13433
      },
      {
        "prompt_tokens": 10332,
        "completion_tokens": 3233,
        "total_tokens": 13565
      },
      {
        "prompt_tokens": 9556,
        "completion_tokens": 3293,
        "total_tokens": 12849
      }
    ]
  },
  "timestamp": "2025-06-27T07:51:24.905628"
}