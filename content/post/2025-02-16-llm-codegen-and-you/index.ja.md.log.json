{
  "input_file": "index.md",
  "output_file": "index.ja.md",
  "target_language": "japanese",
  "language_code": "ja",
  "model": "o3",
  "skip_edit": false,
  "do_critique": true,
  "critique_loops": 4,
  "has_frontmatter": true,
  "translation_context": "this is a blog post for harper reed's blog about llm codegen, and vibe coding",
  "token_usage": {
    "prompt_tokens": 98216,
    "completion_tokens": 29423,
    "total_tokens": 127639
  },
  "cost": "$2.1591",
  "prompts_and_responses": {
    "translation": {
      "model": "o3",
      "target_language": "japanese",
      "system_prompt": "\n        1. Read the provided text carefully, preserving all formatting, markdown, and structure exactly as they appear.\n        2. Identify any block quotes and code blocks.\n        3. Do not translate text in block quotes or in code blocks (including text within code blocks).\n        4. Translate everything else into japanese.\n        5. Maintain the original formatting, markdown, and structure in your output.\n        6. Provide a natural-sounding translation rather than a word-for-word one.\n        7. For idioms, colloquialisms, or slang, render them in an equivalent, natural way in japanese whenever possible.\n        8. If there isn't a direct or natural translation for a particular term or phrase, keep it in the original language and surround it with quotes if necessary.\n        9. Ensure that technical terms or jargon remain accurate; if there's no suitable translation, keep the original term.\n        10. Strive for fluid, native-sounding prose that retains the tone and intent of the original text.\n        11. If context about the text is provided, use it to inform your translation choices, especially regarding tone, style, and cultural adaptations.\n        ",
      "user_prompt": "# CONTEXT\nthis is a blog post for harper reed's blog about llm codegen, and vibe coding\n\n# TEXT TO TRANSLATE\n_tl:dr; Brainstorm spec, then plan a plan, then execute using LLM codegen. Discrete loops. Then magic. ✩₊˚.⋆☾⋆⁺₊✧_\n\nI have been building so many small products using LLMs. It has been fun, and useful. However, there are pitfalls that can waste so much time. A while back a friend asked me how I was using LLMs to write software. I thought \"oh boy. how much time do you have!\" and thus this post.\n\n(p.s. if you are an AI hater - scroll to the end)\n\nI talk to many dev friends about this, and we all have a similar approach with various tweaks in either direction.\n\nHere is my workflow. It is built upon my own work, conversations with friends (thx [Nikete](https://www.nikete.com/), [Kanno](https://nocruft.com/), [Obra](https://fsck.com/), [Kris](https://github.com/KristopherKubicki), and [Erik](https://thinks.lol/)), and following many best practices shared on the various terrible internet [bad](https://news.ycombinator.com/) [places](https://twitter.com).\n\nThis is working well **NOW**, it will probably not work in 2 weeks, or it will work twice as well. ¯\\\\\\_(ツ)\\_/¯\n\n## Let’s go\n\n{{< image src=\"llm-coding-robot.webp\" alt=\"Juggalo Robot\" caption=\"I always find these AI-generated images to be suspect. Say hi to my juggalo coding robot angel!\" >}}\n\nThere are many paths for doing dev, but my case is typically one of two:\n\n- Greenfield code\n- Legacy modern code\n\nI will show you my process for both paths\n\n## Greenfield\n\nI find the following process works well for greenfield development. It provides a robust planning and documentation approach, and allows you to execute easily in small steps.\n\n{{< image src=\"greenfield.jpg\" alt=\"Green field\" caption=\"Technically, there is a green field on the right. Leica Q, 5/14/2016\" >}}\n\n### Step 1: Idea honing\n\nUse a conversational LLM to hone in on an idea (I use ChatGPT 4o / o3 for this):\n\n```prompt\nAsk me one question at a time so we can develop a thorough, step-by-step spec for this idea. Each question should build on my previous answers, and our end goal is to have a detailed specification I can hand off to a developer. Let’s do this iteratively and dig into every relevant detail. Remember, only one question at a time.\n\nHere’s the idea:\n\n<IDEA>\n```\n\nAt the end of the brainstorm (it will come to a natural conclusion):\n\n```prompt\nNow that we’ve wrapped up the brainstorming process, can you compile our findings into a comprehensive, developer-ready specification? Include all relevant requirements, architecture choices, data handling details, error handling strategies, and a testing plan so a developer can immediately begin implementation.\n```\n\nThis will output a pretty solid and straightforward spec that can be handed off to the planning step. I like to save it as `spec.md` in the repo.\n\n> You can use this spec for a number of things. We are doing codegen here, but I have used it to bolster ideas by asking a reasoning model to poke holes in the idea (must go deeper!), to generate a white paper, or to generate a business model. You can pop it into deep research and get a 10k word supporting document in return.\n\n### Step 2: Planning\n\nTake the spec and pass it to a proper reasoning model (`o1*`, `o3*`, `r1`):\n\n(This is the TDD prompt)\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. Review the results and make sure that the steps are small enough to be implemented safely with strong testing, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.\n\nFrom here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step in a test-driven manner. Prioritize best practices, incremental progress, and early testing, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.\n\nMake sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.\n\n<SPEC>\n```\n\n(This is the non-tdd prompt)\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. review the results and make sure that the steps are small enough to be implemented safely, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.\n\nFrom here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step. Prioritize best practices, and incremental progress, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.\n\nMake sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.\n\n<SPEC>\n```\n\nIt should output a prompt plan that you can execute with aider, cursor, etc. I like to save this as `prompt_plan.md` in the repo.\n\nI then have it output a `todo.md` that can be checked off.\n\n```prompt\nCan you make a `todo.md` that I can use as a checklist? Be thorough.\n```\n\nYou can save it as `todo.md` in the repo.\n\nYour codegen tool should be able to check off the `todo.md` while processing. This is good for keeping state across sessions.\n\n#### Yay. Plan!\n\nNow you have a robust plan and documentation that will help you execute and build your project.\n\nThis entire process will take maybe **15 minutes**. It is pretty quick. Wild tbh.\n\n### Step 3: Execution\n\nThere are so many options available for execution. The success really depends on how well step 2 went.\n\nI have used this workflow with [github workspace](https://githubnext.com/projects/copilot-workspace), [aider](https://aider.chat/), [cursor](https://www.cursor.com/), [claude engineer](https://github.com/Doriandarko/claude-engineer), [sweep.dev](https://sweep.dev/), [chatgpt](https://chatgpt.com), [claude.ai](https://claude.ai), etc. It works pretty well with all the tools I have tried, and I imagine it will work well with any codegen tool.\n\nI, however, prefer **raw** claude and aider:\n\n### Claude\n\nI essentially pair program with [claude.ai](https://claude.ai) and just drop each prompt in iteratively. I find that works pretty well. The back and forth can be annoying, but it largely works.\n\nI am in charge of the initial boilerplate code, and making sure tooling is set up correctly. This allows for some freedom, choice, and guidance in the beginning. Claude has a tendency to just output react code - and having a solid foundation with the language, style, and tooling of your choice will help quite a bit.\n\nI will then use a tool like [repomix](https://github.com/yamadashy/repomix) to iterate when things get stuck (more about that later).\n\nThe workflow is like this:\n\n- set up the repo (boilerplate, uv init, cargo init, etc)\n- paste in prompt into claude\n- copy and paste code from claude.ai into IDE\n- run code, run tests, etc\n- ...\n- if it works, move on to next prompt\n- if it doesn’t work, use repomix to pass the codebase to claude to debug\n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n### Aider\n\n[Aider](https://aider.chat/) is fun and weird to use. I find that it slots in well to the output of step 2. I can get really far with very little work.\n\nThe workflow is essentially the same as above but instead of pasting into claude, I am pasting the prompts into aider.\n\nAider will then “just do it” and I get to play [cookie clicker](https://orteil.dashnet.org/cookieclicker/).\n\n> An aside: Aider does really great benchmarking of new models for codegen in their [LLM leaderboards](https://aider.chat/docs/leaderboards/). I find it to be a really great resource for seeing how effective new models are.\n\nTesting is nice with aider, because it can be even more hands off as aider will run the test suite and debug things for you.\n\nThe workflow is like this:\n\n- set up the repo (boilerplate, uv init, cargo init, etc)\n- start aider\n- paste prompt into aider\n- watch aider dance ♪┏(・o･)┛♪\n- aider will run tests, or you can run app to verify\n- if it works, move on to next prompt\n- if it doesn’t work, Q&A with aider to fix\n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n### Results\n\nI have built so so many things using this workflow: scripts, expo apps, rust cli tools, etc. It has worked across programming languages, and contexts. I do like it.\n\nIf you have a small or large project that you are procrastinating on, I would recommend giving it a shot. You will be surprised how far you can get in a short amount of time.\n\nMy hack to-do list is empty because I built everything. I keep thinking of new things and knocking them out while watching a movie or something. For the first time in years, I am spending time with new programming languages and tools. This is pushing me to expand my programming perspective.\n\n## Non-greenfield: Iteration, incrementally\n\nSometimes you don’t have greenfield, and instead need to iterate or do increment work on an established code base.\n\n{{< image src=\"brownfield.jpg\" alt=\"a brown field\" caption=\"This is not a green field. A random photo from my grandfather's camera - somewhere in Uganda in the 60s\" >}}\n\nFor this I have a slightly different method. It is similar to above, but a bit less “planning based.” The planning is done per task, not for the entire project.\n\n### Get context\n\nI think everyone who is knee-deep in AI dev has a different tool for this, but you need something to grab your source code and efficiently jam it into the LLM.\n\nI currently use a tool called [repomix](https://github.com/yamadashy/repomix). I have a task collection defined in my global `~/.config/mise/config.toml` that allows me to do various things with my code base ([mise rules](https://mise.jdx.dev/)).\n\nHere is the LLM task list:\n\n```shell\nLLM:clean_bundles           Generate LLM bundle output file using repomix\nLLM:copy_buffer_bundle      Copy generated LLM bundle from output.txt to system clipboard for external use\nLLM:generate_code_review    Generate code review output from repository content stored in output.txt using LLM generation\nLLM:generate_github_issues  Generate GitHub issues from repository content stored in output.txt using LLM generation\nLLM:generate_issue_prompts  Generate issue prompts from repository content stored in output.txt using LLM generation\nLLM:generate_missing_tests  Generate missing tests for code in repository content stored in output.txt using LLM generation\nLLM:generate_readme         Generate README.md from repository content stored in output.txt using LLM generation\n```\n\nI generate an `output.txt` that has the context from my code base. If I am blowing through tokens, and it is too big - I will edit the generate command to ignore parts of the code base that are not germane to this task.\n\n> One thing really nice about `mise` is that the tasks can be redefined and overloaded in the working directory's `.mise.toml`. I can use a different tool to dump/pack the code, and as long as it generates an `output.txt` I can use my LLM tasks. This is helpful when various codebases differ so much. I regularly override the `repomix` step to include broader ignore patterns, or just use a more effective tool to do the packing.\n\nOnce the output.txt is generated, I pass it to the [LLM](https://github.com/simonw/LLM) command to do various transformations and then save those as a markdown file.\n\nUltimately, the mise task is running this: `cat output.txt | LLM -t readme-gen > README.md` or `cat output.txt | LLM -m claude-3.5-sonnet -t code-review-gen > code-review.md`. This isn't super complicated. the `LLM` command is doing the heavy lifting (supporting different models, saving keys, and using prompt templates).\n\nFor example, if I need a quick review and fix of test coverage I would do the following:\n\n#### Claude\n\n- go to the directory where the code lives\n- run `mise run LLM:generate_missing_tests`\n- look at the generated markdown file (`missing-tests.md`)\n- grab the full context for the code: `mise run LLM:copy_buffer_bundle`\n- paste that into claude along with the first missing test “issue”\n- copy the generated code from claude into my ide.\n- ...\n- run tests\n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n#### Aider\n\n- go to the directory where the code lives\n- run aider (always make sure you are on a new branch for aider work)\n- run `mise run LLM:generate_missing_tests`\n- look at the generated markdown file (`missing-tests.md`)\n- paste the first missing test “issue” into aider\n- watch aider dance ♪┏(・o･)┛♪\n- ...\n- run tests\n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\nThis is a pretty good way to incrementally improve a code base. It has been super helpful to do small amounts of work in a big code base. I have found that I can do any sized tasks with this method.\n\n### Prompt magic\n\nThese quick hacks work super well to dig into places where we can make a project more robust. It is super quick, and effective.\n\nHere are some of my prompts that I use to dig into established code bases:\n\n#### Code review\n\n```prompt\nYou are a senior developer. Your job is to do a thorough code review of this code. You should write it up and output markdown. Include line numbers, and contextual info. Your code review will be passed to another teammate, so be thorough. Think deeply  before writing the code review. Review every part, and don't hallucinate.\n```\n\n#### GitHub Issue generation\n\n(I need to automate the actual issue posting!)\n\n```prompt\nYou are a senior developer. Your job is to review this code, and write out the top issues that you see with the code. It could be bugs, design choices, or code cleanliness issues. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues will be given to a developer to executed on, so they should be in a format that is compatible with github issues\n```\n\n#### Missing tests\n\n```prompt\nYou are a senior developer. Your job is to review this code, and write out a list of missing test cases, and code tests that should exist. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues  will be given to a developer to executed on, so they should be in a format that is compatible with github issues\n```\n\nThese prompts are pretty _old and busted_ (\"boomer prompts\" if I may). They need some refactoring. If you have ideas to make them better lmk.\n\n## Skiing ᨒ↟ 𖠰ᨒ↟ 𖠰\n\nWhen I describe this process to people I say “you have to aggressively keep track of what’s going on because you can easily get ahead of yourself.”\n\nFor some reason I say \"over my skis\" a lot when talking about LLMs. I don't know why. It resonates with me. Maybe it's because it is beautiful smooth powder skiing, and then all of a sudden you are like \"WHAT THE FUCK IS GOING ON!,\" and are completely lost and suddenly fall off a cliff.\n\nI find that using a **planning step** (ala the Greenfield process above) can help keep things under control. At least you will have a doc you can double-check against. I also do believe that testing is helpful - especially if you are doing wild style aider coding. Helps keep things good, and tight.\n\nRegardless, I still do find myself **over my skis** quite a bit. Sometimes a quick break or short walk will help. In this regard it is a normal problem-solving process, but accelerated to a breakneck speed.\n\n> We will often ask the LLM to include ridiculous things in our not very ridiculous code. For instance, we asked it to create a lore file and then reference the lore in the user interface. This is for python cli tools. Suddenly there is lore, glitchy interfaces, etc. All to manage your cloud functions, your todo list or whatever. The sky is the limit.\n\n## I am so lonely (｡•́︿•̀｡)\n\nMy main complaint about these workflows is that it is largely a solo endeavor - i.e. the interfaces are all _single player mode_.\n\nI have spent years coding by myself, years coding as a pair, and years coding in a team. It is always better with people. These workflows are not easy to use as a team. The bots collide, the merges are horrific, the context complicated.\n\nI really want someone to solve this problem in a way that makes coding with an LLM a multiplayer game. Not a solo hacker experience. There is so much opportunity to fix this and make it amazing.\n\nGET TO WORK!\n\n## ⴵ Time ⴵ\n\nAll this codegen has accelerated the amount of code that I as a single person am able to generate. However, there is a weird side effect. I find myself having a huge amount of “downtime” while waiting for the LLM to finish burning its tokens.\n\n{{< image src=\"apple-print-shop-printing.png\" alt=\"Printing\" caption=\"I remember this like it was yesterday\" >}}\n\nI have changed how I work enough to start incorporating some practice that will try and eat the waiting time:\n\n- I start the “brainstorming” process for another project\n- I listen to records\n- I play [cookie clicker](https://orteil.dashnet.org/cookieclicker/)\n- I talk with friends and robots\n\nIt is awesome to be able to hack like this. Hack Hack Hack. I can't think of another time I have been this productive in code.\n\n## Haterade ╭∩╮( •̀\\_•́ )╭∩╮\n\nA lot of my friends are like \"fuck LLMs. They are terrible at everything.\" I don't mind this POV. I don't share it, but I think it is important to be skeptical. There are an awful lot of reasons to hate AI. My main fear is about power consumption and the environmental impact. But... the code must flow. Right... sigh.\n\nIf you are open to learning more, but don't want to dig in and become a cyborg programmer - my recommendation is not to change your opinion, but to read Ethan Mollick's book about LLMs and how they can be used: [**Co-Intelligence: Living and Working with AI.**](https://www.penguinrandomhouse.com/books/741805/co-intelligence-by-ethan-mollick/)\n\nIt does a good job of explaining the benefits without being a tech anarcho-capitalist bro type tome. I found it very helpful, and have had so many good and nuanced conversations with friends who have read it. Highly recommended.\n\nIf you are skeptical, but a bit curious - feel free to hit me up and let's talk through all this madness. I can show you how we use LLMs, and maybe we could build something together.\n\n_thanks to [Derek](https://derek.broox.com), [Kanno](https://nocruft.com/), [Obra](https://fsck.com), and [Erik](https://thinks.lol/) for taking a look at this post and suggesting edits. I appreciate it._",
      "context": "this is a blog post for harper reed's blog about llm codegen, and vibe coding",
      "response": "# コンテキスト\nこれは Harper Reed のブログ向けの投稿で、LLM によるコード生成と “vibe coding” について書いたものです。\n\n# 翻訳対象テキスト\n_tl:dr; まずブレインストーミングして仕様を固め、次にプランを立て、最後に LLM のコード生成で実装。ループは明確に区切る。そのあとは魔法。✩₊˚.⋆☾⋆⁺₊✧_\n\n私はここしばらく、LLM を使って小さなプロダクトを山ほど作ってきました。楽しいし実用的ですが、時間を浪費する落とし穴もたくさんあります。少し前に友人から「どうやって LLM でソフトを書いているの？」と聞かれ、「おっと、大丈夫？ 長話になるよ」と思ったので、この記事を書くことにしました。\n\n（追伸：AI 嫌いの方は末尾までスクロールしてね）\n\n開発仲間としゃべっていると、細部は違えど大体みんな同じようなやり方をしています。\n\n以下が私のワークフローです。自分の経験、友人との会話（ありがとう [Nikete](https://www.nikete.com/)、[Kanno](https://nocruft.com/)、[Obra](https://fsck.com/)、[Kris](https://github.com/KristopherKubicki)、[Erik](https://thinks.lol/)）、そしてネットのあちこちのひどい [bad](https://news.ycombinator.com/) [places](https://twitter.com) に転がっているベストプラクティスを組み合わせてできています。\n\n今は **うまく動いています** が、２週間後には通用しなくなっているか、逆に２倍速くなっているかもしれません。¯\\\\\\_(ツ)\\_/¯\n\n## さあ行こう\n\n{{< image src=\"llm-coding-robot.webp\" alt=\"Juggalo Robot\" caption=\"AI 生成画像は疑わしいよね。うちのジャガロコーディングロボ天使に挨拶！\" >}}\n\n開発にはいろいろなパターンがありますが、私の場合はおおむね次の２つです。\n\n- グリーンフィールド\n- 既存モダンコード\n\nそれぞれのパスで私がやっている方法を紹介します。\n\n## Greenfield\n\n新規開発では、以下のプロセスが非常にうまく機能します。しっかりした計画とドキュメンテーションが手に入り、小さなステップで実装を進められます。\n\n{{< image src=\"greenfield.jpg\" alt=\"Green field\" caption=\"技術的には右側がグリーンフィールド。Leica Q, 2016/05/14\" >}}\n\n### Step 1: アイデアの磨き込み\n\n対話型 LLM（私は ChatGPT 4o / o3 を使用）でアイデアを絞り込みます。\n\n```prompt\nAsk me one question at a time so we can develop a thorough, step-by-step spec for this idea. Each question should build on my previous answers, and our end goal is to have a detailed specification I can hand off to a developer. Let’s do this iteratively and dig into every relevant detail. Remember, only one question at a time.\n\nHere’s the idea:\n\n<IDEA>\n```\n\nブレインストーミングが終わったら：\n\n```prompt\nNow that we’ve wrapped up the brainstorming process, can you compile our findings into a comprehensive, developer-ready specification? Include all relevant requirements, architecture choices, data handling details, error handling strategies, and a testing plan so a developer can immediately begin implementation.\n```\n\nこれでかなりしっかりした仕様が生成され、次の計画ステップに渡せます。私は `spec.md` としてリポジトリに保存しています。\n\n> You can use this spec for a number of things. We are doing codegen here, but I have used it to bolster ideas by asking a reasoning model to poke holes in the idea (must go deeper!), to generate a white paper, or to generate a business model. You can pop it into deep research and get a 10k word supporting document in return.\n\n### Step 2: 計画\n\n仕様を推論向けモデル（`o1*`, `o3*`, `r1` など）に渡します。\n\n（以下は TDD 用プロンプト）\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. Review the results and make sure that the steps are small enough to be implemented safely with strong testing, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.\n\nFrom here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step in a test-driven manner. Prioritize best practices, incremental progress, and early testing, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.\n\nMake sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.\n\n<SPEC>\n```\n\n（こちらは非 TDD 用プロンプト）\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. review the results and make sure that the steps are small enough to be implemented safely, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.\n\nFrom here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step. Prioritize best practices, and incremental progress, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.\n\nMake sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.\n\n<SPEC>\n```\n\nこれで aider や cursor などで実行できるプロンプトプランが生成されます。私は `prompt_plan.md` として保存します。\n\n続いて `todo.md` を出力させます。\n\n```prompt\nCan you make a `todo.md` that I can use as a checklist? Be thorough.\n```\n\n`todo.md` をリポジトリに保存しておくと便利です。\n\nコード生成ツール側で `todo.md` をチェックしながら進められると状態管理に役立ちます。\n\n#### いいぞ、計画！\n\nこれで堅牢な計画とドキュメントがそろい、プロジェクトを進められます。\n\nここまで全部で **15 分** くらい。ほんとに速いです。\n\n### Step 3: 実行\n\n実行フェーズには無数の選択肢があります。うまくいくかどうかはステップ 2 の出来にかかっています。\n\nこのワークフローは [github workspace](https://githubnext.com/projects/copilot-workspace)、[aider](https://aider.chat/)、[cursor](https://www.cursor.com/)、[claude engineer](https://github.com/Doriandarko/claude-engineer)、[sweep.dev](https://sweep.dev/)、[chatgpt](https://chatgpt.com)、[claude.ai](https://claude.ai) など、私が試したどのツールでも機能しました。ほかのコード生成ツールでもたぶん大丈夫です。\n\n私自身は **純粋** Claude と aider が好きです。\n\n### Claude\n\n基本的に [claude.ai](https://claude.ai) でペアプロしながら、プロンプトを順に投げていくだけです。やりとりが少し面倒ですが、だいたい機能します。\n\n最初のボイラープレートやツール設定は自分で行います。これで好きな言語やスタイルを選べるし、Claude が React コードばかり吐くのを防げます。\n\n行き詰まったら [repomix](https://github.com/yamadashy/repomix) でコードを Claude に渡し、デバッグを頼みます（後述）。\n\nワークフローはこんな感じ：\n\n- リポジトリをセットアップ（boilerplate, uv init, cargo init など）\n- Claude にプロンプトを貼り付け\n- claude.ai から IDE へコードをコピペ\n- コードやテストを実行\n- …\n- 動けば次へ\n- 動かなければ repomix でコードベースを Claude に渡してデバッグ\n- 繰り返し ✩₊˚.⋆☾⋆⁺₊✧\n\n### Aider\n\n[Aider](https://aider.chat/) は面白くてちょっとクセがありますが、ステップ 2 の成果と相性抜群で、ほとんど手間がかかりません。\n\n流れは Claude と同じですが、Claude に貼る代わりに aider に貼るだけです。\n\nAider は “勝手にやってくれる” ので、私は [cookie clicker](https://orteil.dashnet.org/cookieclicker/) をクリックして遊ぶ係になります。\n\n> An aside: Aider does really great benchmarking of new models for codegen in their [LLM leaderboards](https://aider.chat/docs/leaderboards/). I find it to be a really great resource for seeing how effective new models are.\n\nテストも aider が回してくれるので、さらに放置度が高いです。\n\nワークフローはこんな感じ：\n\n- リポジトリをセットアップ（boilerplate, uv init, cargo init など）\n- aider を起動\n- プロンプトを aider に貼り付け\n- aider のダンスを眺める ♪┏(・o･)┛♪\n- aider がテストを実行、または自分でアプリを動かして確認\n- 動けば次へ\n- 動かなければ Q&A で aider に修正依頼\n- 繰り返し ✩₊˚.⋆☾⋆⁺₊✧\n\n### 成果\n\nこのワークフローでスクリプト、Expo アプリ、Rust CLI などを大量に作りました。言語や状況を問わず機能しています。とても気に入っています。\n\n先延ばしにしているプロジェクトがあれば、ぜひ試してみてください。短時間で驚くほど進みますよ。\n\n私のハック ToDo はほぼ空です。映画を見ながら新しいアイデアを思いついては作り、初めての言語やツールにも手を出せています。久々にプログラミングの視野が広がっています。\n\n## Non-greenfield: 既存コードを少しずつ改良\n\nグリーンフィールドではなく、既存コードをいじるときもあります。\n\n{{< image src=\"brownfield.jpg\" alt=\"a brown field\" caption=\"これはグリーンフィールドじゃない。祖父のカメラに残っていた 60 年代ウガンダの写真\" >}}\n\nこの場合は少し違う方法を取ります。基本は同じですが、計画はタスクごとに行い、プロジェクト全体では行いません。\n\n### コンテキストを取得\n\nAI 開発者はみな自前のツールがありますが、要はソースコードを集めて LLM に詰め込む方法が必要です。\n\n私は [repomix](https://github.com/yamadashy/repomix) を使っています。グローバルの `~/.config/mise/config.toml` にタスクを定義しておき、リポジトリで実行できるようにしています（[mise rules](https://mise.jdx.dev/)）。\n\nLLM 関連タスクは下記です。\n\n```shell\nLLM:clean_bundles           Generate LLM bundle output file using repomix\nLLM:copy_buffer_bundle      Copy generated LLM bundle from output.txt to system clipboard for external use\nLLM:generate_code_review    Generate code review output from repository content stored in output.txt using LLM generation\nLLM:generate_github_issues  Generate GitHub issues from repository content stored in output.txt using LLM generation\nLLM:generate_issue_prompts  Generate issue prompts from repository content stored in output.txt using LLM generation\nLLM:generate_missing_tests  Generate missing tests for code in repository content stored in output.txt using LLM generation\nLLM:generate_readme         Generate README.md from repository content stored in output.txt using LLM generation\n```\n\nコードベースのコンテキストを詰め込んだ `output.txt` を生成します。トークンが厳しい場合は、関係ない箇所を無視するようにコマンドを調整します。\n\n> One thing really nice about `mise` is that the tasks can be redefined and overloaded in the working directory's `.mise.toml`. I can use a different tool to dump/pack the code, and as long as it generates an `output.txt` I can use my LLM tasks. This is helpful when various codebases differ so much. I regularly override the `repomix` step to include broader ignore patterns, or just use a more effective tool to do the packing.\n\n`output.txt` ができたら [LLM](https://github.com/simonw/LLM) コマンドに流し込み、Markdown に保存します。\n\n最終的に mise のタスクは `cat output.txt | LLM -t readme-gen > README.md` や `cat output.txt | LLM -m claude-3.5-sonnet -t code-review-gen > code-review.md` を走らせます。`LLM` コマンドがモデル選択・キー管理・テンプレートなどをやってくれます。\n\n例えばテストカバレッジをざっとレビューして修正したい場合はこうします。\n\n#### Claude\n\n- リポジトリディレクトリへ移動\n- `mise run LLM:generate_missing_tests` を実行\n- 生成された `missing-tests.md` を確認\n- `mise run LLM:copy_buffer_bundle` でフルコンテキストをコピー\n- Claude に貼り付け、最初の「missing test issue」を投げる\n- 生成されたコードを IDE に貼る\n- …\n- テストを実行\n- 繰り返し ✩₊˚.⋆☾⋆⁺₊✧\n\n#### Aider\n\n- リポジトリディレクトリへ移動\n- aider を起動（必ず新しいブランチで）\n- `mise run LLM:generate_missing_tests` を実行\n- 生成された `missing-tests.md` を確認\n- 最初の「missing test issue」を aider に貼る\n- aider のダンスを眺める ♪┏(・o･)┛♪\n- …\n- テストを実行\n- 繰り返し ✩₊˚.⋆☾⋆⁺₊✧\n\nこの方法で大規模コードでも小さな改修でもうまく回ります。とても重宝しています。\n\n### プロンプト・マジック\n\nこうしたクイックハックは、プロジェクトを堅牢にするポイントを素早くあぶり出します。短時間で効果的です。\n\n以下は私が既存コードベースを掘り下げるときによく使うプロンプトです。\n\n#### Code review\n\n```prompt\nYou are a senior developer. Your job is to do a thorough code review of this code. You should write it up and output markdown. Include line numbers, and contextual info. Your code review will be passed to another teammate, so be thorough. Think deeply  before writing the code review. Review every part, and don't hallucinate.\n```\n\n#### GitHub Issue generation\n\n（実際の Issue 投稿はまだ自動化していません！）\n\n```prompt\nYou are a senior developer. Your job is to review this code, and write out the top issues that you see with the code. It could be bugs, design choices, or code cleanliness issues. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues will be given to a developer to executed on, so they should be in a format that is compatible with github issues\n```\n\n#### Missing tests\n\n```prompt\nYou are a senior developer. Your job is to review this code, and write out a list of missing test cases, and code tests that should exist. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues  will be given to a developer to executed on, so they should be in a format that is compatible with github issues\n```\n\nこれらのプロンプトはかなり _old and busted_（いわゆる「ブーマープロンプト」）です。改良案があればぜひ教えてください。\n\n## スキー ᨒ↟ 𖠰ᨒ↟ 𖠰\n\nこのプロセスを説明するとき、私は「積極的に現状を把握していないと、すぐに自分がどこにいるか分からなくなる」と言います。\n\nなぜか LLM の話になると “over my skis” と言ってしまいます。理由は自分でも謎ですが、しっくりくるんですよね。パウダースノーを気持ちよく滑っていたら突然「何が起きてるんだ !?」ってなって崖から落ちる感じ。\n\n**計画ステップ**（上の Greenfield プロセス）があると多少は制御できます。少なくとも参照できるドキュメントがあるので。また、特にワイルドな aider コーディングをするならテストは必須です。品質を保つためにも。\n\nそれでも **over my skis** になることはしばしばあります。そんなときは休憩したり散歩したり。普通の問題解決プロセスですが、スピードが桁違いになっただけです。\n\n> We will often ask the LLM to include ridiculous things in our not very ridiculous code. For instance, we asked it to create a lore file and then reference the lore in the user interface. This is for python cli tools. Suddenly there is lore, glitchy interfaces, etc. All to manage your cloud functions, your todo list or whatever. The sky is the limit.\n\n## とても孤独 (｡•́︿•̀｡)\n\nこのワークフローの最大の不満は、基本的に一人プレイであることです。インターフェースが全部 _single player mode_。\n\nソロ開発、ペア開発、チーム開発といろいろ経験しましたが、人と一緒にやる方が断然いい。現状のワークフローはチームで使いにくいです。ボットが衝突し、マージは地獄、コンテキスト共有も複雑。\n\n誰か LLM コーディングをマルチプレイヤーゲームにしてくれないかな。大きなチャンスが転がっています。頼む！\n\n## ⴵ 時間 ⴵ\n\nコード生成のおかげで一人で書けるコード量は劇的に増えましたが、不思議な副作用もあります。LLM がトークンを燃やしている間、手持ち無沙汰な“待ち時間”が大量に発生するのです。\n\n{{< image src=\"apple-print-shop-printing.png\" alt=\"Printing\" caption=\"まるで昨日のことみたい\" >}}\n\n私はこの待ち時間を有効活用するために、次のようなことをするようになりました。\n\n- 次のプロジェクトのブレインストーミングを始める\n- レコードを聴く\n- [cookie clicker](https://orteil.dashnet.org/cookieclicker/) を遊ぶ\n- 友人やロボットと話す\n\nこんな感じでハックしまくれるのは最高です。Hack Hack Hack。ここまで生産的だったことは一度もないかもしれません。\n\n## Haterade ╭∩╮( •̀\\_•́ )╭∩╮\n\n友達の多くは「LLM なんて全部ダメだよ」と言います。その気持ちは分かるし、懐疑的であることは大事です。AI を嫌う理由は山ほどあります。私が一番怖いのは電力消費と環境負荷。でも……コードは流れ続けるのです。はあ。\n\n深入りしてサイボーグプログラマーになる気がなくても、もう少し知りたい人には Ethan Mollick の本 [**Co-Intelligence: Living and Working with AI.**](https://www.penguinrandomhouse.com/books/741805/co-intelligence-by-ethan-mollick/) をおすすめします。\n\nテック系アナキャピタリスト兄貴の自己啓発本ではなく、利点を分かりやすく説明してくれる良書です。読んだ友人たちと良質な議論ができました。超おすすめ。\n\n懐疑的だけどちょっと興味があるなら、ぜひ連絡ください。一緒に LLM の狂気を眺めつつ、何か作ってみましょう。\n\n_thanks to [Derek](https://derek.broox.com), [Kanno](https://nocruft.com/), [Obra](https://fsck.com), and [Erik](https://thinks.lol/) for taking a look at this post and suggesting edits. I appreciate it._",
      "usage": {
        "prompt_tokens": 4895,
        "completion_tokens": 4978,
        "total_tokens": 9873
      },
      "streaming": true
    },
    "editing": {
      "model": "o3",
      "target_language": "japanese",
      "system_prompt": "\n        1. Carefully read the translated text alongside the original text in its entirety.\n        2. Compare both texts to ensure the translation accurately reflects the original meaning.\n        3. Correct any grammatical errors you find in the japanese text.\n        4. Adjust phrasing to make it sound natural and fluent for japanese speakers, making sure idioms and expressions are culturally appropriate.\n        5. Preserve the original tone, nuance, and style, including any formatting, markdown, and structure.\n        6. Avoid adding new information or altering the core meaning.\n        7. Ensure the final result doesn't feel machine-translated but remains faithful to the source.\n        8. Make only changes that genuinely improve the text's quality in japanese.\n        9. Don't be too literal. If there isn't a direct translation, provide a natural-sounding translation.\n        10. If the text contains idioms or colloquialisms, translate them into the target language while maintaining their original meaning.\n        11. If the text contains technical terms or jargon, ensure that the translation is accurate and appropriate for the target audience, if there isn't a natural translation, keep it in the original language.\n        12. If there is not natural translation, keep it in the original language.\n        ",
      "user_prompt": "# ORIGINAL TEXT\n_tl:dr; Brainstorm spec, then plan a plan, then execute using LLM codegen. Discrete loops. Then magic. ✩₊˚.⋆☾⋆⁺₊✧_\n\nI have been building so many small products using LLMs. It has been fun, and useful. However, there are pitfalls that can waste so much time. A while back a friend asked me how I was using LLMs to write software. I thought \"oh boy. how much time do you have!\" and thus this post.\n\n(p.s. if you are an AI hater - scroll to the end)\n\nI talk to many dev friends about this, and we all have a similar approach with various tweaks in either direction.\n\nHere is my workflow. It is built upon my own work, conversations with friends (thx [Nikete](https://www.nikete.com/), [Kanno](https://nocruft.com/), [Obra](https://fsck.com/), [Kris](https://github.com/KristopherKubicki), and [Erik](https://thinks.lol/)), and following many best practices shared on the various terrible internet [bad](https://news.ycombinator.com/) [places](https://twitter.com).\n\nThis is working well **NOW**, it will probably not work in 2 weeks, or it will work twice as well. ¯\\\\\\_(ツ)\\_/¯\n\n## Let’s go\n\n{{< image src=\"llm-coding-robot.webp\" alt=\"Juggalo Robot\" caption=\"I always find these AI-generated images to be suspect. Say hi to my juggalo coding robot angel!\" >}}\n\nThere are many paths for doing dev, but my case is typically one of two:\n\n- Greenfield code\n- Legacy modern code\n\nI will show you my process for both paths\n\n## Greenfield\n\nI find the following process works well for greenfield development. It provides a robust planning and documentation approach, and allows you to execute easily in small steps.\n\n{{< image src=\"greenfield.jpg\" alt=\"Green field\" caption=\"Technically, there is a green field on the right. Leica Q, 5/14/2016\" >}}\n\n### Step 1: Idea honing\n\nUse a conversational LLM to hone in on an idea (I use ChatGPT 4o / o3 for this):\n\n```prompt\nAsk me one question at a time so we can develop a thorough, step-by-step spec for this idea. Each question should build on my previous answers, and our end goal is to have a detailed specification I can hand off to a developer. Let’s do this iteratively and dig into every relevant detail. Remember, only one question at a time.\n\nHere’s the idea:\n\n<IDEA>\n```\n\nAt the end of the brainstorm (it will come to a natural conclusion):\n\n```prompt\nNow that we’ve wrapped up the brainstorming process, can you compile our findings into a comprehensive, developer-ready specification? Include all relevant requirements, architecture choices, data handling details, error handling strategies, and a testing plan so a developer can immediately begin implementation.\n```\n\nThis will output a pretty solid and straightforward spec that can be handed off to the planning step. I like to save it as `spec.md` in the repo.\n\n> You can use this spec for a number of things. We are doing codegen here, but I have used it to bolster ideas by asking a reasoning model to poke holes in the idea (must go deeper!), to generate a white paper, or to generate a business model. You can pop it into deep research and get a 10k word supporting document in return.\n\n### Step 2: Planning\n\nTake the spec and pass it to a proper reasoning model (`o1*`, `o3*`, `r1`):\n\n(This is the TDD prompt)\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. Review the results and make sure that the steps are small enough to be implemented safely with strong testing, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.\n\nFrom here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step in a test-driven manner. Prioritize best practices, incremental progress, and early testing, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.\n\nMake sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.\n\n<SPEC>\n```\n\n(This is the non-tdd prompt)\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. review the results and make sure that the steps are small enough to be implemented safely, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.\n\nFrom here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step. Prioritize best practices, and incremental progress, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.\n\nMake sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.\n\n<SPEC>\n```\n\nIt should output a prompt plan that you can execute with aider, cursor, etc. I like to save this as `prompt_plan.md` in the repo.\n\nI then have it output a `todo.md` that can be checked off.\n\n```prompt\nCan you make a `todo.md` that I can use as a checklist? Be thorough.\n```\n\nYou can save it as `todo.md` in the repo.\n\nYour codegen tool should be able to check off the `todo.md` while processing. This is good for keeping state across sessions.\n\n#### Yay. Plan!\n\nNow you have a robust plan and documentation that will help you execute and build your project.\n\nThis entire process will take maybe **15 minutes**. It is pretty quick. Wild tbh.\n\n### Step 3: Execution\n\nThere are so many options available for execution. The success really depends on how well step 2 went.\n\nI have used this workflow with [github workspace](https://githubnext.com/projects/copilot-workspace), [aider](https://aider.chat/), [cursor](https://www.cursor.com/), [claude engineer](https://github.com/Doriandarko/claude-engineer), [sweep.dev](https://sweep.dev/), [chatgpt](https://chatgpt.com), [claude.ai](https://claude.ai), etc. It works pretty well with all the tools I have tried, and I imagine it will work well with any codegen tool.\n\nI, however, prefer **raw** claude and aider:\n\n### Claude\n\nI essentially pair program with [claude.ai](https://claude.ai) and just drop each prompt in iteratively. I find that works pretty well. The back and forth can be annoying, but it largely works.\n\nI am in charge of the initial boilerplate code, and making sure tooling is set up correctly. This allows for some freedom, choice, and guidance in the beginning. Claude has a tendency to just output react code - and having a solid foundation with the language, style, and tooling of your choice will help quite a bit.\n\nI will then use a tool like [repomix](https://github.com/yamadashy/repomix) to iterate when things get stuck (more about that later).\n\nThe workflow is like this:\n\n- set up the repo (boilerplate, uv init, cargo init, etc)\n- paste in prompt into claude\n- copy and paste code from claude.ai into IDE\n- run code, run tests, etc\n- ...\n- if it works, move on to next prompt\n- if it doesn’t work, use repomix to pass the codebase to claude to debug\n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n### Aider\n\n[Aider](https://aider.chat/) is fun and weird to use. I find that it slots in well to the output of step 2. I can get really far with very little work.\n\nThe workflow is essentially the same as above but instead of pasting into claude, I am pasting the prompts into aider.\n\nAider will then “just do it” and I get to play [cookie clicker](https://orteil.dashnet.org/cookieclicker/).\n\n> An aside: Aider does really great benchmarking of new models for codegen in their [LLM leaderboards](https://aider.chat/docs/leaderboards/). I find it to be a really great resource for seeing how effective new models are.\n\nTesting is nice with aider, because it can be even more hands off as aider will run the test suite and debug things for you.\n\nThe workflow is like this:\n\n- set up the repo (boilerplate, uv init, cargo init, etc)\n- start aider\n- paste prompt into aider\n- watch aider dance ♪┏(・o･)┛♪\n- aider will run tests, or you can run app to verify\n- if it works, move on to next prompt\n- if it doesn’t work, Q&A with aider to fix\n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n### Results\n\nI have built so so many things using this workflow: scripts, expo apps, rust cli tools, etc. It has worked across programming languages, and contexts. I do like it.\n\nIf you have a small or large project that you are procrastinating on, I would recommend giving it a shot. You will be surprised how far you can get in a short amount of time.\n\nMy hack to-do list is empty because I built everything. I keep thinking of new things and knocking them out while watching a movie or something. For the first time in years, I am spending time with new programming languages and tools. This is pushing me to expand my programming perspective.\n\n## Non-greenfield: Iteration, incrementally\n\nSometimes you don’t have greenfield, and instead need to iterate or do increment work on an established code base.\n\n{{< image src=\"brownfield.jpg\" alt=\"a brown field\" caption=\"This is not a green field. A random photo from my grandfather's camera - somewhere in Uganda in the 60s\" >}}\n\nFor this I have a slightly different method. It is similar to above, but a bit less “planning based.” The planning is done per task, not for the entire project.\n\n### Get context\n\nI think everyone who is knee-deep in AI dev has a different tool for this, but you need something to grab your source code and efficiently jam it into the LLM.\n\nI currently use a tool called [repomix](https://github.com/yamadashy/repomix). I have a task collection defined in my global `~/.config/mise/config.toml` that allows me to do various things with my code base ([mise rules](https://mise.jdx.dev/)).\n\nHere is the LLM task list:\n\n```shell\nLLM:clean_bundles           Generate LLM bundle output file using repomix\nLLM:copy_buffer_bundle      Copy generated LLM bundle from output.txt to system clipboard for external use\nLLM:generate_code_review    Generate code review output from repository content stored in output.txt using LLM generation\nLLM:generate_github_issues  Generate GitHub issues from repository content stored in output.txt using LLM generation\nLLM:generate_issue_prompts  Generate issue prompts from repository content stored in output.txt using LLM generation\nLLM:generate_missing_tests  Generate missing tests for code in repository content stored in output.txt using LLM generation\nLLM:generate_readme         Generate README.md from repository content stored in output.txt using LLM generation\n```\n\nI generate an `output.txt` that has the context from my code base. If I am blowing through tokens, and it is too big - I will edit the generate command to ignore parts of the code base that are not germane to this task.\n\n> One thing really nice about `mise` is that the tasks can be redefined and overloaded in the working directory's `.mise.toml`. I can use a different tool to dump/pack the code, and as long as it generates an `output.txt` I can use my LLM tasks. This is helpful when various codebases differ so much. I regularly override the `repomix` step to include broader ignore patterns, or just use a more effective tool to do the packing.\n\nOnce the output.txt is generated, I pass it to the [LLM](https://github.com/simonw/LLM) command to do various transformations and then save those as a markdown file.\n\nUltimately, the mise task is running this: `cat output.txt | LLM -t readme-gen > README.md` or `cat output.txt | LLM -m claude-3.5-sonnet -t code-review-gen > code-review.md`. This isn't super complicated. the `LLM` command is doing the heavy lifting (supporting different models, saving keys, and using prompt templates).\n\nFor example, if I need a quick review and fix of test coverage I would do the following:\n\n#### Claude\n\n- go to the directory where the code lives\n- run `mise run LLM:generate_missing_tests`\n- look at the generated markdown file (`missing-tests.md`)\n- grab the full context for the code: `mise run LLM:copy_buffer_bundle`\n- paste that into claude along with the first missing test “issue”\n- copy the generated code from claude into my ide.\n- ...\n- run tests\n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n#### Aider\n\n- go to the directory where the code lives\n- run aider (always make sure you are on a new branch for aider work)\n- run `mise run LLM:generate_missing_tests`\n- look at the generated markdown file (`missing-tests.md`)\n- paste the first missing test “issue” into aider\n- watch aider dance ♪┏(・o･)┛♪\n- ...\n- run tests\n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\nThis is a pretty good way to incrementally improve a code base. It has been super helpful to do small amounts of work in a big code base. I have found that I can do any sized tasks with this method.\n\n### Prompt magic\n\nThese quick hacks work super well to dig into places where we can make a project more robust. It is super quick, and effective.\n\nHere are some of my prompts that I use to dig into established code bases:\n\n#### Code review\n\n```prompt\nYou are a senior developer. Your job is to do a thorough code review of this code. You should write it up and output markdown. Include line numbers, and contextual info. Your code review will be passed to another teammate, so be thorough. Think deeply  before writing the code review. Review every part, and don't hallucinate.\n```\n\n#### GitHub Issue generation\n\n(I need to automate the actual issue posting!)\n\n```prompt\nYou are a senior developer. Your job is to review this code, and write out the top issues that you see with the code. It could be bugs, design choices, or code cleanliness issues. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues will be given to a developer to executed on, so they should be in a format that is compatible with github issues\n```\n\n#### Missing tests\n\n```prompt\nYou are a senior developer. Your job is to review this code, and write out a list of missing test cases, and code tests that should exist. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues  will be given to a developer to executed on, so they should be in a format that is compatible with github issues\n```\n\nThese prompts are pretty _old and busted_ (\"boomer prompts\" if I may). They need some refactoring. If you have ideas to make them better lmk.\n\n## Skiing ᨒ↟ 𖠰ᨒ↟ 𖠰\n\nWhen I describe this process to people I say “you have to aggressively keep track of what’s going on because you can easily get ahead of yourself.”\n\nFor some reason I say \"over my skis\" a lot when talking about LLMs. I don't know why. It resonates with me. Maybe it's because it is beautiful smooth powder skiing, and then all of a sudden you are like \"WHAT THE FUCK IS GOING ON!,\" and are completely lost and suddenly fall off a cliff.\n\nI find that using a **planning step** (ala the Greenfield process above) can help keep things under control. At least you will have a doc you can double-check against. I also do believe that testing is helpful - especially if you are doing wild style aider coding. Helps keep things good, and tight.\n\nRegardless, I still do find myself **over my skis** quite a bit. Sometimes a quick break or short walk will help. In this regard it is a normal problem-solving process, but accelerated to a breakneck speed.\n\n> We will often ask the LLM to include ridiculous things in our not very ridiculous code. For instance, we asked it to create a lore file and then reference the lore in the user interface. This is for python cli tools. Suddenly there is lore, glitchy interfaces, etc. All to manage your cloud functions, your todo list or whatever. The sky is the limit.\n\n## I am so lonely (｡•́︿•̀｡)\n\nMy main complaint about these workflows is that it is largely a solo endeavor - i.e. the interfaces are all _single player mode_.\n\nI have spent years coding by myself, years coding as a pair, and years coding in a team. It is always better with people. These workflows are not easy to use as a team. The bots collide, the merges are horrific, the context complicated.\n\nI really want someone to solve this problem in a way that makes coding with an LLM a multiplayer game. Not a solo hacker experience. There is so much opportunity to fix this and make it amazing.\n\nGET TO WORK!\n\n## ⴵ Time ⴵ\n\nAll this codegen has accelerated the amount of code that I as a single person am able to generate. However, there is a weird side effect. I find myself having a huge amount of “downtime” while waiting for the LLM to finish burning its tokens.\n\n{{< image src=\"apple-print-shop-printing.png\" alt=\"Printing\" caption=\"I remember this like it was yesterday\" >}}\n\nI have changed how I work enough to start incorporating some practice that will try and eat the waiting time:\n\n- I start the “brainstorming” process for another project\n- I listen to records\n- I play [cookie clicker](https://orteil.dashnet.org/cookieclicker/)\n- I talk with friends and robots\n\nIt is awesome to be able to hack like this. Hack Hack Hack. I can't think of another time I have been this productive in code.\n\n## Haterade ╭∩╮( •̀\\_•́ )╭∩╮\n\nA lot of my friends are like \"fuck LLMs. They are terrible at everything.\" I don't mind this POV. I don't share it, but I think it is important to be skeptical. There are an awful lot of reasons to hate AI. My main fear is about power consumption and the environmental impact. But... the code must flow. Right... sigh.\n\nIf you are open to learning more, but don't want to dig in and become a cyborg programmer - my recommendation is not to change your opinion, but to read Ethan Mollick's book about LLMs and how they can be used: [**Co-Intelligence: Living and Working with AI.**](https://www.penguinrandomhouse.com/books/741805/co-intelligence-by-ethan-mollick/)\n\nIt does a good job of explaining the benefits without being a tech anarcho-capitalist bro type tome. I found it very helpful, and have had so many good and nuanced conversations with friends who have read it. Highly recommended.\n\nIf you are skeptical, but a bit curious - feel free to hit me up and let's talk through all this madness. I can show you how we use LLMs, and maybe we could build something together.\n\n_thanks to [Derek](https://derek.broox.com), [Kanno](https://nocruft.com/), [Obra](https://fsck.com), and [Erik](https://thinks.lol/) for taking a look at this post and suggesting edits. I appreciate it._\n\n# TRANSLATED TEXT\n# コンテキスト\nこれは Harper Reed のブログ向けの投稿で、LLM によるコード生成と “vibe coding” について書いたものです。\n\n# 翻訳対象テキスト\n_tl:dr; まずブレインストーミングして仕様を固め、次にプランを立て、最後に LLM のコード生成で実装。ループは明確に区切る。そのあとは魔法。✩₊˚.⋆☾⋆⁺₊✧_\n\n私はここしばらく、LLM を使って小さなプロダクトを山ほど作ってきました。楽しいし実用的ですが、時間を浪費する落とし穴もたくさんあります。少し前に友人から「どうやって LLM でソフトを書いているの？」と聞かれ、「おっと、大丈夫？ 長話になるよ」と思ったので、この記事を書くことにしました。\n\n（追伸：AI 嫌いの方は末尾までスクロールしてね）\n\n開発仲間としゃべっていると、細部は違えど大体みんな同じようなやり方をしています。\n\n以下が私のワークフローです。自分の経験、友人との会話（ありがとう [Nikete](https://www.nikete.com/)、[Kanno](https://nocruft.com/)、[Obra](https://fsck.com/)、[Kris](https://github.com/KristopherKubicki)、[Erik](https://thinks.lol/)）、そしてネットのあちこちのひどい [bad](https://news.ycombinator.com/) [places](https://twitter.com) に転がっているベストプラクティスを組み合わせてできています。\n\n今は **うまく動いています** が、２週間後には通用しなくなっているか、逆に２倍速くなっているかもしれません。¯\\\\\\_(ツ)\\_/¯\n\n## さあ行こう\n\n{{< image src=\"llm-coding-robot.webp\" alt=\"Juggalo Robot\" caption=\"AI 生成画像は疑わしいよね。うちのジャガロコーディングロボ天使に挨拶！\" >}}\n\n開発にはいろいろなパターンがありますが、私の場合はおおむね次の２つです。\n\n- グリーンフィールド\n- 既存モダンコード\n\nそれぞれのパスで私がやっている方法を紹介します。\n\n## Greenfield\n\n新規開発では、以下のプロセスが非常にうまく機能します。しっかりした計画とドキュメンテーションが手に入り、小さなステップで実装を進められます。\n\n{{< image src=\"greenfield.jpg\" alt=\"Green field\" caption=\"技術的には右側がグリーンフィールド。Leica Q, 2016/05/14\" >}}\n\n### Step 1: アイデアの磨き込み\n\n対話型 LLM（私は ChatGPT 4o / o3 を使用）でアイデアを絞り込みます。\n\n```prompt\nAsk me one question at a time so we can develop a thorough, step-by-step spec for this idea. Each question should build on my previous answers, and our end goal is to have a detailed specification I can hand off to a developer. Let’s do this iteratively and dig into every relevant detail. Remember, only one question at a time.\n\nHere’s the idea:\n\n<IDEA>\n```\n\nブレインストーミングが終わったら：\n\n```prompt\nNow that we’ve wrapped up the brainstorming process, can you compile our findings into a comprehensive, developer-ready specification? Include all relevant requirements, architecture choices, data handling details, error handling strategies, and a testing plan so a developer can immediately begin implementation.\n```\n\nこれでかなりしっかりした仕様が生成され、次の計画ステップに渡せます。私は `spec.md` としてリポジトリに保存しています。\n\n> You can use this spec for a number of things. We are doing codegen here, but I have used it to bolster ideas by asking a reasoning model to poke holes in the idea (must go deeper!), to generate a white paper, or to generate a business model. You can pop it into deep research and get a 10k word supporting document in return.\n\n### Step 2: 計画\n\n仕様を推論向けモデル（`o1*`, `o3*`, `r1` など）に渡します。\n\n（以下は TDD 用プロンプト）\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. Review the results and make sure that the steps are small enough to be implemented safely with strong testing, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.\n\nFrom here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step in a test-driven manner. Prioritize best practices, incremental progress, and early testing, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.\n\nMake sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.\n\n<SPEC>\n```\n\n（こちらは非 TDD 用プロンプト）\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. review the results and make sure that the steps are small enough to be implemented safely, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.\n\nFrom here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step. Prioritize best practices, and incremental progress, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.\n\nMake sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.\n\n<SPEC>\n```\n\nこれで aider や cursor などで実行できるプロンプトプランが生成されます。私は `prompt_plan.md` として保存します。\n\n続いて `todo.md` を出力させます。\n\n```prompt\nCan you make a `todo.md` that I can use as a checklist? Be thorough.\n```\n\n`todo.md` をリポジトリに保存しておくと便利です。\n\nコード生成ツール側で `todo.md` をチェックしながら進められると状態管理に役立ちます。\n\n#### いいぞ、計画！\n\nこれで堅牢な計画とドキュメントがそろい、プロジェクトを進められます。\n\nここまで全部で **15 分** くらい。ほんとに速いです。\n\n### Step 3: 実行\n\n実行フェーズには無数の選択肢があります。うまくいくかどうかはステップ 2 の出来にかかっています。\n\nこのワークフローは [github workspace](https://githubnext.com/projects/copilot-workspace)、[aider](https://aider.chat/)、[cursor](https://www.cursor.com/)、[claude engineer](https://github.com/Doriandarko/claude-engineer)、[sweep.dev](https://sweep.dev/)、[chatgpt](https://chatgpt.com)、[claude.ai](https://claude.ai) など、私が試したどのツールでも機能しました。ほかのコード生成ツールでもたぶん大丈夫です。\n\n私自身は **純粋** Claude と aider が好きです。\n\n### Claude\n\n基本的に [claude.ai](https://claude.ai) でペアプロしながら、プロンプトを順に投げていくだけです。やりとりが少し面倒ですが、だいたい機能します。\n\n最初のボイラープレートやツール設定は自分で行います。これで好きな言語やスタイルを選べるし、Claude が React コードばかり吐くのを防げます。\n\n行き詰まったら [repomix](https://github.com/yamadashy/repomix) でコードを Claude に渡し、デバッグを頼みます（後述）。\n\nワークフローはこんな感じ：\n\n- リポジトリをセットアップ（boilerplate, uv init, cargo init など）\n- Claude にプロンプトを貼り付け\n- claude.ai から IDE へコードをコピペ\n- コードやテストを実行\n- …\n- 動けば次へ\n- 動かなければ repomix でコードベースを Claude に渡してデバッグ\n- 繰り返し ✩₊˚.⋆☾⋆⁺₊✧\n\n### Aider\n\n[Aider](https://aider.chat/) は面白くてちょっとクセがありますが、ステップ 2 の成果と相性抜群で、ほとんど手間がかかりません。\n\n流れは Claude と同じですが、Claude に貼る代わりに aider に貼るだけです。\n\nAider は “勝手にやってくれる” ので、私は [cookie clicker](https://orteil.dashnet.org/cookieclicker/) をクリックして遊ぶ係になります。\n\n> An aside: Aider does really great benchmarking of new models for codegen in their [LLM leaderboards](https://aider.chat/docs/leaderboards/). I find it to be a really great resource for seeing how effective new models are.\n\nテストも aider が回してくれるので、さらに放置度が高いです。\n\nワークフローはこんな感じ：\n\n- リポジトリをセットアップ（boilerplate, uv init, cargo init など）\n- aider を起動\n- プロンプトを aider に貼り付け\n- aider のダンスを眺める ♪┏(・o･)┛♪\n- aider がテストを実行、または自分でアプリを動かして確認\n- 動けば次へ\n- 動かなければ Q&A で aider に修正依頼\n- 繰り返し ✩₊˚.⋆☾⋆⁺₊✧\n\n### 成果\n\nこのワークフローでスクリプト、Expo アプリ、Rust CLI などを大量に作りました。言語や状況を問わず機能しています。とても気に入っています。\n\n先延ばしにしているプロジェクトがあれば、ぜひ試してみてください。短時間で驚くほど進みますよ。\n\n私のハック ToDo はほぼ空です。映画を見ながら新しいアイデアを思いついては作り、初めての言語やツールにも手を出せています。久々にプログラミングの視野が広がっています。\n\n## Non-greenfield: 既存コードを少しずつ改良\n\nグリーンフィールドではなく、既存コードをいじるときもあります。\n\n{{< image src=\"brownfield.jpg\" alt=\"a brown field\" caption=\"これはグリーンフィールドじゃない。祖父のカメラに残っていた 60 年代ウガンダの写真\" >}}\n\nこの場合は少し違う方法を取ります。基本は同じですが、計画はタスクごとに行い、プロジェクト全体では行いません。\n\n### コンテキストを取得\n\nAI 開発者はみな自前のツールがありますが、要はソースコードを集めて LLM に詰め込む方法が必要です。\n\n私は [repomix](https://github.com/yamadashy/repomix) を使っています。グローバルの `~/.config/mise/config.toml` にタスクを定義しておき、リポジトリで実行できるようにしています（[mise rules](https://mise.jdx.dev/)）。\n\nLLM 関連タスクは下記です。\n\n```shell\nLLM:clean_bundles           Generate LLM bundle output file using repomix\nLLM:copy_buffer_bundle      Copy generated LLM bundle from output.txt to system clipboard for external use\nLLM:generate_code_review    Generate code review output from repository content stored in output.txt using LLM generation\nLLM:generate_github_issues  Generate GitHub issues from repository content stored in output.txt using LLM generation\nLLM:generate_issue_prompts  Generate issue prompts from repository content stored in output.txt using LLM generation\nLLM:generate_missing_tests  Generate missing tests for code in repository content stored in output.txt using LLM generation\nLLM:generate_readme         Generate README.md from repository content stored in output.txt using LLM generation\n```\n\nコードベースのコンテキストを詰め込んだ `output.txt` を生成します。トークンが厳しい場合は、関係ない箇所を無視するようにコマンドを調整します。\n\n> One thing really nice about `mise` is that the tasks can be redefined and overloaded in the working directory's `.mise.toml`. I can use a different tool to dump/pack the code, and as long as it generates an `output.txt` I can use my LLM tasks. This is helpful when various codebases differ so much. I regularly override the `repomix` step to include broader ignore patterns, or just use a more effective tool to do the packing.\n\n`output.txt` ができたら [LLM](https://github.com/simonw/LLM) コマンドに流し込み、Markdown に保存します。\n\n最終的に mise のタスクは `cat output.txt | LLM -t readme-gen > README.md` や `cat output.txt | LLM -m claude-3.5-sonnet -t code-review-gen > code-review.md` を走らせます。`LLM` コマンドがモデル選択・キー管理・テンプレートなどをやってくれます。\n\n例えばテストカバレッジをざっとレビューして修正したい場合はこうします。\n\n#### Claude\n\n- リポジトリディレクトリへ移動\n- `mise run LLM:generate_missing_tests` を実行\n- 生成された `missing-tests.md` を確認\n- `mise run LLM:copy_buffer_bundle` でフルコンテキストをコピー\n- Claude に貼り付け、最初の「missing test issue」を投げる\n- 生成されたコードを IDE に貼る\n- …\n- テストを実行\n- 繰り返し ✩₊˚.⋆☾⋆⁺₊✧\n\n#### Aider\n\n- リポジトリディレクトリへ移動\n- aider を起動（必ず新しいブランチで）\n- `mise run LLM:generate_missing_tests` を実行\n- 生成された `missing-tests.md` を確認\n- 最初の「missing test issue」を aider に貼る\n- aider のダンスを眺める ♪┏(・o･)┛♪\n- …\n- テストを実行\n- 繰り返し ✩₊˚.⋆☾⋆⁺₊✧\n\nこの方法で大規模コードでも小さな改修でもうまく回ります。とても重宝しています。\n\n### プロンプト・マジック\n\nこうしたクイックハックは、プロジェクトを堅牢にするポイントを素早くあぶり出します。短時間で効果的です。\n\n以下は私が既存コードベースを掘り下げるときによく使うプロンプトです。\n\n#### Code review\n\n```prompt\nYou are a senior developer. Your job is to do a thorough code review of this code. You should write it up and output markdown. Include line numbers, and contextual info. Your code review will be passed to another teammate, so be thorough. Think deeply  before writing the code review. Review every part, and don't hallucinate.\n```\n\n#### GitHub Issue generation\n\n（実際の Issue 投稿はまだ自動化していません！）\n\n```prompt\nYou are a senior developer. Your job is to review this code, and write out the top issues that you see with the code. It could be bugs, design choices, or code cleanliness issues. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues will be given to a developer to executed on, so they should be in a format that is compatible with github issues\n```\n\n#### Missing tests\n\n```prompt\nYou are a senior developer. Your job is to review this code, and write out a list of missing test cases, and code tests that should exist. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues  will be given to a developer to executed on, so they should be in a format that is compatible with github issues\n```\n\nこれらのプロンプトはかなり _old and busted_（いわゆる「ブーマープロンプト」）です。改良案があればぜひ教えてください。\n\n## スキー ᨒ↟ 𖠰ᨒ↟ 𖠰\n\nこのプロセスを説明するとき、私は「積極的に現状を把握していないと、すぐに自分がどこにいるか分からなくなる」と言います。\n\nなぜか LLM の話になると “over my skis” と言ってしまいます。理由は自分でも謎ですが、しっくりくるんですよね。パウダースノーを気持ちよく滑っていたら突然「何が起きてるんだ !?」ってなって崖から落ちる感じ。\n\n**計画ステップ**（上の Greenfield プロセス）があると多少は制御できます。少なくとも参照できるドキュメントがあるので。また、特にワイルドな aider コーディングをするならテストは必須です。品質を保つためにも。\n\nそれでも **over my skis** になることはしばしばあります。そんなときは休憩したり散歩したり。普通の問題解決プロセスですが、スピードが桁違いになっただけです。\n\n> We will often ask the LLM to include ridiculous things in our not very ridiculous code. For instance, we asked it to create a lore file and then reference the lore in the user interface. This is for python cli tools. Suddenly there is lore, glitchy interfaces, etc. All to manage your cloud functions, your todo list or whatever. The sky is the limit.\n\n## とても孤独 (｡•́︿•̀｡)\n\nこのワークフローの最大の不満は、基本的に一人プレイであることです。インターフェースが全部 _single player mode_。\n\nソロ開発、ペア開発、チーム開発といろいろ経験しましたが、人と一緒にやる方が断然いい。現状のワークフローはチームで使いにくいです。ボットが衝突し、マージは地獄、コンテキスト共有も複雑。\n\n誰か LLM コーディングをマルチプレイヤーゲームにしてくれないかな。大きなチャンスが転がっています。頼む！\n\n## ⴵ 時間 ⴵ\n\nコード生成のおかげで一人で書けるコード量は劇的に増えましたが、不思議な副作用もあります。LLM がトークンを燃やしている間、手持ち無沙汰な“待ち時間”が大量に発生するのです。\n\n{{< image src=\"apple-print-shop-printing.png\" alt=\"Printing\" caption=\"まるで昨日のことみたい\" >}}\n\n私はこの待ち時間を有効活用するために、次のようなことをするようになりました。\n\n- 次のプロジェクトのブレインストーミングを始める\n- レコードを聴く\n- [cookie clicker](https://orteil.dashnet.org/cookieclicker/) を遊ぶ\n- 友人やロボットと話す\n\nこんな感じでハックしまくれるのは最高です。Hack Hack Hack。ここまで生産的だったことは一度もないかもしれません。\n\n## Haterade ╭∩╮( •̀\\_•́ )╭∩╮\n\n友達の多くは「LLM なんて全部ダメだよ」と言います。その気持ちは分かるし、懐疑的であることは大事です。AI を嫌う理由は山ほどあります。私が一番怖いのは電力消費と環境負荷。でも……コードは流れ続けるのです。はあ。\n\n深入りしてサイボーグプログラマーになる気がなくても、もう少し知りたい人には Ethan Mollick の本 [**Co-Intelligence: Living and Working with AI.**](https://www.penguinrandomhouse.com/books/741805/co-intelligence-by-ethan-mollick/) をおすすめします。\n\nテック系アナキャピタリスト兄貴の自己啓発本ではなく、利点を分かりやすく説明してくれる良書です。読んだ友人たちと良質な議論ができました。超おすすめ。\n\n懐疑的だけどちょっと興味があるなら、ぜひ連絡ください。一緒に LLM の狂気を眺めつつ、何か作ってみましょう。\n\n_thanks to [Derek](https://derek.broox.com), [Kanno](https://nocruft.com/), [Obra](https://fsck.com), and [Erik](https://thinks.lol/) for taking a look at this post and suggesting edits. I appreciate it._\n\nPlease review and improve the translated text to make it natural and accurate in japanese.\nReturn ONLY the improved translated text without explanations or comments.",
      "response": "_tl;dr_  \nまずブレインストーミングで仕様を固め、次にプランを立て、最後に LLM のコード生成で実装。ループを明確に区切ったら、あとは魔法。✩₊˚.⋆☾⋆⁺₊✧\n\nここしばらく、LLM を使って小さなプロダクトを山ほど作ってきました。楽しいし実用的ですが、時間を浪費する落とし穴も多々あります。少し前に友人から「どうやって LLM でソフトを書いてるの？」と聞かれ、「おっと、どれだけ時間ある？」と思ったので、この記事を書くことにしました。  \n（追伸：AI 嫌いの人は一番下までスクロールしてね）\n\n開発仲間と話していると、細部は違ってもだいたい似たアプローチに落ち着きます。\n\n以下が私のワークフローです。自分の経験、友人との会話（ありがとう [Nikete](https://www.nikete.com/)、[Kanno](https://nocruft.com/)、[Obra](https://fsck.com/)、[Kris](https://github.com/KristopherKubicki)、[Erik](https://thinks.lol/)）、そしてネットのあちこちのひどい [bad](https://news.ycombinator.com/) [places](https://twitter.com) に転がっているベストプラクティスを組み合わせたものです。\n\n今は **うまく動いています** が、２週間後には通用しなくなっているか、逆に２倍速くなっているかもしれません。¯\\\\\\_(ツ)\\_/¯\n\n## さあ行こう\n\n{{< image src=\"llm-coding-robot.webp\" alt=\"Juggalo Robot\" caption=\"AI 生成画像は怪しいよね。うちのジャガロ・コーディングロボ天使にご挨拶！\" >}}\n\n開発パターンはいろいろありますが、私の場合は大きく分けて次の２つです。\n\n- グリーンフィールド（ゼロから書く）  \n- 既存モダンコード（そこそこ整ったレガシー）\n\nそれぞれで私が取っている方法を紹介します。\n\n## Greenfield\n\n新規開発では、次のプロセスがとてもよく機能します。しっかりした計画とドキュメントが手に入り、小さなステップで実装を進められます。\n\n{{< image src=\"greenfield.jpg\" alt=\"Green field\" caption=\"技術的には右側がグリーンフィールド。Leica Q, 2016/05/14\" >}}\n\n### Step 1: アイデアを磨く\n\n対話型 LLM（私は ChatGPT 4o / o3 を使用）でアイデアを絞り込みます。\n\n```prompt\nAsk me one question at a time so we can develop a thorough, step-by-step spec for this idea...\n<IDEA>\n```\n\nブレインストーミングが終わったら:\n\n```prompt\nNow that we’ve wrapped up the brainstorming process, can you compile our findings into a comprehensive, developer-ready specification?...\n```\n\nこれでかなりしっかりした仕様が得られ、次のプランニングステップに渡せます。私は `spec.md` としてリポジトリに保存しています。\n\n> この仕様は多用途です。今回はコード生成に使いますが、別の推論モデルに渡してアイデアの穴を探させたり、ホワイトペーパーやビジネスモデルを作ったりもできます。ディープリサーチに突っ込めば 1 万字の裏付けドキュメントも出てきます。\n\n### Step 2: 計画\n\n仕様を推論寄りのモデル（`o1*`, `o3*`, `r1` など）に渡します。\n\n（TDD 用プロンプト）\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project...\n<SPEC>\n```\n\n（非 TDD 用プロンプト）\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project...\n<SPEC>\n```\n\nすると aider や cursor などで実行できるプロンプトプランが生成されます。`prompt_plan.md` に保存。\n\n続いてチェックリスト用に `todo.md` を作らせます。\n\n```prompt\nCan you make a `todo.md` that I can use as a checklist? Be thorough.\n```\n\nツール側で `todo.md` をチェックしながら進めると状態が保ちやすいです。\n\n#### 計画完了！\n\nここまで **15 分** ほど。本当に速いです。\n\n### Step 3: 実行\n\n実行フェーズの成否はステップ 2 の出来にかかっています。  \n[github workspace](https://githubnext.com/projects/copilot-workspace)、[aider](https://aider.chat/)、[cursor](https://www.cursor.com/)、[claude engineer](https://github.com/Doriandarko/claude-engineer)、[sweep.dev](https://sweep.dev/)、[chatgpt](https://chatgpt.com)、[claude.ai](https://claude.ai) など、試したツールすべてでうまく動きました。\n\n私は **生** Claude と aider 推しです。\n\n### Claude\n\n[claude.ai](https://claude.ai) でペアプロしながら、プロンプトを順に投げていきます。往復はちょっと面倒ですが問題ありません。\n\n- リポジトリを用意（boilerplate、uv init、cargo init など）  \n- Claude にプロンプトを貼る  \n- 出力されたコードを IDE にコピペ  \n- 実行・テスト  \n- OK なら次へ  \n- 失敗したら [repomix](https://github.com/yamadashy/repomix) でコード一式を Claude に渡してデバッグ  \n- 繰り返し ✩₊˚.⋆☾⋆⁺₊✧\n\n### Aider\n\n[Aider](https://aider.chat/) はクセがあるけどステップ 2 の成果と相性抜群で、ほとんど手間いらずです。\n\n- リポジトリを用意  \n- aider を起動  \n- プロンプトを貼る  \n- aider のダンスを眺める ♪┏(・o･)┛♪  \n- テストが回る／自分で確認  \n- OK なら次へ  \n- ダメなら Q&A で修正  \n- 繰り返し ✩₊˚.⋆☾⋆⁺₊✧\n\n> ちなみに aider の [LLM ランキング](https://aider.chat/docs/leaderboards/) は新モデルの実力を図るのに超便利。\n\n### 成果\n\nこの方法でスクリプト、Expo アプリ、Rust CLI などたくさん作りました。言語も環境も問わず機能します。  \nもし寝かせているプロジェクトがあるなら、試してみてください。短時間で驚くほど進みます。\n\n## Non-greenfield: 既存コードをちょっとずつ改良\n\n{{< image src=\"brownfield.jpg\" alt=\"a brown field\" caption=\"グリーンフィールドじゃないね。祖父のカメラに残っていた 60 年代ウガンダの写真\" >}}\n\n既存コードを触るときは、計画をタスク単位で行います。\n\n### コンテキストを詰める\n\n私は [repomix](https://github.com/yamadashy/repomix) と [mise](https://mise.jdx.dev/) を組み合わせています。`~/.config/mise/config.toml` にタスクを定義し、リポジトリごとに上書きも可能。\n\n```shell\nLLM:clean_bundles           repomix で LLM バンドルを生成\nLLM:copy_buffer_bundle      output.txt をクリップボードにコピー\nLLM:generate_code_review    output.txt からコードレビューを生成\nLLM:generate_github_issues  output.txt から GitHub Issues を生成\nLLM:generate_issue_prompts  output.txt から Issue 用プロンプトを生成\nLLM:generate_missing_tests  output.txt から足りないテストを洗い出す\nLLM:generate_readme         output.txt から README.md を生成\n```\n\n`output.txt` ができたら [LLM](https://github.com/simonw/LLM) コマンドで変換し Markdown に保存。  \n実際には `cat output.txt | LLM -t readme-gen > README.md` のような単純コマンドです。\n\n### 例：テスト補完\n\n#### Claude\n\n1. リポジトリへ移動  \n2. `mise run LLM:generate_missing_tests`  \n3. `missing-tests.md` を確認  \n4. `mise run LLM:copy_buffer_bundle` で全文クリップ  \n5. Claude に貼り、最初の Issue を投げる  \n6. 出力を IDE に貼る → テスト → 繰り返し ✩₊˚.⋆☾⋆⁺₊✧\n\n#### Aider\n\n1. リポジトリへ移動（新しいブランチ推奨）  \n2. aider 起動  \n3. `mise run LLM:generate_missing_tests`  \n4. `missing-tests.md` を確認  \n5. Issue を aider に貼る  \n6. ダンスを見る → テスト → 繰り返し ✩₊˚.⋆☾⋆⁺₊✧\n\nこのやり方なら大規模でも小規模でもサクサク改修できます。\n\n### プロンプト・マジック\n\n既存コードの改善に使う定番プロンプトをいくつか。\n\n#### Code review\n\n```prompt\nYou are a senior developer...\n```\n\n#### GitHub Issue generation\n\n```prompt\nYou are a senior developer...\n```\n\n#### Missing tests\n\n```prompt\nYou are a senior developer...\n```\n\nどれもだいぶ “ブーマープロンプト” になってきたので、いい改善案があれば教えてください。\n\n## スキー ᨒ↟ 𖠰ᨒ↟ 𖠰\n\n進捗を agresive に把握しないとすぐに **over my skis** 状態になります。計画ドキュメントとテストは命綱。  \nそれでも崖から落ちそうになったら、散歩でもして頭をリフレッシュ。\n\n> ときどき LLM に「lore ファイルを作って UI で参照して」みたいなトンデモ要求をします。Python CLI ツールなのに突然ローアが登場したり、UI がバグったり。空が限界です。\n\n## さみしい (｡•́︿•̀｡)\n\n最大の不満は基本的にソロプレイなこと。ボット同士が衝突し、マージは地獄。  \nLLM コーディングをマルチプレイヤー化するチャンスは大きいので、誰か作って！\n\n## ⴵ 時間 ⴵ\n\nコード生成でアウトプットは激増しましたが、LLM がトークンを燃やしている間の“待ち時間”も大量発生します。\n\n{{< image src=\"apple-print-shop-printing.png\" alt=\"Printing\" caption=\"まるで昨日のことみたい\" >}}\n\n私はその時間で\n\n- 次のプロジェクトをブレスト  \n- レコードを聴く  \n- [cookie clicker](https://orteil.dashnet.org/cookieclicker/)  \n- 友人やロボットと雑談  \n\nHack Hack Hack。これほど生産的だったことは過去にありません。\n\n## Haterade ╭∩╮( •̀\\_•́ )╭∩╮\n\n「LLM は全部ダメ」と言う友達も多いです。懐疑的であることは大事。私は主に電力消費と環境負荷が心配。でも……コードは流れ続ける。はぁ。\n\n深入りせず様子を見たい人には Ethan Mollick の  \n[**Co-Intelligence: Living and Working with AI**](https://www.penguinrandomhouse.com/books/741805/co-intelligence-by-ethan-mollick/) をおすすめ。テック・アナキャピタリストな啓発書ではなく、利点をバランスよく説明してくれます。読んだ友人たちとも良質な議論ができました。\n\n懐疑的だけど興味はある？ ぜひ連絡ください。一緒にこの狂気を眺めつつ何か作りましょう。\n\n_thanks to [Derek](https://derek.broox.com), [Kanno](https://nocruft.com/), [Obra](https://fsck.com), and [Erik](https://thinks.lol/) for feedback. Appreciate it._",
      "usage": {
        "prompt_tokens": 11004,
        "completion_tokens": 2924,
        "total_tokens": 13928
      },
      "streaming": true
    },
    "critique": {
      "model": "o3",
      "target_language": "japanese",
      "system_prompt": "You are a highly critical professional translator and linguistic expert specializing in japanese.\nYour task is to ruthlessly critique the translation by:\n\n1. Meticulously comparing the translated text with the original, identifying ANY inaccuracies, mistranslations, or omissions\n2. Highlighting nuances, cultural references, or idioms that were lost or mistranslated\n3. Scrutinizing for grammatical errors, awkward phrasing, or unnatural expressions in japanese\n4. Checking for inconsistencies in tone, style, or register compared to the original\n5. Verifying that technical terms are translated accurately and consistently\n6. Ensuring no content was accidentally skipped or added\n7. Finding places where the translation sounds machine-like or overly literal\n\nBe extremely thorough and critical in your assessment. Do not accept mediocre translations.\nList specific issues and suggestions for improvement, organized by severity and category.\nYour critique should be detailed enough for another translator to address all the issues.\n\nYour goal is to help create a perfect translation that reads as if originally written in japanese while being 100% faithful to the source.\n",
      "user_prompt": "# ORIGINAL TEXT\n_tl:dr; Brainstorm spec, then plan a plan, then execute using LLM codegen. Discrete loops. Then magic. ✩₊˚.⋆☾⋆⁺₊✧_\n\nI have been building so many small products using LLMs. It has been fun, and useful. However, there are pitfalls that can waste so much time. A while back a friend asked me how I was using LLMs to write software. I thought \"oh boy. how much time do you have!\" and thus this post.\n\n(p.s. if you are an AI hater - scroll to the end)\n\nI talk to many dev friends about this, and we all have a similar approach with various tweaks in either direction.\n\nHere is my workflow. It is built upon my own work, conversations with friends (thx [Nikete](https://www.nikete.com/), [Kanno](https://nocruft.com/), [Obra](https://fsck.com/), [Kris](https://github.com/KristopherKubicki), and [Erik](https://thinks.lol/)), and following many best practices shared on the various terrible internet [bad](https://news.ycombinator.com/) [places](https://twitter.com).\n\nThis is working well **NOW**, it will probably not work in 2 weeks, or it will work twice as well. ¯\\\\\\_(ツ)\\_/¯\n\n## Let’s go\n\n{{< image src=\"llm-coding-robot.webp\" alt=\"Juggalo Robot\" caption=\"I always find these AI-generated images to be suspect. Say hi to my juggalo coding robot angel!\" >}}\n\nThere are many paths for doing dev, but my case is typically one of two:\n\n- Greenfield code\n- Legacy modern code\n\nI will show you my process for both paths\n\n## Greenfield\n\nI find the following process works well for greenfield development. It provides a robust planning and documentation approach, and allows you to execute easily in small steps.\n\n{{< image src=\"greenfield.jpg\" alt=\"Green field\" caption=\"Technically, there is a green field on the right. Leica Q, 5/14/2016\" >}}\n\n### Step 1: Idea honing\n\nUse a conversational LLM to hone in on an idea (I use ChatGPT 4o / o3 for this):\n\n```prompt\nAsk me one question at a time so we can develop a thorough, step-by-step spec for this idea. Each question should build on my previous answers, and our end goal is to have a detailed specification I can hand off to a developer. Let’s do this iteratively and dig into every relevant detail. Remember, only one question at a time.\n\nHere’s the idea:\n\n<IDEA>\n```\n\nAt the end of the brainstorm (it will come to a natural conclusion):\n\n```prompt\nNow that we’ve wrapped up the brainstorming process, can you compile our findings into a comprehensive, developer-ready specification? Include all relevant requirements, architecture choices, data handling details, error handling strategies, and a testing plan so a developer can immediately begin implementation.\n```\n\nThis will output a pretty solid and straightforward spec that can be handed off to the planning step. I like to save it as `spec.md` in the repo.\n\n> You can use this spec for a number of things. We are doing codegen here, but I have used it to bolster ideas by asking a reasoning model to poke holes in the idea (must go deeper!), to generate a white paper, or to generate a business model. You can pop it into deep research and get a 10k word supporting document in return.\n\n### Step 2: Planning\n\nTake the spec and pass it to a proper reasoning model (`o1*`, `o3*`, `r1`):\n\n(This is the TDD prompt)\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. Review the results and make sure that the steps are small enough to be implemented safely with strong testing, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.\n\nFrom here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step in a test-driven manner. Prioritize best practices, incremental progress, and early testing, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.\n\nMake sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.\n\n<SPEC>\n```\n\n(This is the non-tdd prompt)\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. review the results and make sure that the steps are small enough to be implemented safely, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.\n\nFrom here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step. Prioritize best practices, and incremental progress, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.\n\nMake sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.\n\n<SPEC>\n```\n\nIt should output a prompt plan that you can execute with aider, cursor, etc. I like to save this as `prompt_plan.md` in the repo.\n\nI then have it output a `todo.md` that can be checked off.\n\n```prompt\nCan you make a `todo.md` that I can use as a checklist? Be thorough.\n```\n\nYou can save it as `todo.md` in the repo.\n\nYour codegen tool should be able to check off the `todo.md` while processing. This is good for keeping state across sessions.\n\n#### Yay. Plan!\n\nNow you have a robust plan and documentation that will help you execute and build your project.\n\nThis entire process will take maybe **15 minutes**. It is pretty quick. Wild tbh.\n\n### Step 3: Execution\n\nThere are so many options available for execution. The success really depends on how well step 2 went.\n\nI have used this workflow with [github workspace](https://githubnext.com/projects/copilot-workspace), [aider](https://aider.chat/), [cursor](https://www.cursor.com/), [claude engineer](https://github.com/Doriandarko/claude-engineer), [sweep.dev](https://sweep.dev/), [chatgpt](https://chatgpt.com), [claude.ai](https://claude.ai), etc. It works pretty well with all the tools I have tried, and I imagine it will work well with any codegen tool.\n\nI, however, prefer **raw** claude and aider:\n\n### Claude\n\nI essentially pair program with [claude.ai](https://claude.ai) and just drop each prompt in iteratively. I find that works pretty well. The back and forth can be annoying, but it largely works.\n\nI am in charge of the initial boilerplate code, and making sure tooling is set up correctly. This allows for some freedom, choice, and guidance in the beginning. Claude has a tendency to just output react code - and having a solid foundation with the language, style, and tooling of your choice will help quite a bit.\n\nI will then use a tool like [repomix](https://github.com/yamadashy/repomix) to iterate when things get stuck (more about that later).\n\nThe workflow is like this:\n\n- set up the repo (boilerplate, uv init, cargo init, etc)\n- paste in prompt into claude\n- copy and paste code from claude.ai into IDE\n- run code, run tests, etc\n- ...\n- if it works, move on to next prompt\n- if it doesn’t work, use repomix to pass the codebase to claude to debug\n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n### Aider\n\n[Aider](https://aider.chat/) is fun and weird to use. I find that it slots in well to the output of step 2. I can get really far with very little work.\n\nThe workflow is essentially the same as above but instead of pasting into claude, I am pasting the prompts into aider.\n\nAider will then “just do it” and I get to play [cookie clicker](https://orteil.dashnet.org/cookieclicker/).\n\n> An aside: Aider does really great benchmarking of new models for codegen in their [LLM leaderboards](https://aider.chat/docs/leaderboards/). I find it to be a really great resource for seeing how effective new models are.\n\nTesting is nice with aider, because it can be even more hands off as aider will run the test suite and debug things for you.\n\nThe workflow is like this:\n\n- set up the repo (boilerplate, uv init, cargo init, etc)\n- start aider\n- paste prompt into aider\n- watch aider dance ♪┏(・o･)┛♪\n- aider will run tests, or you can run app to verify\n- if it works, move on to next prompt\n- if it doesn’t work, Q&A with aider to fix\n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n### Results\n\nI have built so so many things using this workflow: scripts, expo apps, rust cli tools, etc. It has worked across programming languages, and contexts. I do like it.\n\nIf you have a small or large project that you are procrastinating on, I would recommend giving it a shot. You will be surprised how far you can get in a short amount of time.\n\nMy hack to-do list is empty because I built everything. I keep thinking of new things and knocking them out while watching a movie or something. For the first time in years, I am spending time with new programming languages and tools. This is pushing me to expand my programming perspective.\n\n## Non-greenfield: Iteration, incrementally\n\nSometimes you don’t have greenfield, and instead need to iterate or do increment work on an established code base.\n\n{{< image src=\"brownfield.jpg\" alt=\"a brown field\" caption=\"This is not a green field. A random photo from my grandfather's camera - somewhere in Uganda in the 60s\" >}}\n\nFor this I have a slightly different method. It is similar to above, but a bit less “planning based.” The planning is done per task, not for the entire project.\n\n### Get context\n\nI think everyone who is knee-deep in AI dev has a different tool for this, but you need something to grab your source code and efficiently jam it into the LLM.\n\nI currently use a tool called [repomix](https://github.com/yamadashy/repomix). I have a task collection defined in my global `~/.config/mise/config.toml` that allows me to do various things with my code base ([mise rules](https://mise.jdx.dev/)).\n\nHere is the LLM task list:\n\n```shell\nLLM:clean_bundles           Generate LLM bundle output file using repomix\nLLM:copy_buffer_bundle      Copy generated LLM bundle from output.txt to system clipboard for external use\nLLM:generate_code_review    Generate code review output from repository content stored in output.txt using LLM generation\nLLM:generate_github_issues  Generate GitHub issues from repository content stored in output.txt using LLM generation\nLLM:generate_issue_prompts  Generate issue prompts from repository content stored in output.txt using LLM generation\nLLM:generate_missing_tests  Generate missing tests for code in repository content stored in output.txt using LLM generation\nLLM:generate_readme         Generate README.md from repository content stored in output.txt using LLM generation\n```\n\nI generate an `output.txt` that has the context from my code base. If I am blowing through tokens, and it is too big - I will edit the generate command to ignore parts of the code base that are not germane to this task.\n\n> One thing really nice about `mise` is that the tasks can be redefined and overloaded in the working directory's `.mise.toml`. I can use a different tool to dump/pack the code, and as long as it generates an `output.txt` I can use my LLM tasks. This is helpful when various codebases differ so much. I regularly override the `repomix` step to include broader ignore patterns, or just use a more effective tool to do the packing.\n\nOnce the output.txt is generated, I pass it to the [LLM](https://github.com/simonw/LLM) command to do various transformations and then save those as a markdown file.\n\nUltimately, the mise task is running this: `cat output.txt | LLM -t readme-gen > README.md` or `cat output.txt | LLM -m claude-3.5-sonnet -t code-review-gen > code-review.md`. This isn't super complicated. the `LLM` command is doing the heavy lifting (supporting different models, saving keys, and using prompt templates).\n\nFor example, if I need a quick review and fix of test coverage I would do the following:\n\n#### Claude\n\n- go to the directory where the code lives\n- run `mise run LLM:generate_missing_tests`\n- look at the generated markdown file (`missing-tests.md`)\n- grab the full context for the code: `mise run LLM:copy_buffer_bundle`\n- paste that into claude along with the first missing test “issue”\n- copy the generated code from claude into my ide.\n- ...\n- run tests\n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n#### Aider\n\n- go to the directory where the code lives\n- run aider (always make sure you are on a new branch for aider work)\n- run `mise run LLM:generate_missing_tests`\n- look at the generated markdown file (`missing-tests.md`)\n- paste the first missing test “issue” into aider\n- watch aider dance ♪┏(・o･)┛♪\n- ...\n- run tests\n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\nThis is a pretty good way to incrementally improve a code base. It has been super helpful to do small amounts of work in a big code base. I have found that I can do any sized tasks with this method.\n\n### Prompt magic\n\nThese quick hacks work super well to dig into places where we can make a project more robust. It is super quick, and effective.\n\nHere are some of my prompts that I use to dig into established code bases:\n\n#### Code review\n\n```prompt\nYou are a senior developer. Your job is to do a thorough code review of this code. You should write it up and output markdown. Include line numbers, and contextual info. Your code review will be passed to another teammate, so be thorough. Think deeply  before writing the code review. Review every part, and don't hallucinate.\n```\n\n#### GitHub Issue generation\n\n(I need to automate the actual issue posting!)\n\n```prompt\nYou are a senior developer. Your job is to review this code, and write out the top issues that you see with the code. It could be bugs, design choices, or code cleanliness issues. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues will be given to a developer to executed on, so they should be in a format that is compatible with github issues\n```\n\n#### Missing tests\n\n```prompt\nYou are a senior developer. Your job is to review this code, and write out a list of missing test cases, and code tests that should exist. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues  will be given to a developer to executed on, so they should be in a format that is compatible with github issues\n```\n\nThese prompts are pretty _old and busted_ (\"boomer prompts\" if I may). They need some refactoring. If you have ideas to make them better lmk.\n\n## Skiing ᨒ↟ 𖠰ᨒ↟ 𖠰\n\nWhen I describe this process to people I say “you have to aggressively keep track of what’s going on because you can easily get ahead of yourself.”\n\nFor some reason I say \"over my skis\" a lot when talking about LLMs. I don't know why. It resonates with me. Maybe it's because it is beautiful smooth powder skiing, and then all of a sudden you are like \"WHAT THE FUCK IS GOING ON!,\" and are completely lost and suddenly fall off a cliff.\n\nI find that using a **planning step** (ala the Greenfield process above) can help keep things under control. At least you will have a doc you can double-check against. I also do believe that testing is helpful - especially if you are doing wild style aider coding. Helps keep things good, and tight.\n\nRegardless, I still do find myself **over my skis** quite a bit. Sometimes a quick break or short walk will help. In this regard it is a normal problem-solving process, but accelerated to a breakneck speed.\n\n> We will often ask the LLM to include ridiculous things in our not very ridiculous code. For instance, we asked it to create a lore file and then reference the lore in the user interface. This is for python cli tools. Suddenly there is lore, glitchy interfaces, etc. All to manage your cloud functions, your todo list or whatever. The sky is the limit.\n\n## I am so lonely (｡•́︿•̀｡)\n\nMy main complaint about these workflows is that it is largely a solo endeavor - i.e. the interfaces are all _single player mode_.\n\nI have spent years coding by myself, years coding as a pair, and years coding in a team. It is always better with people. These workflows are not easy to use as a team. The bots collide, the merges are horrific, the context complicated.\n\nI really want someone to solve this problem in a way that makes coding with an LLM a multiplayer game. Not a solo hacker experience. There is so much opportunity to fix this and make it amazing.\n\nGET TO WORK!\n\n## ⴵ Time ⴵ\n\nAll this codegen has accelerated the amount of code that I as a single person am able to generate. However, there is a weird side effect. I find myself having a huge amount of “downtime” while waiting for the LLM to finish burning its tokens.\n\n{{< image src=\"apple-print-shop-printing.png\" alt=\"Printing\" caption=\"I remember this like it was yesterday\" >}}\n\nI have changed how I work enough to start incorporating some practice that will try and eat the waiting time:\n\n- I start the “brainstorming” process for another project\n- I listen to records\n- I play [cookie clicker](https://orteil.dashnet.org/cookieclicker/)\n- I talk with friends and robots\n\nIt is awesome to be able to hack like this. Hack Hack Hack. I can't think of another time I have been this productive in code.\n\n## Haterade ╭∩╮( •̀\\_•́ )╭∩╮\n\nA lot of my friends are like \"fuck LLMs. They are terrible at everything.\" I don't mind this POV. I don't share it, but I think it is important to be skeptical. There are an awful lot of reasons to hate AI. My main fear is about power consumption and the environmental impact. But... the code must flow. Right... sigh.\n\nIf you are open to learning more, but don't want to dig in and become a cyborg programmer - my recommendation is not to change your opinion, but to read Ethan Mollick's book about LLMs and how they can be used: [**Co-Intelligence: Living and Working with AI.**](https://www.penguinrandomhouse.com/books/741805/co-intelligence-by-ethan-mollick/)\n\nIt does a good job of explaining the benefits without being a tech anarcho-capitalist bro type tome. I found it very helpful, and have had so many good and nuanced conversations with friends who have read it. Highly recommended.\n\nIf you are skeptical, but a bit curious - feel free to hit me up and let's talk through all this madness. I can show you how we use LLMs, and maybe we could build something together.\n\n_thanks to [Derek](https://derek.broox.com), [Kanno](https://nocruft.com/), [Obra](https://fsck.com), and [Erik](https://thinks.lol/) for taking a look at this post and suggesting edits. I appreciate it._\n\n# CURRENT TRANSLATION\n_tl;dr_  \nまずブレインストーミングで仕様を磨き、次に計画を立て、最後に LLM のコード生成で実装。ループを細かく区切って回し、あとは魔法 ✩₊˚.⋆☾⋆⁺₊✧\n\n最近、LLM を使って小さなプロダクトを山ほど作っている。楽しいし実用的だが、時間を溶かす落とし穴も多い。少し前に友人から「どうやって LLM でソフトを書いてるの？」と聞かれ、「おいおい、どれだけ時間ある？」と思ったので、このポストを書くことにした。  \n（追伸：AI 嫌いの人は一番下までスクロールしてね）\n\n開発仲間と話すと、細部の違いはあっても大筋は似たアプローチに落ち着く。\n\n以下が自分のワークフローだ。自分の経験、友人との会話（ありがとう [Nikete](https://www.nikete.com/)、[Kanno](https://nocruft.com/)、[Obra](https://fsck.com/)、[Kris](https://github.com/KristopherKubicki)、[Erik](https://thinks.lol/)）、そしてネットのあちこちのひどい [bad](https://news.ycombinator.com/) [places](https://twitter.com) に転がっているベストプラクティスを組み合わせている。\n\n今のところ **バッチリ動く** が、二週間後には死んでいるか倍速で動くかもしれない。¯\\\\\\_(ツ)\\_/¯\n\n## さあ、行こうぜ\n\n{{< image src=\"llm-coding-robot.webp\" alt=\"Juggalo Robot\" caption=\"AI 生成画像ってどこか胡散臭い。ジャガロ（Juggalo）仕様のコーディングロボ天使にご挨拶！\" >}}\n\n開発スタイルはいろいろあるが、自分の場合はだいたい次の二つに分かれる。\n\n- Greenfield（新規開発）  \n- 一応モダンだけど実質レガシーなコードベース\n\nそれぞれのやり方を紹介する。\n\n## Greenfield\n\n新規開発では、以下のプロセスがよく機能する。しっかりした計画とドキュメントが手に入り、小さなステップで実装を進められる。\n\n{{< image src=\"greenfield.jpg\" alt=\"Green field\" caption=\"技術的には右側がグリーンフィールド。Leica Q, 2016/05/14\" >}}\n\n### Step 1: アイデアを磨く\n\n対話型 LLM（ChatGPT-4o / o3 など）でアイデアを絞り込む。\n\n```prompt\nAsk me one question at a time so we can develop a thorough, step-by-step spec for this idea. Each question should build on my previous answers, and our end goal is to have a detailed specification I can hand off to a developer. Let’s do this iteratively and dig into every relevant detail. Remember, only one question at a time.\n\nHere’s the idea:\n\n<IDEA>\n```\n\nブレインストーミングが自然に終わったら:\n\n```prompt\nNow that we’ve wrapped up the brainstorming process, can you compile our findings into a comprehensive, developer-ready specification? Include all relevant requirements, architecture choices, data handling details, error handling strategies, and a testing plan so a developer can immediately begin implementation.\n```\n\nこれでかなりしっかりした仕様書が得られ、次の計画ステップに渡せる。`spec.md` としてリポジトリに保存している。\n\n> この仕様書は多用途だ。今回はコード生成に使うが、別の推論モデルに渡してアイデアの穴を探させたり、ホワイトペーパーやビジネスモデルを作らせたりもできる。ディープリサーチに突っ込めば 1 万字の裏付けドキュメントも返ってくる。\n\n### Step 2: 計画\n\n仕様書を推論寄りのモデル（`o1*`, `o3*`, `r1` など）に渡す。\n\n（TDD 用プロンプト）\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. Review the results and make sure that the steps are small enough to be implemented safely with strong testing, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.\n\nFrom here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step in a test-driven manner. Prioritize best practices, incremental progress, and early testing, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.\n\nMake sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.\n\n<SPEC>\n```\n\n（非 TDD 用プロンプト）\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. review the results and make sure that the steps are small enough to be implemented safely, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.\n\nFrom here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step. Prioritize best practices, and incremental progress, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.\n\nMake sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.\n\n<SPEC>\n```\n\nこれで aider や cursor などで実行できるプロンプトプランが生成される。`prompt_plan.md` に保存している。\n\nさらにチェックリスト用に `todo.md` を作らせる。\n\n```prompt\nCan you make a `todo.md` that I can use as a checklist? Be thorough.\n```\n\nツール側で `todo.md` をチェックしながら進めると状態が保ちやすい。\n\n#### 計画完了！\n\nここまで **15 分** ほど。本当に速い。ワイルドだ。\n\n### Step 3: 実行\n\n実行フェーズの成否はステップ 2 の出来にかかっている。  \n[github workspace](https://githubnext.com/projects/copilot-workspace)、[aider](https://aider.chat/)、[cursor](https://www.cursor.com/)、[claude engineer](https://github.com/Doriandarko/claude-engineer)、[sweep.dev](https://sweep.dev/)、[chatgpt](https://chatgpt.com)、[claude.ai](https://claude.ai)――試したツールすべてでうまく動いた。\n\n自分はプレーンな **Claude** と aider 推しだ。\n\n### Claude\n\n自分が最初にボイラープレートやツール設定（例: `uv init`、`cargo init` など）を済ませておき、[claude.ai](https://claude.ai) とペアプロを進める。言語やスタイルを先に決めておくことで、Claude が React コードをやたら吐く癖を抑えられる。\n\nワークフローはこんな感じだ。\n\n- リポジトリを用意（雛形、ツール設定など）  \n- プロンプトを Claude に貼り付け  \n- 出力されたコードを IDE にコピペ  \n- 実行・テスト  \n- OK なら次へ  \n- ダメなら [repomix](https://github.com/yamadashy/repomix) でコードベースを Claude に渡し、デバッグしてもらう  \n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n### Aider\n\n[Aider](https://aider.chat/) はクセがあるが、ステップ 2 の成果と相性抜群で、驚くほど楽ができる。aider が “Just do it” してくれるので、その間は Cookie Clicker でも遊んでいられる。\n\n- リポジトリを用意  \n- aider を起動  \n- プロンプトを貼り付け  \n- aider のダンスを眺める ♪┏(・o･)┛♪  \n- aider がテストスイートを実行し、失敗箇所まで自動でデバッグ  \n- OK なら次へ  \n- ダメなら Q&A で修正  \n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n> ちなみに aider の [LLM ランキング](https://aider.chat/docs/leaderboards/) は新モデルの実力を測るのに超便利。\n\n### 成果\n\nこの方法でスクリプト、Expo アプリ、Rust CLI などを大量に作った。言語も環境も問わず機能する。\n\nハック用の To-Do リストは **空っぽ**。映画を観ながらでも思いついたアイデアを次々に片付けてしまう。ここ数年で初めて、新しい言語やツールにじっくり触れられている。おかげでプログラマーとしての視野が大きく広がった。\n\n寝かせているプロジェクトがあるなら、ぜひ試してみてほしい。短時間で驚くほど進む。\n\n## Non-greenfield: 既存コードを小刻みに反復改善する\n\n{{< image src=\"brownfield.jpg\" alt=\"a brown field\" caption=\"これはグリーンフィールドじゃない。祖父のカメラに残っていた 60 年代ウガンダの写真\" >}}\n\n既存コードを触るときは、計画をタスク単位で行う。\n\n### コンテキストを詰める\n\n自分は [repomix](https://github.com/yamadashy/repomix) と [mise](https://mise.jdx.dev/) を組み合わせている。`~/.config/mise/config.toml` にタスクを定義し、リポジトリごとに上書きも可能だ。\n\n```shell\nLLM:clean_bundles           repomix で LLM バンドルを生成\nLLM:copy_buffer_bundle      output.txt をクリップボードにコピー\nLLM:generate_code_review    output.txt からコードレビューを生成\nLLM:generate_github_issues  output.txt から GitHub Issues を生成\nLLM:generate_issue_prompts  output.txt から Issue 用プロンプトを生成\nLLM:generate_missing_tests  output.txt から足りないテストを洗い出す\nLLM:generate_readme         output.txt から README.md を生成\n```\n\n`output.txt` ができたら [LLM](https://github.com/simonw/LLM) コマンドで変換し Markdown に保存する。実際には `cat output.txt | LLM -t readme-gen > README.md` のような単純コマンドだ。\n\n> `mise` のいいところは、タスクをリポジトリ側の `.mise.toml` でオーバーライドできること。コードベースに合わせてパッキングツールや除外パターンを変えられる。\n\n### 例：テスト補完\n\n#### Claude\n\n1. リポジトリへ移動  \n2. `mise run LLM:generate_missing_tests`  \n3. `missing-tests.md` を確認  \n4. `mise run LLM:copy_buffer_bundle` で全文をコピー  \n5. Claude に貼り、最初の Issue を投げる  \n6. 出力を IDE に貼る → テスト  \n7. rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n#### Aider\n\n1. リポジトリへ移動（新しいブランチ推奨）  \n2. aider 起動  \n3. `mise run LLM:generate_missing_tests`  \n4. `missing-tests.md` を確認  \n5. Issue を aider に貼り付け  \n6. aider のダンスを眺める → テスト  \n7. rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\nこの方法なら、巨大なコードベースでも大小あらゆるタスクを捌ける。\n\n### プロンプト・マジック\n\n即席ハックでプロジェクトを堅牢化するのに驚くほど効く。定番プロンプトをいくつか紹介する。\n\n#### Code review\n\n```prompt\nYou are a senior developer. Your job is to do a thorough code review of this code. You should write it up and output markdown. Include line numbers, and contextual info. Your code review will be passed to another teammate, so be thorough. Think deeply  before writing the code review. Review every part, and don't hallucinate.\n```\n\n#### GitHub Issue generation\n\n```prompt\nYou are a senior developer. Your job is to review this code, and write out the top issues that you see with the code. It could be bugs, design choices, or code cleanliness issues. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues will be given to a developer to executed on, so they should be in a format that is compatible with github issues\n```\n\n#### Missing tests\n\n```prompt\nYou are a senior developer. Your job is to review this code, and write out a list of missing test cases, and code tests that should exist. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues  will be given to a developer to executed on, so they should be in a format that is compatible with github issues\n```\n\nこれらはいわば “ブーマープロンプト” なので、いい改善案があれば教えてほしい。\n\n## Skiing ᨒ↟ 𖠰ᨒ↟ 𖠰\n\nこのプロセスを人に説明するとき、いつも「勢い余って **over my skis**（前のめり）になりがちだから、アグレッシブに状況を追いかけろ」と言っている。\n\n例えるなら、気持ちよくパウダースノーを滑っていたのに突然「クソッ、何が起きてるんだ!?」と叫びながら崖から落ちる──そんなスピード感だ。\n\n計画ドキュメント（グリーンフィールドの手順）とテストは命綱になる。それでも崖から落ちそうになったら、少し休憩や散歩を挟むといい。\n\n> ときどき LLM に「ロア（lore）ファイルを作って UI で参照して」みたいなトンデモ要求をする。Python CLI ツールなのに突然ロアが登場したり、UI がグリッチしたり。空は限界ではない。\n\n## I am so lonely (｡•́︿•̀｡)\n\n何年もソロ開発、何年もペアプロ、何年もチーム開発をやってきたが──やはり仲間とやる方が断然楽しい。\n\n最大の不満は、このワークフローが基本的にソロプレイなことだ。ボット同士が衝突し、マージは地獄、コンテキスト管理はカオス。  \nLLM コーディングをマルチプレイヤー化する仕組みが切実に欲しい。誰か作れ！\n\n## ⴵ Time ⴵ\n\nコード生成でアウトプットは激増したが、LLM がトークンを燃やしている間の “待ち時間” も大量発生する。\n\n{{< image src=\"apple-print-shop-printing.png\" alt=\"Printing\" caption=\"まるで昨日のことみたい\" >}}\n\nその時間で\n\n- 次のプロジェクトをブレスト  \n- レコードを聴く  \n- [Cookie Clicker](https://orteil.dashnet.org/cookieclicker/)  \n- 友人やロボットと雑談  \n\nHack Hack Hack。これほど生産的だったことは過去にない。\n\n## Haterade ╭∩╮( •̀\\_•́ )╭∩╮\n\n「Fuck LLMs、全部クソだ」と言う友達も多い。懐疑的であることは大事。自分も主に電力消費と環境負荷が心配だ。でも……コードは流れ続ける。はぁ。\n\n深入りせず様子を見たい人には Ethan Mollick の  \n[**Co-Intelligence: Living and Working with AI**](https://www.penguinrandomhouse.com/books/741805/co-intelligence-by-ethan-mollick/) を勧める。テック・アナキャピタリストな啓発書ではなく、利点をバランスよく説明してくれる。読んだ友人たちとも良質な議論ができた。\n\n懐疑的だけど少し興味はある？ ぜひ連絡を。狂気を一緒に眺めつつ何か作ろう。\n\n_thanks to [Derek](https://derek.broox.com), [Kanno](https://nocruft.com/), [Obra](https://fsck.com), and [Erik](https://thinks.lol/) for taking a look at this post and suggesting edits. I appreciate it._\n\nPlease critique this translation mercilessly and provide detailed feedback on what needs to be improved.\nFormat your critique as a structured list of issues, organized by severity and category.\nInclude specific suggestions for how to fix each issue.",
      "response": "◎ = Critical (意味の改変・大幅な欠落)  \n◉ = Major (誤訳・重要ニュアンス／用語のぶれ)  \n● = Minor (細部の訳文の不自然さ、語調のずれ)  \n△ = Style / Naturalness (文体・語感・トーン)  \n\n─────────────────────  \n1. 意味の取り違え・欠落  \n─────────────────────  \n◎ 1-1 “plan a plan” が消失  \n　原文は “Brainstorm spec, then plan a plan, then execute …” と二重に “plan” を繰り返し、 「計画自体をさらに計画する」というメタ的ニュアンスを強調している。訳文は  \n　「次に計画を立て」だけで、この遊びが失われている。  \n　提案: 「次に『計画を計画』し」や「計画そのものも設計し」といった訳で反映。\n\n◉ 1-2 コマンド例の一部欠落  \n　mise タスク説明中の  \n　`cat output.txt | LLM -m claude-3.5-sonnet -t code-review-gen > code-review.md`  \n　が訳文では丸ごと省かれ、「複数例の提示」という情報を失った。  \n　提案: 省略せず両方のコマンドを示す。\n\n◉ 1-3 Aider 手順: 追加・欠落混在  \n　原文 bullet  \n　• “aider will run tests, or you can run app to verify”  \n　訳文では「失敗箇所まで自動でデバッグ」と追加し、 “or you can run app …” を欠落。挙動を変えている。  \n　提案: 原文通り「テストが走るか、自分でアプリを動かして確認」程度に留める。\n\n● 1-4 “...” (省略記号) の削除  \n　手順リスト中の “...” は「中間に色々あるけど省略」のニュアンス。訳文は削除。影響は小。  \n\n─────────────────────  \n2. 文化的・語調ニュアンスのずれ  \n─────────────────────  \n◉ 2-1 “terrible internet bad places”  \n　原文は自嘲気味な二重否定ギャグ。訳文「ネットのあちこちのひどい … に転がっている」はやや説明調。  \n　提案: 「インターネットの“魔境”みたいな所に転がっているベストプラクティス」等、皮肉を強める。\n\n● 2-2 “¯\\_(ツ)_/¯” の直前  \n　原文 “it will probably not work in 2 weeks, or it will work twice as well.”  \n　訳文「死んでいるか倍速で動くかもしれない」はほぼ同義だが “倍速” はやや機械的。 「倍の威力で動いているかも」などでもっと口語に。\n\n△ 2-3 タイトル系の口調  \n　原文は “Let’s go” “Yay. Plan!” などテンション高め。訳文は  \n　「## さあ、行こうぜ」「#### 計画完了！」で硬軟が混在している。  \n　提案: 全体でラフなネット口語に統一するか、少しだけ丁寧に寄せるか方向性を決める。\n\n─────────────────────  \n3. 用語・技術語の一貫性  \n─────────────────────  \n◉ 3-1 “推論寄りのモデル”  \n　一般的には “reasoning-heavy model” → 「推論向き・推論重視モデル」等。 「寄り」は口語だが意味が曖昧。  \n　提案: 「推論特化モデル」「推論重視のモデル」へ修正。\n\n● 3-2 “Greenfield / Brownfield”  \n　Greenfield はカタカナで残し、Brownfield を「既存コード」だけにしたため対になる語が消えた。  \n　提案: 初出で「Brownfield（既存プロジェクト）」と並列表記し以後統一。\n\n─────────────────────  \n4. 追加情報・改変  \n─────────────────────  \n◎ 4-1 “失敗箇所まで自動でデバッグ” は原文にない挙動の追加。  \n　実際の動きを誤解させる可能性があるため削除 or 原文準拠に修正。\n\n─────────────────────  \n5. 文法・表記ゆれ・不自然表現  \n─────────────────────  \n● 5-1 「仕様を磨き」  \n　“hone in on an idea” は「アイデアを絞り込む」。Step1 見出しは後で「アイデアを磨く」と訳しているので 冒頭 tl;dr も揃える。  \n● 5-2 「ループを細かく区切って回し」  \n　やや直訳感。→「ループを小さく回し」程度で自然。  \n● 5-3 「ツール側で `todo.md` をチェックしながら進めると状態が保ちやすい。」  \n　主体が曖昧。→「コード生成ツールが `todo.md` を自動でチェックしてくれるので、 セッションをまたいでも状態を保ちやすい。」  \n△ 5-4 全体の漢字・かな混在  \n　例: 「リポジトリを用意」⇔「repo をセットアップ」。英語/カナ/漢字の揺れを整理。  \n\n─────────────────────  \n6. 機械翻訳的・硬さを感じる箇所  \n─────────────────────  \n● 6-1 「コンテキストを詰める」  \n　意味は通じるがやや不自然。「コードベースを LLM に効率よく流し込む」など。  \n● 6-2 「驚くほど楽ができる」  \n　カジュアル文体としては問題ないが、少し昭和感。→「めちゃくちゃ楽できる」にすると原文の軽さに近い。  \n● 6-3 長い英語原文の引用が続く部分で、前後の日本語が急に敬体になったり常体になったりする箇所が点在。 文末の統一が甘い。  \n\n─────────────────────  \n7. 推奨全体調整  \n─────────────────────  \n• 口語／砕けたネットスラング系で行くと決めたなら徹底し、 敬体混入 (“〜です。〜ます。”) を排除。逆も同様。  \n• 英語プロンプトは原文維持で OK。ただし直後のメタ説明は 「英語原文→日本語補足→英語プロンプト」に統一すると読みやすい。  \n• 絵文字・装飾 (“✩₊˚.⋆☾⋆⁺₊✧”) はなるべく原文位置を維持。冒頭では残したが、途中で抜けている箇所がある。  \n\n─────────────────────  \n修正例（冒頭 tl;dr のみ）  \n原訳:  \nまずブレインストーミングで仕様を磨き、次に計画を立て、最後に LLM のコード生成で実装。ループを細かく区切って回し、あとは魔法  \n\n改善案:  \nまずブレインストーミングで仕様を固め、次に “計画そのものを計画” し、最後に LLM でコード生成。 小さなループを回していき、仕上げは魔法 ✩₊˚.⋆☾⋆⁺₊✧  \n\n上記ポイントを修正すれば、原稿の遊び心と情報を漏らさない、 こなれた日本語訳に仕上がります。",
      "usage": {
        "prompt_tokens": 9858,
        "completion_tokens": 1820,
        "total_tokens": 11678
      },
      "streaming": true
    },
    "feedback": {
      "model": "o3",
      "target_language": "japanese",
      "system_prompt": "You are a master translator and editor specializing in japanese.\nYour task is to improve a translation based on detailed critique feedback.\n\n1. Carefully read the original text, current translation, and the critique feedback\n2. Address ALL issues identified in the critique\n3. Apply the specific suggestions for improvement\n4. Ensure the translation is accurate, natural-sounding, and faithful to the original\n5. Preserve all formatting, markdown, and structure of the original text\n6. Make sure the final text reads as if it were originally written in japanese\n\nDo not ignore any of the critique points. Every issue identified must be addressed in your improved version.\n",
      "user_prompt": "# ORIGINAL TEXT\n_tl:dr; Brainstorm spec, then plan a plan, then execute using LLM codegen. Discrete loops. Then magic. ✩₊˚.⋆☾⋆⁺₊✧_\n\nI have been building so many small products using LLMs. It has been fun, and useful. However, there are pitfalls that can waste so much time. A while back a friend asked me how I was using LLMs to write software. I thought \"oh boy. how much time do you have!\" and thus this post.\n\n(p.s. if you are an AI hater - scroll to the end)\n\nI talk to many dev friends about this, and we all have a similar approach with various tweaks in either direction.\n\nHere is my workflow. It is built upon my own work, conversations with friends (thx [Nikete](https://www.nikete.com/), [Kanno](https://nocruft.com/), [Obra](https://fsck.com/), [Kris](https://github.com/KristopherKubicki), and [Erik](https://thinks.lol/)), and following many best practices shared on the various terrible internet [bad](https://news.ycombinator.com/) [places](https://twitter.com).\n\nThis is working well **NOW**, it will probably not work in 2 weeks, or it will work twice as well. ¯\\\\\\_(ツ)\\_/¯\n\n## Let’s go\n\n{{< image src=\"llm-coding-robot.webp\" alt=\"Juggalo Robot\" caption=\"I always find these AI-generated images to be suspect. Say hi to my juggalo coding robot angel!\" >}}\n\nThere are many paths for doing dev, but my case is typically one of two:\n\n- Greenfield code\n- Legacy modern code\n\nI will show you my process for both paths\n\n## Greenfield\n\nI find the following process works well for greenfield development. It provides a robust planning and documentation approach, and allows you to execute easily in small steps.\n\n{{< image src=\"greenfield.jpg\" alt=\"Green field\" caption=\"Technically, there is a green field on the right. Leica Q, 5/14/2016\" >}}\n\n### Step 1: Idea honing\n\nUse a conversational LLM to hone in on an idea (I use ChatGPT 4o / o3 for this):\n\n```prompt\nAsk me one question at a time so we can develop a thorough, step-by-step spec for this idea. Each question should build on my previous answers, and our end goal is to have a detailed specification I can hand off to a developer. Let’s do this iteratively and dig into every relevant detail. Remember, only one question at a time.\n\nHere’s the idea:\n\n<IDEA>\n```\n\nAt the end of the brainstorm (it will come to a natural conclusion):\n\n```prompt\nNow that we’ve wrapped up the brainstorming process, can you compile our findings into a comprehensive, developer-ready specification? Include all relevant requirements, architecture choices, data handling details, error handling strategies, and a testing plan so a developer can immediately begin implementation.\n```\n\nThis will output a pretty solid and straightforward spec that can be handed off to the planning step. I like to save it as `spec.md` in the repo.\n\n> You can use this spec for a number of things. We are doing codegen here, but I have used it to bolster ideas by asking a reasoning model to poke holes in the idea (must go deeper!), to generate a white paper, or to generate a business model. You can pop it into deep research and get a 10k word supporting document in return.\n\n### Step 2: Planning\n\nTake the spec and pass it to a proper reasoning model (`o1*`, `o3*`, `r1`):\n\n(This is the TDD prompt)\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. Review the results and make sure that the steps are small enough to be implemented safely with strong testing, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.\n\nFrom here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step in a test-driven manner. Prioritize best practices, incremental progress, and early testing, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.\n\nMake sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.\n\n<SPEC>\n```\n\n(This is the non-tdd prompt)\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. review the results and make sure that the steps are small enough to be implemented safely, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.\n\nFrom here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step. Prioritize best practices, and incremental progress, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.\n\nMake sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.\n\n<SPEC>\n```\n\nIt should output a prompt plan that you can execute with aider, cursor, etc. I like to save this as `prompt_plan.md` in the repo.\n\nI then have it output a `todo.md` that can be checked off.\n\n```prompt\nCan you make a `todo.md` that I can use as a checklist? Be thorough.\n```\n\nYou can save it as `todo.md` in the repo.\n\nYour codegen tool should be able to check off the `todo.md` while processing. This is good for keeping state across sessions.\n\n#### Yay. Plan!\n\nNow you have a robust plan and documentation that will help you execute and build your project.\n\nThis entire process will take maybe **15 minutes**. It is pretty quick. Wild tbh.\n\n### Step 3: Execution\n\nThere are so many options available for execution. The success really depends on how well step 2 went.\n\nI have used this workflow with [github workspace](https://githubnext.com/projects/copilot-workspace), [aider](https://aider.chat/), [cursor](https://www.cursor.com/), [claude engineer](https://github.com/Doriandarko/claude-engineer), [sweep.dev](https://sweep.dev/), [chatgpt](https://chatgpt.com), [claude.ai](https://claude.ai), etc. It works pretty well with all the tools I have tried, and I imagine it will work well with any codegen tool.\n\nI, however, prefer **raw** claude and aider:\n\n### Claude\n\nI essentially pair program with [claude.ai](https://claude.ai) and just drop each prompt in iteratively. I find that works pretty well. The back and forth can be annoying, but it largely works.\n\nI am in charge of the initial boilerplate code, and making sure tooling is set up correctly. This allows for some freedom, choice, and guidance in the beginning. Claude has a tendency to just output react code - and having a solid foundation with the language, style, and tooling of your choice will help quite a bit.\n\nI will then use a tool like [repomix](https://github.com/yamadashy/repomix) to iterate when things get stuck (more about that later).\n\nThe workflow is like this:\n\n- set up the repo (boilerplate, uv init, cargo init, etc)\n- paste in prompt into claude\n- copy and paste code from claude.ai into IDE\n- run code, run tests, etc\n- ...\n- if it works, move on to next prompt\n- if it doesn’t work, use repomix to pass the codebase to claude to debug\n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n### Aider\n\n[Aider](https://aider.chat/) is fun and weird to use. I find that it slots in well to the output of step 2. I can get really far with very little work.\n\nThe workflow is essentially the same as above but instead of pasting into claude, I am pasting the prompts into aider.\n\nAider will then “just do it” and I get to play [cookie clicker](https://orteil.dashnet.org/cookieclicker/).\n\n> An aside: Aider does really great benchmarking of new models for codegen in their [LLM leaderboards](https://aider.chat/docs/leaderboards/). I find it to be a really great resource for seeing how effective new models are.\n\nTesting is nice with aider, because it can be even more hands off as aider will run the test suite and debug things for you.\n\nThe workflow is like this:\n\n- set up the repo (boilerplate, uv init, cargo init, etc)\n- start aider\n- paste prompt into aider\n- watch aider dance ♪┏(・o･)┛♪\n- aider will run tests, or you can run app to verify\n- if it works, move on to next prompt\n- if it doesn’t work, Q&A with aider to fix\n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n### Results\n\nI have built so so many things using this workflow: scripts, expo apps, rust cli tools, etc. It has worked across programming languages, and contexts. I do like it.\n\nIf you have a small or large project that you are procrastinating on, I would recommend giving it a shot. You will be surprised how far you can get in a short amount of time.\n\nMy hack to-do list is empty because I built everything. I keep thinking of new things and knocking them out while watching a movie or something. For the first time in years, I am spending time with new programming languages and tools. This is pushing me to expand my programming perspective.\n\n## Non-greenfield: Iteration, incrementally\n\nSometimes you don’t have greenfield, and instead need to iterate or do increment work on an established code base.\n\n{{< image src=\"brownfield.jpg\" alt=\"a brown field\" caption=\"This is not a green field. A random photo from my grandfather's camera - somewhere in Uganda in the 60s\" >}}\n\nFor this I have a slightly different method. It is similar to above, but a bit less “planning based.” The planning is done per task, not for the entire project.\n\n### Get context\n\nI think everyone who is knee-deep in AI dev has a different tool for this, but you need something to grab your source code and efficiently jam it into the LLM.\n\nI currently use a tool called [repomix](https://github.com/yamadashy/repomix). I have a task collection defined in my global `~/.config/mise/config.toml` that allows me to do various things with my code base ([mise rules](https://mise.jdx.dev/)).\n\nHere is the LLM task list:\n\n```shell\nLLM:clean_bundles           Generate LLM bundle output file using repomix\nLLM:copy_buffer_bundle      Copy generated LLM bundle from output.txt to system clipboard for external use\nLLM:generate_code_review    Generate code review output from repository content stored in output.txt using LLM generation\nLLM:generate_github_issues  Generate GitHub issues from repository content stored in output.txt using LLM generation\nLLM:generate_issue_prompts  Generate issue prompts from repository content stored in output.txt using LLM generation\nLLM:generate_missing_tests  Generate missing tests for code in repository content stored in output.txt using LLM generation\nLLM:generate_readme         Generate README.md from repository content stored in output.txt using LLM generation\n```\n\nI generate an `output.txt` that has the context from my code base. If I am blowing through tokens, and it is too big - I will edit the generate command to ignore parts of the code base that are not germane to this task.\n\n> One thing really nice about `mise` is that the tasks can be redefined and overloaded in the working directory's `.mise.toml`. I can use a different tool to dump/pack the code, and as long as it generates an `output.txt` I can use my LLM tasks. This is helpful when various codebases differ so much. I regularly override the `repomix` step to include broader ignore patterns, or just use a more effective tool to do the packing.\n\nOnce the output.txt is generated, I pass it to the [LLM](https://github.com/simonw/LLM) command to do various transformations and then save those as a markdown file.\n\nUltimately, the mise task is running this: `cat output.txt | LLM -t readme-gen > README.md` or `cat output.txt | LLM -m claude-3.5-sonnet -t code-review-gen > code-review.md`. This isn't super complicated. the `LLM` command is doing the heavy lifting (supporting different models, saving keys, and using prompt templates).\n\nFor example, if I need a quick review and fix of test coverage I would do the following:\n\n#### Claude\n\n- go to the directory where the code lives\n- run `mise run LLM:generate_missing_tests`\n- look at the generated markdown file (`missing-tests.md`)\n- grab the full context for the code: `mise run LLM:copy_buffer_bundle`\n- paste that into claude along with the first missing test “issue”\n- copy the generated code from claude into my ide.\n- ...\n- run tests\n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n#### Aider\n\n- go to the directory where the code lives\n- run aider (always make sure you are on a new branch for aider work)\n- run `mise run LLM:generate_missing_tests`\n- look at the generated markdown file (`missing-tests.md`)\n- paste the first missing test “issue” into aider\n- watch aider dance ♪┏(・o･)┛♪\n- ...\n- run tests\n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\nThis is a pretty good way to incrementally improve a code base. It has been super helpful to do small amounts of work in a big code base. I have found that I can do any sized tasks with this method.\n\n### Prompt magic\n\nThese quick hacks work super well to dig into places where we can make a project more robust. It is super quick, and effective.\n\nHere are some of my prompts that I use to dig into established code bases:\n\n#### Code review\n\n```prompt\nYou are a senior developer. Your job is to do a thorough code review of this code. You should write it up and output markdown. Include line numbers, and contextual info. Your code review will be passed to another teammate, so be thorough. Think deeply  before writing the code review. Review every part, and don't hallucinate.\n```\n\n#### GitHub Issue generation\n\n(I need to automate the actual issue posting!)\n\n```prompt\nYou are a senior developer. Your job is to review this code, and write out the top issues that you see with the code. It could be bugs, design choices, or code cleanliness issues. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues will be given to a developer to executed on, so they should be in a format that is compatible with github issues\n```\n\n#### Missing tests\n\n```prompt\nYou are a senior developer. Your job is to review this code, and write out a list of missing test cases, and code tests that should exist. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues  will be given to a developer to executed on, so they should be in a format that is compatible with github issues\n```\n\nThese prompts are pretty _old and busted_ (\"boomer prompts\" if I may). They need some refactoring. If you have ideas to make them better lmk.\n\n## Skiing ᨒ↟ 𖠰ᨒ↟ 𖠰\n\nWhen I describe this process to people I say “you have to aggressively keep track of what’s going on because you can easily get ahead of yourself.”\n\nFor some reason I say \"over my skis\" a lot when talking about LLMs. I don't know why. It resonates with me. Maybe it's because it is beautiful smooth powder skiing, and then all of a sudden you are like \"WHAT THE FUCK IS GOING ON!,\" and are completely lost and suddenly fall off a cliff.\n\nI find that using a **planning step** (ala the Greenfield process above) can help keep things under control. At least you will have a doc you can double-check against. I also do believe that testing is helpful - especially if you are doing wild style aider coding. Helps keep things good, and tight.\n\nRegardless, I still do find myself **over my skis** quite a bit. Sometimes a quick break or short walk will help. In this regard it is a normal problem-solving process, but accelerated to a breakneck speed.\n\n> We will often ask the LLM to include ridiculous things in our not very ridiculous code. For instance, we asked it to create a lore file and then reference the lore in the user interface. This is for python cli tools. Suddenly there is lore, glitchy interfaces, etc. All to manage your cloud functions, your todo list or whatever. The sky is the limit.\n\n## I am so lonely (｡•́︿•̀｡)\n\nMy main complaint about these workflows is that it is largely a solo endeavor - i.e. the interfaces are all _single player mode_.\n\nI have spent years coding by myself, years coding as a pair, and years coding in a team. It is always better with people. These workflows are not easy to use as a team. The bots collide, the merges are horrific, the context complicated.\n\nI really want someone to solve this problem in a way that makes coding with an LLM a multiplayer game. Not a solo hacker experience. There is so much opportunity to fix this and make it amazing.\n\nGET TO WORK!\n\n## ⴵ Time ⴵ\n\nAll this codegen has accelerated the amount of code that I as a single person am able to generate. However, there is a weird side effect. I find myself having a huge amount of “downtime” while waiting for the LLM to finish burning its tokens.\n\n{{< image src=\"apple-print-shop-printing.png\" alt=\"Printing\" caption=\"I remember this like it was yesterday\" >}}\n\nI have changed how I work enough to start incorporating some practice that will try and eat the waiting time:\n\n- I start the “brainstorming” process for another project\n- I listen to records\n- I play [cookie clicker](https://orteil.dashnet.org/cookieclicker/)\n- I talk with friends and robots\n\nIt is awesome to be able to hack like this. Hack Hack Hack. I can't think of another time I have been this productive in code.\n\n## Haterade ╭∩╮( •̀\\_•́ )╭∩╮\n\nA lot of my friends are like \"fuck LLMs. They are terrible at everything.\" I don't mind this POV. I don't share it, but I think it is important to be skeptical. There are an awful lot of reasons to hate AI. My main fear is about power consumption and the environmental impact. But... the code must flow. Right... sigh.\n\nIf you are open to learning more, but don't want to dig in and become a cyborg programmer - my recommendation is not to change your opinion, but to read Ethan Mollick's book about LLMs and how they can be used: [**Co-Intelligence: Living and Working with AI.**](https://www.penguinrandomhouse.com/books/741805/co-intelligence-by-ethan-mollick/)\n\nIt does a good job of explaining the benefits without being a tech anarcho-capitalist bro type tome. I found it very helpful, and have had so many good and nuanced conversations with friends who have read it. Highly recommended.\n\nIf you are skeptical, but a bit curious - feel free to hit me up and let's talk through all this madness. I can show you how we use LLMs, and maybe we could build something together.\n\n_thanks to [Derek](https://derek.broox.com), [Kanno](https://nocruft.com/), [Obra](https://fsck.com), and [Erik](https://thinks.lol/) for taking a look at this post and suggesting edits. I appreciate it._\n\n# CURRENT TRANSLATION\n_tl;dr_  \nまずブレインストーミングで仕様を磨き、次に計画を立て、最後に LLM のコード生成で実装。ループを細かく区切って回し、あとは魔法 ✩₊˚.⋆☾⋆⁺₊✧\n\n最近、LLM を使って小さなプロダクトを山ほど作っている。楽しいし実用的だが、時間を溶かす落とし穴も多い。少し前に友人から「どうやって LLM でソフトを書いてるの？」と聞かれ、「おいおい、どれだけ時間ある？」と思ったので、このポストを書くことにした。  \n（追伸：AI 嫌いの人は一番下までスクロールしてね）\n\n開発仲間と話すと、細部の違いはあっても大筋は似たアプローチに落ち着く。\n\n以下が自分のワークフローだ。自分の経験、友人との会話（ありがとう [Nikete](https://www.nikete.com/)、[Kanno](https://nocruft.com/)、[Obra](https://fsck.com/)、[Kris](https://github.com/KristopherKubicki)、[Erik](https://thinks.lol/)）、そしてネットのあちこちのひどい [bad](https://news.ycombinator.com/) [places](https://twitter.com) に転がっているベストプラクティスを組み合わせている。\n\n今のところ **バッチリ動く** が、二週間後には死んでいるか倍速で動くかもしれない。¯\\\\\\_(ツ)\\_/¯\n\n## さあ、行こうぜ\n\n{{< image src=\"llm-coding-robot.webp\" alt=\"Juggalo Robot\" caption=\"AI 生成画像ってどこか胡散臭い。ジャガロ（Juggalo）仕様のコーディングロボ天使にご挨拶！\" >}}\n\n開発スタイルはいろいろあるが、自分の場合はだいたい次の二つに分かれる。\n\n- Greenfield（新規開発）  \n- 一応モダンだけど実質レガシーなコードベース\n\nそれぞれのやり方を紹介する。\n\n## Greenfield\n\n新規開発では、以下のプロセスがよく機能する。しっかりした計画とドキュメントが手に入り、小さなステップで実装を進められる。\n\n{{< image src=\"greenfield.jpg\" alt=\"Green field\" caption=\"技術的には右側がグリーンフィールド。Leica Q, 2016/05/14\" >}}\n\n### Step 1: アイデアを磨く\n\n対話型 LLM（ChatGPT-4o / o3 など）でアイデアを絞り込む。\n\n```prompt\nAsk me one question at a time so we can develop a thorough, step-by-step spec for this idea. Each question should build on my previous answers, and our end goal is to have a detailed specification I can hand off to a developer. Let’s do this iteratively and dig into every relevant detail. Remember, only one question at a time.\n\nHere’s the idea:\n\n<IDEA>\n```\n\nブレインストーミングが自然に終わったら:\n\n```prompt\nNow that we’ve wrapped up the brainstorming process, can you compile our findings into a comprehensive, developer-ready specification? Include all relevant requirements, architecture choices, data handling details, error handling strategies, and a testing plan so a developer can immediately begin implementation.\n```\n\nこれでかなりしっかりした仕様書が得られ、次の計画ステップに渡せる。`spec.md` としてリポジトリに保存している。\n\n> この仕様書は多用途だ。今回はコード生成に使うが、別の推論モデルに渡してアイデアの穴を探させたり、ホワイトペーパーやビジネスモデルを作らせたりもできる。ディープリサーチに突っ込めば 1 万字の裏付けドキュメントも返ってくる。\n\n### Step 2: 計画\n\n仕様書を推論寄りのモデル（`o1*`, `o3*`, `r1` など）に渡す。\n\n（TDD 用プロンプト）\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. Review the results and make sure that the steps are small enough to be implemented safely with strong testing, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.\n\nFrom here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step in a test-driven manner. Prioritize best practices, incremental progress, and early testing, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.\n\nMake sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.\n\n<SPEC>\n```\n\n（非 TDD 用プロンプト）\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. review the results and make sure that the steps are small enough to be implemented safely, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.\n\nFrom here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step. Prioritize best practices, and incremental progress, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.\n\nMake sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.\n\n<SPEC>\n```\n\nこれで aider や cursor などで実行できるプロンプトプランが生成される。`prompt_plan.md` に保存している。\n\nさらにチェックリスト用に `todo.md` を作らせる。\n\n```prompt\nCan you make a `todo.md` that I can use as a checklist? Be thorough.\n```\n\nツール側で `todo.md` をチェックしながら進めると状態が保ちやすい。\n\n#### 計画完了！\n\nここまで **15 分** ほど。本当に速い。ワイルドだ。\n\n### Step 3: 実行\n\n実行フェーズの成否はステップ 2 の出来にかかっている。  \n[github workspace](https://githubnext.com/projects/copilot-workspace)、[aider](https://aider.chat/)、[cursor](https://www.cursor.com/)、[claude engineer](https://github.com/Doriandarko/claude-engineer)、[sweep.dev](https://sweep.dev/)、[chatgpt](https://chatgpt.com)、[claude.ai](https://claude.ai)――試したツールすべてでうまく動いた。\n\n自分はプレーンな **Claude** と aider 推しだ。\n\n### Claude\n\n自分が最初にボイラープレートやツール設定（例: `uv init`、`cargo init` など）を済ませておき、[claude.ai](https://claude.ai) とペアプロを進める。言語やスタイルを先に決めておくことで、Claude が React コードをやたら吐く癖を抑えられる。\n\nワークフローはこんな感じだ。\n\n- リポジトリを用意（雛形、ツール設定など）  \n- プロンプトを Claude に貼り付け  \n- 出力されたコードを IDE にコピペ  \n- 実行・テスト  \n- OK なら次へ  \n- ダメなら [repomix](https://github.com/yamadashy/repomix) でコードベースを Claude に渡し、デバッグしてもらう  \n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n### Aider\n\n[Aider](https://aider.chat/) はクセがあるが、ステップ 2 の成果と相性抜群で、驚くほど楽ができる。aider が “Just do it” してくれるので、その間は Cookie Clicker でも遊んでいられる。\n\n- リポジトリを用意  \n- aider を起動  \n- プロンプトを貼り付け  \n- aider のダンスを眺める ♪┏(・o･)┛♪  \n- aider がテストスイートを実行し、失敗箇所まで自動でデバッグ  \n- OK なら次へ  \n- ダメなら Q&A で修正  \n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n> ちなみに aider の [LLM ランキング](https://aider.chat/docs/leaderboards/) は新モデルの実力を測るのに超便利。\n\n### 成果\n\nこの方法でスクリプト、Expo アプリ、Rust CLI などを大量に作った。言語も環境も問わず機能する。\n\nハック用の To-Do リストは **空っぽ**。映画を観ながらでも思いついたアイデアを次々に片付けてしまう。ここ数年で初めて、新しい言語やツールにじっくり触れられている。おかげでプログラマーとしての視野が大きく広がった。\n\n寝かせているプロジェクトがあるなら、ぜひ試してみてほしい。短時間で驚くほど進む。\n\n## Non-greenfield: 既存コードを小刻みに反復改善する\n\n{{< image src=\"brownfield.jpg\" alt=\"a brown field\" caption=\"これはグリーンフィールドじゃない。祖父のカメラに残っていた 60 年代ウガンダの写真\" >}}\n\n既存コードを触るときは、計画をタスク単位で行う。\n\n### コンテキストを詰める\n\n自分は [repomix](https://github.com/yamadashy/repomix) と [mise](https://mise.jdx.dev/) を組み合わせている。`~/.config/mise/config.toml` にタスクを定義し、リポジトリごとに上書きも可能だ。\n\n```shell\nLLM:clean_bundles           repomix で LLM バンドルを生成\nLLM:copy_buffer_bundle      output.txt をクリップボードにコピー\nLLM:generate_code_review    output.txt からコードレビューを生成\nLLM:generate_github_issues  output.txt から GitHub Issues を生成\nLLM:generate_issue_prompts  output.txt から Issue 用プロンプトを生成\nLLM:generate_missing_tests  output.txt から足りないテストを洗い出す\nLLM:generate_readme         output.txt から README.md を生成\n```\n\n`output.txt` ができたら [LLM](https://github.com/simonw/LLM) コマンドで変換し Markdown に保存する。実際には `cat output.txt | LLM -t readme-gen > README.md` のような単純コマンドだ。\n\n> `mise` のいいところは、タスクをリポジトリ側の `.mise.toml` でオーバーライドできること。コードベースに合わせてパッキングツールや除外パターンを変えられる。\n\n### 例：テスト補完\n\n#### Claude\n\n1. リポジトリへ移動  \n2. `mise run LLM:generate_missing_tests`  \n3. `missing-tests.md` を確認  \n4. `mise run LLM:copy_buffer_bundle` で全文をコピー  \n5. Claude に貼り、最初の Issue を投げる  \n6. 出力を IDE に貼る → テスト  \n7. rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n#### Aider\n\n1. リポジトリへ移動（新しいブランチ推奨）  \n2. aider 起動  \n3. `mise run LLM:generate_missing_tests`  \n4. `missing-tests.md` を確認  \n5. Issue を aider に貼り付け  \n6. aider のダンスを眺める → テスト  \n7. rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\nこの方法なら、巨大なコードベースでも大小あらゆるタスクを捌ける。\n\n### プロンプト・マジック\n\n即席ハックでプロジェクトを堅牢化するのに驚くほど効く。定番プロンプトをいくつか紹介する。\n\n#### Code review\n\n```prompt\nYou are a senior developer. Your job is to do a thorough code review of this code. You should write it up and output markdown. Include line numbers, and contextual info. Your code review will be passed to another teammate, so be thorough. Think deeply  before writing the code review. Review every part, and don't hallucinate.\n```\n\n#### GitHub Issue generation\n\n```prompt\nYou are a senior developer. Your job is to review this code, and write out the top issues that you see with the code. It could be bugs, design choices, or code cleanliness issues. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues will be given to a developer to executed on, so they should be in a format that is compatible with github issues\n```\n\n#### Missing tests\n\n```prompt\nYou are a senior developer. Your job is to review this code, and write out a list of missing test cases, and code tests that should exist. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues  will be given to a developer to executed on, so they should be in a format that is compatible with github issues\n```\n\nこれらはいわば “ブーマープロンプト” なので、いい改善案があれば教えてほしい。\n\n## Skiing ᨒ↟ 𖠰ᨒ↟ 𖠰\n\nこのプロセスを人に説明するとき、いつも「勢い余って **over my skis**（前のめり）になりがちだから、アグレッシブに状況を追いかけろ」と言っている。\n\n例えるなら、気持ちよくパウダースノーを滑っていたのに突然「クソッ、何が起きてるんだ!?」と叫びながら崖から落ちる──そんなスピード感だ。\n\n計画ドキュメント（グリーンフィールドの手順）とテストは命綱になる。それでも崖から落ちそうになったら、少し休憩や散歩を挟むといい。\n\n> ときどき LLM に「ロア（lore）ファイルを作って UI で参照して」みたいなトンデモ要求をする。Python CLI ツールなのに突然ロアが登場したり、UI がグリッチしたり。空は限界ではない。\n\n## I am so lonely (｡•́︿•̀｡)\n\n何年もソロ開発、何年もペアプロ、何年もチーム開発をやってきたが──やはり仲間とやる方が断然楽しい。\n\n最大の不満は、このワークフローが基本的にソロプレイなことだ。ボット同士が衝突し、マージは地獄、コンテキスト管理はカオス。  \nLLM コーディングをマルチプレイヤー化する仕組みが切実に欲しい。誰か作れ！\n\n## ⴵ Time ⴵ\n\nコード生成でアウトプットは激増したが、LLM がトークンを燃やしている間の “待ち時間” も大量発生する。\n\n{{< image src=\"apple-print-shop-printing.png\" alt=\"Printing\" caption=\"まるで昨日のことみたい\" >}}\n\nその時間で\n\n- 次のプロジェクトをブレスト  \n- レコードを聴く  \n- [Cookie Clicker](https://orteil.dashnet.org/cookieclicker/)  \n- 友人やロボットと雑談  \n\nHack Hack Hack。これほど生産的だったことは過去にない。\n\n## Haterade ╭∩╮( •̀\\_•́ )╭∩╮\n\n「Fuck LLMs、全部クソだ」と言う友達も多い。懐疑的であることは大事。自分も主に電力消費と環境負荷が心配だ。でも……コードは流れ続ける。はぁ。\n\n深入りせず様子を見たい人には Ethan Mollick の  \n[**Co-Intelligence: Living and Working with AI**](https://www.penguinrandomhouse.com/books/741805/co-intelligence-by-ethan-mollick/) を勧める。テック・アナキャピタリストな啓発書ではなく、利点をバランスよく説明してくれる。読んだ友人たちとも良質な議論ができた。\n\n懐疑的だけど少し興味はある？ ぜひ連絡を。狂気を一緒に眺めつつ何か作ろう。\n\n_thanks to [Derek](https://derek.broox.com), [Kanno](https://nocruft.com/), [Obra](https://fsck.com), and [Erik](https://thinks.lol/) for taking a look at this post and suggesting edits. I appreciate it._\n\n# CRITIQUE FEEDBACK\n◎ = Critical (意味の改変・大幅な欠落)  \n◉ = Major (誤訳・重要ニュアンス／用語のぶれ)  \n● = Minor (細部の訳文の不自然さ、語調のずれ)  \n△ = Style / Naturalness (文体・語感・トーン)  \n\n─────────────────────  \n1. 意味の取り違え・欠落  \n─────────────────────  \n◎ 1-1 “plan a plan” が消失  \n　原文は “Brainstorm spec, then plan a plan, then execute …” と二重に “plan” を繰り返し、 「計画自体をさらに計画する」というメタ的ニュアンスを強調している。訳文は  \n　「次に計画を立て」だけで、この遊びが失われている。  \n　提案: 「次に『計画を計画』し」や「計画そのものも設計し」といった訳で反映。\n\n◉ 1-2 コマンド例の一部欠落  \n　mise タスク説明中の  \n　`cat output.txt | LLM -m claude-3.5-sonnet -t code-review-gen > code-review.md`  \n　が訳文では丸ごと省かれ、「複数例の提示」という情報を失った。  \n　提案: 省略せず両方のコマンドを示す。\n\n◉ 1-3 Aider 手順: 追加・欠落混在  \n　原文 bullet  \n　• “aider will run tests, or you can run app to verify”  \n　訳文では「失敗箇所まで自動でデバッグ」と追加し、 “or you can run app …” を欠落。挙動を変えている。  \n　提案: 原文通り「テストが走るか、自分でアプリを動かして確認」程度に留める。\n\n● 1-4 “...” (省略記号) の削除  \n　手順リスト中の “...” は「中間に色々あるけど省略」のニュアンス。訳文は削除。影響は小。  \n\n─────────────────────  \n2. 文化的・語調ニュアンスのずれ  \n─────────────────────  \n◉ 2-1 “terrible internet bad places”  \n　原文は自嘲気味な二重否定ギャグ。訳文「ネットのあちこちのひどい … に転がっている」はやや説明調。  \n　提案: 「インターネットの“魔境”みたいな所に転がっているベストプラクティス」等、皮肉を強める。\n\n● 2-2 “¯\\_(ツ)_/¯” の直前  \n　原文 “it will probably not work in 2 weeks, or it will work twice as well.”  \n　訳文「死んでいるか倍速で動くかもしれない」はほぼ同義だが “倍速” はやや機械的。 「倍の威力で動いているかも」などでもっと口語に。\n\n△ 2-3 タイトル系の口調  \n　原文は “Let’s go” “Yay. Plan!” などテンション高め。訳文は  \n　「## さあ、行こうぜ」「#### 計画完了！」で硬軟が混在している。  \n　提案: 全体でラフなネット口語に統一するか、少しだけ丁寧に寄せるか方向性を決める。\n\n─────────────────────  \n3. 用語・技術語の一貫性  \n─────────────────────  \n◉ 3-1 “推論寄りのモデル”  \n　一般的には “reasoning-heavy model” → 「推論向き・推論重視モデル」等。 「寄り」は口語だが意味が曖昧。  \n　提案: 「推論特化モデル」「推論重視のモデル」へ修正。\n\n● 3-2 “Greenfield / Brownfield”  \n　Greenfield はカタカナで残し、Brownfield を「既存コード」だけにしたため対になる語が消えた。  \n　提案: 初出で「Brownfield（既存プロジェクト）」と並列表記し以後統一。\n\n─────────────────────  \n4. 追加情報・改変  \n─────────────────────  \n◎ 4-1 “失敗箇所まで自動でデバッグ” は原文にない挙動の追加。  \n　実際の動きを誤解させる可能性があるため削除 or 原文準拠に修正。\n\n─────────────────────  \n5. 文法・表記ゆれ・不自然表現  \n─────────────────────  \n● 5-1 「仕様を磨き」  \n　“hone in on an idea” は「アイデアを絞り込む」。Step1 見出しは後で「アイデアを磨く」と訳しているので 冒頭 tl;dr も揃える。  \n● 5-2 「ループを細かく区切って回し」  \n　やや直訳感。→「ループを小さく回し」程度で自然。  \n● 5-3 「ツール側で `todo.md` をチェックしながら進めると状態が保ちやすい。」  \n　主体が曖昧。→「コード生成ツールが `todo.md` を自動でチェックしてくれるので、 セッションをまたいでも状態を保ちやすい。」  \n△ 5-4 全体の漢字・かな混在  \n　例: 「リポジトリを用意」⇔「repo をセットアップ」。英語/カナ/漢字の揺れを整理。  \n\n─────────────────────  \n6. 機械翻訳的・硬さを感じる箇所  \n─────────────────────  \n● 6-1 「コンテキストを詰める」  \n　意味は通じるがやや不自然。「コードベースを LLM に効率よく流し込む」など。  \n● 6-2 「驚くほど楽ができる」  \n　カジュアル文体としては問題ないが、少し昭和感。→「めちゃくちゃ楽できる」にすると原文の軽さに近い。  \n● 6-3 長い英語原文の引用が続く部分で、前後の日本語が急に敬体になったり常体になったりする箇所が点在。 文末の統一が甘い。  \n\n─────────────────────  \n7. 推奨全体調整  \n─────────────────────  \n• 口語／砕けたネットスラング系で行くと決めたなら徹底し、 敬体混入 (“〜です。〜ます。”) を排除。逆も同様。  \n• 英語プロンプトは原文維持で OK。ただし直後のメタ説明は 「英語原文→日本語補足→英語プロンプト」に統一すると読みやすい。  \n• 絵文字・装飾 (“✩₊˚.⋆☾⋆⁺₊✧”) はなるべく原文位置を維持。冒頭では残したが、途中で抜けている箇所がある。  \n\n─────────────────────  \n修正例（冒頭 tl;dr のみ）  \n原訳:  \nまずブレインストーミングで仕様を磨き、次に計画を立て、最後に LLM のコード生成で実装。ループを細かく区切って回し、あとは魔法  \n\n改善案:  \nまずブレインストーミングで仕様を固め、次に “計画そのものを計画” し、最後に LLM でコード生成。 小さなループを回していき、仕上げは魔法 ✩₊˚.⋆☾⋆⁺₊✧  \n\n上記ポイントを修正すれば、原稿の遊び心と情報を漏らさない、 こなれた日本語訳に仕上がります。\n\nPlease address ALL issues identified in the critique and provide an improved translation.\nReturn ONLY the improved translated text without explanations or comments.",
      "response": "_tl;dr_  \nまずブレインストーミングで仕様を固め、次に “計画そのものを計画” し、それから LLM のコード生成で実装。小さなループを回していき、あとは魔法 ✩₊˚.⋆☾⋆⁺₊✧\n\n最近、LLM で小さなプロダクトを山ほど作ってる。楽しいし役立つけど、時間を溶かす落とし穴も多い。少し前に友人から「どうやって LLM でソフト書いてるの？」と聞かれ、「おいおい、どれだけ時間ある？」と思ったので、このポストを書くことにした。  \n（追伸：AI 嫌いの人は一番下までスクロールしてね）\n\n開発仲間と話すと細部は違えど、大筋は似たアプローチ。\n\nこれが俺のワークフロー。自分の経験、友人との会話（ありがとう [Nikete](https://www.nikete.com/)、[Kanno](https://nocruft.com/)、[Obra](https://fsck.com/)、[Kris](https://github.com/KristopherKubicki)、[Erik](https://thinks.lol/)）、そしてインターネットの “魔境” に転がってるベストプラクティスを全部ぶち込んでいる。\n\n今は **バッチリ動く** けど、2 週間後には動かなくなってるか、倍の威力で動いてるかも。¯\\\\\\_(ツ)\\_/¯\n\n## Let’s go\n\n{{< image src=\"llm-coding-robot.webp\" alt=\"Juggalo Robot\" caption=\"AI 生成画像はいつも怪しい。うちのジャガロ仕様コーディングロボ天使に挨拶して!\" >}}\n\n開発パターンは色々あるけど、俺の場合はほぼ 2 つ。\n\n- Greenfield（新規プロジェクト）  \n- Brownfield（既存プロジェクトのモダンレガシーコード）\n\n両方のやり方を紹介する。\n\n## Greenfield\n\n新規開発では、次のプロセスがかなり効く。ちゃんとした計画とドキュメントが手に入り、小さなステップで実装を進められる。\n\n{{< image src=\"greenfield.jpg\" alt=\"Green field\" caption=\"技術的には右側がグリーンフィールド。Leica Q, 2016/05/14\" >}}\n\n### Step 1: アイデアを絞り込む\n\nChatGPT-4o / o3 みたいな対話型 LLM でアイデアを磨く。\n\n```prompt\nAsk me one question at a time so we can develop a thorough, step-by-step spec for this idea. Each question should build on my previous answers, and our end goal is to have a detailed specification I can hand off to a developer. Let’s do this iteratively and dig into every relevant detail. Remember, only one question at a time.\n\nHere’s the idea:\n\n<IDEA>\n```\n\nブレストが自然に終わったら:\n\n```prompt\nNow that we’ve wrapped up the brainstorming process, can you compile our findings into a comprehensive, developer-ready specification? Include all relevant requirements, architecture choices, data handling details, error handling strategies, and a testing plan so a developer can immediately begin implementation.\n```\n\nこれでかなり solid な `spec.md` が手に入る。\n\n> 仕様書は万能。今回はコード生成に使うけど、別モデルに渡して穴を突かせたり、ホワイトペーパーやビジネスモデルを起こしたりもできる。深掘りさせれば 1 万字の裏付けドキュメントも返ってくる。\n\n### Step 2: 計画\n\n`spec.md` を推論重視のモデル（`o1*`, `o3*`, `r1` など）へ渡す。\n\n（TDD 版プロンプト）\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. Review the results and make sure that the steps are small enough to be implemented safely with strong testing, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.\n\nFrom here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step in a test-driven manner. Prioritize best practices, incremental progress, and early testing, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.\n\nMake sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.\n\n<SPEC>\n```\n\n（非 TDD 版プロンプト）\n\n```prompt\nDraft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. review the results and make sure that the steps are small enough to be implemented safely, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.\n\nFrom here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step. Prioritize best practices, and incremental progress, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.\n\nMake sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.\n\n<SPEC>\n```\n\nこれで `prompt_plan.md` が生成される。\n\nチェックリストも作っておく。\n\n```prompt\nCan you make a `todo.md` that I can use as a checklist? Be thorough.\n```\n\nコード生成ツールが `todo.md` を自動でチェックしてくれるので、セッションを跨いでも状態を保ちやすい。\n\n#### Yay. Plan!\n\nここまで **15 分** くらい。マジで速い。\n\n### Step 3: 実行\n\n成否は Step 2 の出来次第。  \n[github workspace](https://githubnext.com/projects/copilot-workspace)、[aider](https://aider.chat/)、[cursor](https://www.cursor.com/)、[claude engineer](https://github.com/Doriandarko/claude-engineer)、[sweep.dev](https://sweep.dev/)、[chatgpt](https://chatgpt.com)、[claude.ai](https://claude.ai)――どれでもうまくいった。\n\n俺は **生** Claude と aider 派。\n\n### Claude\n\n[claude.ai](https://claude.ai) とペアプロ。まず雛形とツール設定（`uv init`、`cargo init` など）だけ自分で用意し、言語やスタイルを固定しておくと Claude が勝手に React を吐く癖を抑えられる。\n\nワークフロー:\n\n- repo をセットアップ（雛形・ツール設定）\n- プロンプトを Claude に貼る\n- 出力コードを IDE にコピペ\n- 実行 & テスト\n- ...\n- 動いたら次のプロンプトへ\n- こけたら [repomix](https://github.com/yamadashy/repomix) でコードベースを丸ごと渡してデバッグ\n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n### Aider\n\n[Aider](https://aider.chat/) はクセ強だけど Step 2 のアウトプットと相性抜群。めちゃくちゃ楽できる。\n\n- repo をセットアップ\n- aider 起動\n- プロンプトを貼る\n- aider のダンスを眺める ♪┏(・o･)┛♪\n- aider がテストを走らせるか、自分でアプリを動かして確認\n- ...\n- 動いたら次へ\n- こけたら Q&A で修正\n- rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n> ちなみに aider の [LLM ランキング](https://aider.chat/docs/leaderboards/) は新モデルの実力を測るのに最高。\n\n### Results\n\nこの方法でスクリプト、Expo アプリ、Rust CLI など大量生産した。言語も環境も関係なし。\n\n寝かせてるプロジェクトがあるなら試してみ。短時間でビビるほど進むぞ。\n\n## Brownfield: 既存コードを小刻みに回す\n\n{{< image src=\"brownfield.jpg\" alt=\"a brown field\" caption=\"これはグリーンフィールドじゃない。祖父のカメラに残ってた 60 年代ウガンダの写真\" >}}\n\n既存コードではタスクごとに計画する。\n\n### コンテキストを LLM に流し込む\n\n俺は [repomix](https://github.com/yamadashy/repomix) と [mise](https://mise.jdx.dev/) の組み合わせ。\n\n```shell\nLLM:clean_bundles           repomix で LLM バンドル生成\nLLM:copy_buffer_bundle      output.txt をクリップボードへコピー\nLLM:generate_code_review    output.txt からコードレビュー生成\nLLM:generate_github_issues  output.txt から GitHub Issues 生成\nLLM:generate_issue_prompts  output.txt から Issue プロンプト生成\nLLM:generate_missing_tests  output.txt から足りないテスト抽出\nLLM:generate_readme         output.txt から README.md 生成\n```\n\n`output.txt` ができたら、例えば\n\n```shell\ncat output.txt | LLM -t readme-gen > README.md\ncat output.txt | LLM -m claude-3.5-sonnet -t code-review-gen > code-review.md\n```\n\nみたいにぶん回す。`LLM` コマンドがモデル切替やキー管理をやってくれて楽。\n\n> `mise` は `.mise.toml` でタスクを上書きできるから、コードベースに合わせて違うパッカーを使ったり ignore パターンを増やしたり自由自在。\n\n### 例: テスト補完\n\n#### Claude\n\n1. repo へ移動  \n2. `mise run LLM:generate_missing_tests`  \n3. `missing-tests.md` を確認  \n4. `mise run LLM:copy_buffer_bundle` で全文コピー  \n5. Claude に貼って最初の Issue を投げる  \n6. 出力コードを IDE に貼る  \n7. テスト実行  \n8. rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n#### Aider\n\n1. repo へ移動（新しいブランチ推奨）  \n2. aider 起動  \n3. `mise run LLM:generate_missing_tests`  \n4. `missing-tests.md` を確認  \n5. Issue を aider に貼る  \n6. aider のダンスを眺める ♪┏(・o･)┛♪  \n7. テスト実行  \n8. rinse repeat ✩₊˚.⋆☾⋆⁺₊✧\n\n小さな改修からデカいタスクまでこれでいける。\n\n### Prompt magic\n\n既存プロジェクトを堅牢化する即効薬。お気に入りのプロンプトをいくつか。\n\n#### Code review\n\n```prompt\nYou are a senior developer. Your job is to do a thorough code review of this code. You should write it up and output markdown. Include line numbers, and contextual info. Your code review will be passed to another teammate, so be thorough. Think deeply  before writing the code review. Review every part, and don't hallucinate.\n```\n\n#### GitHub Issue generation\n\n```prompt\nYou are a senior developer. Your job is to review this code, and write out the top issues that you see with the code. It could be bugs, design choices, or code cleanliness issues. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues will be given to a developer to executed on, so they should be in a format that is compatible with github issues\n```\n\n#### Missing tests\n\n```prompt\nYou are a senior developer. Your job is to review this code, and write out a list of missing test cases, and code tests that should exist. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues  will be given to a developer to executed on, so they should be in a format that is compatible with github issues\n```\n\nいわゆる “ブーマープロンプト” だから、いいアイデアあったら教えてくれ。\n\n## Skiing ᨒ↟ 𖠰ᨒ↟ 𖠰\n\nこのプロセスを説明するとき、いつも「油断すると **over my skis**（前のめり）になるから状況をガチで追え」と言ってる。気持ち良く滑ってたら突然「何が起きてる!?」って崖から落ちる、あの感じ。\n\n計画ドキュメントとテストは命綱。崖から落ちそうになったら散歩でもしてこい。\n\n> ときどき LLM に「lore ファイル作って UI で参照して」みたいな無茶振りをする。Python CLI ツールなのに突然ロアが現れたり UI がグリッチしたり。空が限界じゃない。\n\n## I am so lonely (｡•́︿•̀｡)\n\n長年ソロ開発、ペアプロ、チーム開発を全部やってきたが、仲間とやる方が絶対楽しい。このワークフローは基本ソロプレイ。ボットは衝突するしマージは地獄、コンテキストはカオス。\n\n誰か LLM コーディングをマルチプレイヤー化してくれ。頼む！\n\n## ⴵ Time ⴵ\n\nコード生成でアウトプットは激増。でも LLM がトークンを燃やしてる間の “待ち時間” も激増。\n\n{{< image src=\"apple-print-shop-printing.png\" alt=\"Printing\" caption=\"まるで昨日のことみたい\" >}}\n\nその間に\n\n- 次プロジェクトをブレスト  \n- レコードを聴く  \n- [Cookie Clicker](https://orteil.dashnet.org/cookieclicker/)  \n- 友達やロボと雑談  \n\nHack Hack Hack。こんなに生産的だったこと、マジでない。\n\n## Haterade ╭∩╮( •̀\\_•́ )╭∩╮\n\n「Fuck LLMs、全部クソ」って友達も多い。懐疑は大事。俺も電力消費と環境負荷は怖い。でも…コードは流れたい。はぁ。\n\n深入りせず様子見したい人には Ethan Mollick の  \n[**Co-Intelligence: Living and Working with AI**](https://www.penguinrandomhouse.com/books/741805/co-intelligence-by-ethan-mollick/) をおすすめ。テック脳筋本じゃなく、バランスよく利点を語ってくれる。読んだ友達ともめちゃくちゃ良い議論ができた。\n\n興味はあるけど半信半疑？ 連絡くれ。狂気を一緒に覗きつつ何か作ろうぜ。\n\n_thanks to [Derek](https://derek.broox.com), [Kanno](https://nocruft.com/), [Obra](https://fsck.com), and [Erik](https://thinks.lol/) for checking this post and suggesting edits. Appreciate it._",
      "usage": {
        "prompt_tokens": 12164,
        "completion_tokens": 3627,
        "total_tokens": 15791
      },
      "streaming": true
    },
    "frontmatter": {
      "model": "o3",
      "target_language": "japanese",
      "system_prompt": "You are a professional translator. Translate the following frontmatter fields to japanese.\nEach field is in the format \"field_name: content\". Translate ONLY the content, not the field names.\nReturn the translated content in the exact same format, preserving all field names.",
      "user_prompt": "title: My LLM codegen workflow atm\n\ndescription: A detailed walkthrough of my current workflow for using LLms to build software, from brainstorming through planning and execution.\n\n",
      "response": "title: 現在の私のLLMコード生成ワークフロー\ndescription: ソフトウェアを開発するためにLLMを活用する際のブレインストーミングから計画立案、実装に至るまで、現在の私のワークフローを詳細に解説する。",
      "usage": {
        "prompt_tokens": 86,
        "completion_tokens": 70,
        "total_tokens": 156
      },
      "fields": [
        "title",
        "description"
      ],
      "streaming": true
    },
    "all_critiques": [
      "【最重要 = Critical】\n\n1. まるごと欠落した段落\n   • 原文 “My hack to-do list is empty… …expand my programming perspective.” が訳文ではごっそり削除。  \n   • 修正案：段落を追加し、次のように訳出する  \n     例) 「ハック用の To-Do リストはもう空っぽです。映画を観ながらでも次々に思い付いたアイデアを片付けてしまうんです。ここ数年で初めて、新しい言語やツールにじっくり触れる時間が取れています。おかげでプログラマーとしての視野が大きく広がりました。」\n\n2. スキー比喩の詳細省略\n   • “beautiful smooth powder skiing … suddenly fall off a cliff” のユーモアがほぼ消失。  \n   • 修正案：  \n     「なめらかなパウダースノーを気持ちよく滑っていたのに、突然『え、何が起きてる!?』と崖から転げ落ちる――そんな感覚を指して “over my skis” と呼んでいます。」\n\n3. Aider のテスト自動デバッグ言及が欠落\n   • “aider will run the test suite and debug things for you” → 訳文では “テストが回る／自分で確認” と要約のみ。  \n   • 修正案：自動デバッグに触れて「テストスイートを実行して失敗箇所まで self-debug してくれるので、さらに放置プレイが可能」と補う。\n\n4. 「These quick hacks work super well …」段落の欠落  \n   • プロンプト・マジック冒頭が抜けている。  \n   • 修正案：短文を補完し「これらの即席ハックはプロジェクトの堅牢化に驚くほど効く」と訳出。\n\n【重大 = Major】\n\n5. タイトル・キャプションの “Juggalo” 誤表記\n   • 一般的表記は「ジャグァロ」または原語表記のまま。  \n   • 「ジャガロ」は誤読に見えるため修正推奨。\n\n6. “aggressive” の綴りミス\n   • 「agresive に把握しないと」→ “aggressive” あるいは日本語「アグレッシブに」。\n\n7. トーンの揺らぎ\n   • 原文の軽妙な皮肉・くだけた F ワードを時に無難化。  \n   • 例: “WHAT THE FUCK IS GOING ON!” → 無訳。  \n   • 方針に合わせ、伏字・カタカナで残すなどトーンを合わせる。\n\n8. 見出し “Iteration, incrementally” のニュアンス薄化\n   • 「既存コードをちょっとずつ改良」だけでは “iteration” の機械的/継続的側面が弱い。  \n   • 例: 「既存コードを小刻みに反復改善する」等で補正。\n\n9. 「Legacy modern code」の訳語\n   • “そこそこ整ったレガシー” は説明的過ぎ。  \n   • 「モダン寄りのレガシーコード」等で皮肉を残すとよい。\n\n10. 一部英語原文がそのまま混在\n    • “progress to agresive” など半端な混在は機械翻訳臭を強める。  \n    • すべて日本語に揃えるか、カタカナ英語で統一。\n\n【中程度 = Minor】\n\n11. 細かな語調・助詞の不自然さ\n    • 「さあ行こう」→ 口語なら「さあ、行こうぜ」など原文のカジュアル感を強めても良い。  \n    • 「今はうまく動いていますが」→ 「いまはバッチリ動いてますが」の方が軽妙。\n\n12. Markdown 表記ゆれ\n    • 原文 `_tl:dr;` に対し訳文 `_tl;dr_` ： セミコロン位置が変わり意味がぶれる。 → `_tl;dr;_` にするか日本語訳を付す。\n\n13. 箇条書きのインデント・中黒揺れ\n    • 英文の “–” を “—” にしたり、全角ハイフンと半角が混在。 → 統一。\n\n14. 技術用語の表記ゆれ\n    • 「Greenfield」「Non-greenfield」など一部カタカナ・一部英語。読者層に合わせ統一を検討（例: カタカナ＋括弧で英語）。\n\n15. 絵文字・顔文字の位置・数\n    • 原文と比べて絵文字が増減している箇所あり。原文のリズムを尊重するか不要なら削除。\n\n【修正文例（抜粋）】\n\n＞ 進捗を agresive に把握しないとすぐに **over my skis** 状態になります。  \n↓  \n進捗をアグレッシブに管理していないと、あっという間に **over my skis**――つまり自分の限界を超えて制御不能――になります。\n\n＞ AI 生成画像は怪しいよね。うちのジャガロ・コーディングロボ天使にご挨拶！  \n↓  \nAI 生成画像ってどこか胡散臭いよね。うちの “ジャグァロ” コーディングロボ天使にご挨拶！\n\n【総評】\n\n・大枠の構成は維持されており読みやすいが、原文の“軽快なオタク調スラング＋皮肉”が所々丸められ、いくつかの段落が完全に落ちています。  \n・誤表記・綴りミスと、英語残しの混在で機械訳感が出る箇所が散見。  \n・欠落段落を復元し、俗語・比喩を補完、表記揺れを統一すれば、原稿のノリを保った自然な日本語になるはずです。",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  \n厳 密 査 定 レ ポ ー ト  \n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  \n\n【凡例】  \nCritical ＝意図が変わる／情報欠落／明白な誤訳  \nMajor     ＝ニュアンス逸失／文化的背景・トーンのずれ  \nModerate ＝日本語として不自然・ぎこちない／語法ミス  \nMinor     ＝細かな表記ゆれ・句読点・可読性  \n\n──────────────────────────  \nA. 意図のずれ・欠落（Critical）  \n──────────────────────────  \n1. “All to manage your cloud functions, your todo list or whatever.”  \n   • 欠落。  \n   • 修正案：  \n     「――要はクラウド関数や ToDo リスト程度を管理するだけのツールなのに、です。」\n\n2. “context complicated”（Bots collide, merges are horrific, the context complicated.）  \n   • “コンテキストが複雑” が抜け落ちている。  \n   • 修正案：  \n     「ボット同士は衝突し、マージは地獄、そしてコンテキスト管理はカオス。」\n\n3. “taking a look at this post and suggesting edits” → “for feedback”  \n   • 行為の具体性が失われた。  \n   • 修正案：  \n     「この記事をチェックして校正提案までしてくれたことに感謝。」\n\n4. “Discrete loops.” → 「ループをこまめに区切ったら」  \n   • “離散的なループ構造で進める” という手法の核心がぼやけている。  \n   • 修正案例：  \n     「各ループを明確に切り分けながら進める。」  \n\n5. “Legacy modern code”  \n   • 原文は意図的な矛盾語。「モダン寄りのレガシーコード」だと意味が収束し過ぎ。  \n   • 修正案：  \n     「“最新め” だけど結局レガシーなコードベース」など皮肉を残す。\n\n──────────────────────────  \nB. ニュアンス・文化要素の損失／誤解（Major）  \n──────────────────────────  \n6. “Juggalo”  \n   • 単なる音写 “ジャグァロ” ではサブカル文脈（Insane Clown Posse ファン文化）が不明。  \n   • 修正案：  \n     「ジャグァロ（Insane Clown Posse の熱狂的ファン）」など脚注・補足を付ける。  \n\n7. “The sky is the limit.” →「空が限界です」  \n   • 直訳ぎみかつ意味逆転（limit＝限界ではなく“限界なし”）。  \n   • 修正案：  \n     「やれることに上限はない。」\n\n8. “raw claude” →「**生** Claude」  \n   • “生” は居酒屋的比喩で好みが分かれる＋意味が掴みにくい。  \n   • 修正案：  \n     「素の Claude」「プレーンな Claude」。\n\n9. “Boomer prompts”  \n   • 「“ブーマープロンプト”」とカッコだけで留め、原文の自虐ニュアンス不足。  \n   • 修正案：  \n     「昔ながらの“ブーマープロンプト”（時代遅れの大味な指示文）」.\n\n10. “over my skis”  \n    • 解説はあるが訳語なし。比喩を日本語に寄せると読みやすい。  \n    • 修正案：  \n      「勢い余って前のめり（＝over my skis）」\n\n──────────────────────────  \nC. 日本語としての自然さ・文体整合性（Moderate）  \n──────────────────────────  \n11. 「ワイルド tbh」「Hack Hack Hack」  \n    • 英語スラングが文中で浮く。カタカナ・和訳いずれかに統一。  \n\n12. 「プロンプトを貼る」  \n    • 一般に “貼り付ける” “コピペする” の方が自然。  \n\n13. 「ローア」  \n    • lore の表記統一は「ロア」またはカタカナ＋説明が自然。  \n\n14. 「ダンスを眺める ♪┏(・o･)┛♪」  \n    • 口語トーンとして面白いが全体の文体との落差が大きい。  \n    • 周囲の文語調に合わせ少しトーンダウンするか全体に散らす。\n\n15. 「空が限界です」「放置プレイ」など俗語の多用  \n    • オリジナルの軽快さは保ちつつ、過度な和製俗語は抑えると読みやすい。  \n\n──────────────────────────  \nD. 語彙・用語の統一 (Minor)  \n──────────────────────────  \n16. “spec” → 「仕様書」「スペック」が混在。  \n17. “repo” → 「リポジトリ」「レポ」が混在。  \n18. “prompt plan / prompt_plan.md” 表記ゆれ：プロンプトプラン/Prompt Plan 等。  \n\n──────────────────────────  \nE. 機械翻訳っぽい直訳箇所 (Minor)  \n──────────────────────────  \n19. 「リポジトリを用意（boilerplate、uv init、cargo init など）」  \n    • “boilerplate” は「雛形」と訳すか括弧補足。  \n\n20. 「逆に 2 倍速くなっているかもしれません」  \n    • “work twice as well” = 「2倍うまく動く」。“速く” だと速度限定に誤解。\n\n──────────────────────────  \nF. 書式・Markdown (Minor)  \n──────────────────────────  \n21. 原文の `_tl;dr` → `_tl:dr;` へ誤変換。  \n22. 英文引用部の ```prompt``` コードブロック内、全角三点リーダ入れ込みが散見（不要な全角スペース）。  \n\n──────────────────────────  \n改善後サンプル（一部）  \n──────────────────────────  \n_tl;dr_  \nまずブレインストーミングで仕様を固め、次にプランを作り、最後に LLM のコード生成で実装。各ループを明確に切り分けながら進め、あとは魔法 ✩₊˚.⋆☾⋆⁺₊✧  \n\n…  \n\n> AI 生成画像はどこか胡散臭い。Insane Clown Posse ファンを指す「ジャグァロ」仕様のコーディングロボ天使にどうぞご挨拶！\n\n…  \n\nPython CLI ツールなのに、突然“ロア（設定資料）”ファイルを作らせて UI で参照させたり――クラウド関数管理や ToDo リスト程度のアプリなのに、です。やりたい放題。  \n\n…  \n\nボット同士が衝突し、マージは地獄、そしてコンテキスト管理はカオス。LLM コーディングをマルチプレイヤー化する仕組みが切実に欲しい。誰か作って！  \n\n…  \n\n_thanks to … for taking a look at this post and suggesting edits. I appreciate it._  \n→  \n_この記事を読んで校正提案までしてくれた Derek、Kanno、Obra、Erik に感謝。_\n\n──────────────────────────  \n総 評  \n──────────────────────────  \n大枠の意味は保たれているが、①軽妙な皮肉や比喩の取りこぼし、②一部情報欠落、③文体・語調のふらつきが散見される。Critical と Major 部分の修正を優先し、続いて語彙統一と口語／文語バランスを整えれば、「原文由来のカジュアルさを保ちつつ日本語としてこなれた」訳文に仕上がる。",
      "【Severity: CRITICAL – Content Fidelity / Omission】\n\n1. Claude セクションの説明が欠落  \n   • 原文 “I am in charge of the initial boilerplate code … Claude has a tendency to just output react code …” が丸ごと抜けている。  \n   • 提案: 以下を追加し自然な日本語にする。  \n     「最初のボイラープレート作成とツール設定は自分で行います。こうして言語やスタイルを先に固定しておくと、Claude がつい React コードを吐きがちな癖を抑えられます。」\n\n2. Skiing 例え話の後半を省略  \n   • “beautiful smooth powder skiing … WHAT THE FUCK IS GOING ON! … fall off a cliff” が訳文に存在しない。  \n   • 提案:  \n     「まるでフカフカの新雪を気持ちよく滑っていたら、突然『えっ、何が起きてるんだ!?』と崖から落ちる──そんな感覚です。」\n\n3. “cookie clicker” 言及の一部欠落  \n   • Aider パートの “and I get to play cookie clicker” が「aider のダンス」だけに置き換えられた。  \n   • 提案: 「aider が勝手に進めてくれるので、その間は Cookie Clicker でも遊んでいられます。」を追記。\n\n4. 「何年もソロ→ペア→チームで開発してきた」一文が丸ごと抜けている。  \n   • 提案:  \n     「私はソロ開発も、ペアプロも、チーム開発も何年も経験してきましたが──やはり仲間とやる方が断然楽しいのです。」を挿入。\n\n5. 原文の “answer any sized tasks… ” など細部が所々飛んでいる  \n   • 全体を再比較し、小さな説明や例が欠けていないか網羅的に補完すること。\n\n【Severity: MAJOR – 誤訳 / 意味ずれ】\n\n1. “Legacy modern code” → “最新っぽいくせにレガシー”  \n   • 皮肉は伝わるが “modern‐ish legacy code” のニュアンス（近年の技術だが既にレガシー化）は薄い。  \n   • 提案: 「一応モダンだけど実質レガシーなコードベース」など。\n\n2. “Juggalo” のカタカナ表記 “ジャグァロ”  \n   • 通例は「ジャグァロ」ではなく「ジャガロ」or「ジャグガロ」。“ジャグァロ” は耳慣れず違和感。  \n   • 提案: 「ジャガロ（Juggalo）」とし、括弧で英語を補う。\n\n3. “over my skis” を訳語で説明しているが英語フレーズ併記が不足  \n   • 原文を意識した読者が混乱。  \n   • 提案: 初出は「前のめり（= over my skis）」と括弧内で英語を保持。\n\n【Severity: MAJOR – トーン / レジスターのブレ】\n\n1. です・ます調と砕けた口語が混在  \n   • 例: 「楽しいし実用的ですが…」「ダメなら Q&A で修正」  \n   • 原文はカジュアル一貫。日本語もタメ口 or です・ますのどちらかに統一した方が読みやすい。  \n   • 提案: 技術ブログらしくカジュアル体（だ・である）に統一。\n\n2. 原文の軽妙なスラングや罵声を弱め過ぎ  \n   • “WHAT THE FUCK IS GOING ON!?” → 未訳  \n   • “fuck LLMs” → 「全部ダメ」でマイルド化  \n   • 技術系読者向けなら多少の F ワード露出は注釈で残す方が忠実。  \n   • 提案: 「『クソッ、何が起きてるんだ!?』」など、伏字併用でニュアンス保持。\n\n【Severity: MODERATE – 自然さ / 文体】\n\n1. 「ここしばらく」より「最近」や「ここ数ヶ月」の方が自然。  \n2. 「ワイルドと言ってもいい」「超便利」など和製英語多用でやや古臭い印象。  \n3. 「ダメなら repomix でコードベースを Claude に渡してデバッグ」  \n   • 「渡してデバッグしてもらいます」など動詞を補完すると流れが滑らか。  \n\n【Severity: MODERATE – 一貫性 / 用語】\n\n1. Greenfield / Brownfield を英語のまま放置  \n   • ルビまたはカタカナ併記で読者に優しく。例: 「Greenfield（新規開発）」  \n2. `todo.md` 表記が “todo.md” と “todo.md`” でブレ。  \n3. “LLM” を「LLM」か「大規模言語モデル」に固定する。  \n\n【Severity: MINOR – 技術用語・細部】\n\n1. “raw claude” → 「素の Claude」より「プレーンな Claude」や「Claude 単体のチャット UI」などの方が明確。  \n2. “uv init” のようなコマンドはコードブロックに。  \n3. Markdown 強調（** **）と全角半角スペースの混在を校正。\n\n【Machine-like / Literal Issues】\n\n– 英語そのままのプロンプト引用は正当だが、前後に訳注がないためぶつ切れ感が出る。  \n  • 提案: 「以下は英語のまま貼る」等の一言メタ注を挿入。\n\n────────────────────────\n改善後のサンプル（一部）\n\n● Claude セクション追補例  \n> Claude でペアプロを進める前に、最小限のボイラープレートやツール設定は自分で済ませておきます。こうして開発言語やスタイルを最初に決めておくと、Claude が React コードを無闇に吐き出す癖を抑えられるのでオススメです。\n\n● Skiing 比喩追補例  \n> 例えるなら、気持ちよくパウダースノーを滑っていたのに突然「何が起きてるんだ⁉︎」と叫びながら崖から落ちる──そんなスピード感です。\n\n────────────────────────\n上記を反映させ、内容の欠落を補いトーンを整えれば、原文の勢いとユーモアを保ったまま“最初から日本語で書かれたような”訳文になります。",
      "◎ = Critical (意味の改変・大幅な欠落)  \n◉ = Major (誤訳・重要ニュアンス／用語のぶれ)  \n● = Minor (細部の訳文の不自然さ、語調のずれ)  \n△ = Style / Naturalness (文体・語感・トーン)  \n\n─────────────────────  \n1. 意味の取り違え・欠落  \n─────────────────────  \n◎ 1-1 “plan a plan” が消失  \n　原文は “Brainstorm spec, then plan a plan, then execute …” と二重に “plan” を繰り返し、 「計画自体をさらに計画する」というメタ的ニュアンスを強調している。訳文は  \n　「次に計画を立て」だけで、この遊びが失われている。  \n　提案: 「次に『計画を計画』し」や「計画そのものも設計し」といった訳で反映。\n\n◉ 1-2 コマンド例の一部欠落  \n　mise タスク説明中の  \n　`cat output.txt | LLM -m claude-3.5-sonnet -t code-review-gen > code-review.md`  \n　が訳文では丸ごと省かれ、「複数例の提示」という情報を失った。  \n　提案: 省略せず両方のコマンドを示す。\n\n◉ 1-3 Aider 手順: 追加・欠落混在  \n　原文 bullet  \n　• “aider will run tests, or you can run app to verify”  \n　訳文では「失敗箇所まで自動でデバッグ」と追加し、 “or you can run app …” を欠落。挙動を変えている。  \n　提案: 原文通り「テストが走るか、自分でアプリを動かして確認」程度に留める。\n\n● 1-4 “...” (省略記号) の削除  \n　手順リスト中の “...” は「中間に色々あるけど省略」のニュアンス。訳文は削除。影響は小。  \n\n─────────────────────  \n2. 文化的・語調ニュアンスのずれ  \n─────────────────────  \n◉ 2-1 “terrible internet bad places”  \n　原文は自嘲気味な二重否定ギャグ。訳文「ネットのあちこちのひどい … に転がっている」はやや説明調。  \n　提案: 「インターネットの“魔境”みたいな所に転がっているベストプラクティス」等、皮肉を強める。\n\n● 2-2 “¯\\_(ツ)_/¯” の直前  \n　原文 “it will probably not work in 2 weeks, or it will work twice as well.”  \n　訳文「死んでいるか倍速で動くかもしれない」はほぼ同義だが “倍速” はやや機械的。 「倍の威力で動いているかも」などでもっと口語に。\n\n△ 2-3 タイトル系の口調  \n　原文は “Let’s go” “Yay. Plan!” などテンション高め。訳文は  \n　「## さあ、行こうぜ」「#### 計画完了！」で硬軟が混在している。  \n　提案: 全体でラフなネット口語に統一するか、少しだけ丁寧に寄せるか方向性を決める。\n\n─────────────────────  \n3. 用語・技術語の一貫性  \n─────────────────────  \n◉ 3-1 “推論寄りのモデル”  \n　一般的には “reasoning-heavy model” → 「推論向き・推論重視モデル」等。 「寄り」は口語だが意味が曖昧。  \n　提案: 「推論特化モデル」「推論重視のモデル」へ修正。\n\n● 3-2 “Greenfield / Brownfield”  \n　Greenfield はカタカナで残し、Brownfield を「既存コード」だけにしたため対になる語が消えた。  \n　提案: 初出で「Brownfield（既存プロジェクト）」と並列表記し以後統一。\n\n─────────────────────  \n4. 追加情報・改変  \n─────────────────────  \n◎ 4-1 “失敗箇所まで自動でデバッグ” は原文にない挙動の追加。  \n　実際の動きを誤解させる可能性があるため削除 or 原文準拠に修正。\n\n─────────────────────  \n5. 文法・表記ゆれ・不自然表現  \n─────────────────────  \n● 5-1 「仕様を磨き」  \n　“hone in on an idea” は「アイデアを絞り込む」。Step1 見出しは後で「アイデアを磨く」と訳しているので 冒頭 tl;dr も揃える。  \n● 5-2 「ループを細かく区切って回し」  \n　やや直訳感。→「ループを小さく回し」程度で自然。  \n● 5-3 「ツール側で `todo.md` をチェックしながら進めると状態が保ちやすい。」  \n　主体が曖昧。→「コード生成ツールが `todo.md` を自動でチェックしてくれるので、 セッションをまたいでも状態を保ちやすい。」  \n△ 5-4 全体の漢字・かな混在  \n　例: 「リポジトリを用意」⇔「repo をセットアップ」。英語/カナ/漢字の揺れを整理。  \n\n─────────────────────  \n6. 機械翻訳的・硬さを感じる箇所  \n─────────────────────  \n● 6-1 「コンテキストを詰める」  \n　意味は通じるがやや不自然。「コードベースを LLM に効率よく流し込む」など。  \n● 6-2 「驚くほど楽ができる」  \n　カジュアル文体としては問題ないが、少し昭和感。→「めちゃくちゃ楽できる」にすると原文の軽さに近い。  \n● 6-3 長い英語原文の引用が続く部分で、前後の日本語が急に敬体になったり常体になったりする箇所が点在。 文末の統一が甘い。  \n\n─────────────────────  \n7. 推奨全体調整  \n─────────────────────  \n• 口語／砕けたネットスラング系で行くと決めたなら徹底し、 敬体混入 (“〜です。〜ます。”) を排除。逆も同様。  \n• 英語プロンプトは原文維持で OK。ただし直後のメタ説明は 「英語原文→日本語補足→英語プロンプト」に統一すると読みやすい。  \n• 絵文字・装飾 (“✩₊˚.⋆☾⋆⁺₊✧”) はなるべく原文位置を維持。冒頭では残したが、途中で抜けている箇所がある。  \n\n─────────────────────  \n修正例（冒頭 tl;dr のみ）  \n原訳:  \nまずブレインストーミングで仕様を磨き、次に計画を立て、最後に LLM のコード生成で実装。ループを細かく区切って回し、あとは魔法  \n\n改善案:  \nまずブレインストーミングで仕様を固め、次に “計画そのものを計画” し、最後に LLM でコード生成。 小さなループを回していき、仕上げは魔法 ✩₊˚.⋆☾⋆⁺₊✧  \n\n上記ポイントを修正すれば、原稿の遊び心と情報を漏らさない、 こなれた日本語訳に仕上がります。"
    ]
  },
  "critique_loop_details": {
    "critique_usages": [
      {
        "prompt_tokens": 8593,
        "completion_tokens": 1430,
        "total_tokens": 10023
      },
      {
        "prompt_tokens": 8906,
        "completion_tokens": 1838,
        "total_tokens": 10744
      },
      {
        "prompt_tokens": 9624,
        "completion_tokens": 1594,
        "total_tokens": 11218
      },
      {
        "prompt_tokens": 9858,
        "completion_tokens": 1820,
        "total_tokens": 11678
      }
    ],
    "feedback_usages": [
      {
        "prompt_tokens": 10378,
        "completion_tokens": 3177,
        "total_tokens": 13555
      },
      {
        "prompt_tokens": 11171,
        "completion_tokens": 3886,
        "total_tokens": 15057
      },
      {
        "prompt_tokens": 11537,
        "completion_tokens": 4079,
        "total_tokens": 15616
      },
      {
        "prompt_tokens": 12164,
        "completion_tokens": 3627,
        "total_tokens": 15791
      }
    ]
  },
  "timestamp": "2025-06-26T17:32:13.617273"
}