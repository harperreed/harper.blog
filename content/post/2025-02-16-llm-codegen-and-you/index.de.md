---
bsky: https://bsky.app/profile/harper.lol/post/3lidixzdr5j2e
date: 2025-02-16 18:00:00-05:00
description: Eine detaillierte Schritt-f√ºr-Schritt-Anleitung meines aktuellen Workflows
  f√ºr den Einsatz von LLMs zur Softwareentwicklung, vom Brainstorming √ºber die Planung
  bis zur Umsetzung.
draft: false
generateSocialImage: true
slug: my-llm-codegen-workflow-atm
tags:
- LLM
- coding
- ai
- workflow
- software-development
- productivity
title: Mein derzeitiger LLM-Codegen-Workflow
translationKey: My LLM codegen workflow atm
---

_tl;dr: Erst eine Spezifikation brainstormen, dann einen Plan schmieden und anschlie√üend in klar abgegrenzten Iterationen mit LLM-Codegen umsetzen. Danach passiert Magie. ‚ú©‚ÇäÀö.‚ãÜ‚òæ‚ãÜ‚Å∫‚Çä‚úß_

Ich habe in letzter Zeit wahnsinnig viele kleine Produkte mithilfe von LLMs gebaut. Das macht Spa√ü und ist n√ºtzlich. Allerdings gibt es Fallstricke, die enorm viel Zeit fressen k√∂nnen. Vor einer Weile fragte mich ein Freund, wie ich LLMs zum Programmieren nutze. Ich dachte: ‚ÄûOh boy, wie viel Zeit hast du?!‚Äú ‚Äì und so entstand dieser Beitrag.

(P.S. Wenn du zu den KI-Skeptiker*innen geh√∂rst ‚Äì einfach bis ganz nach unten scrollen.)

Ich spreche mit vielen Entwickler¬≠freund*innen dar√ºber, und wir alle verfolgen einen √§hnlichen Ansatz, nur mit kleinen Feinschliffen hier und da.

Hier ist mein Ablauf. Er basiert auf meiner eigenen Arbeit, Gespr√§chen mit Freund*innen (Danke [Nikete](https://www.nikete.com/), [Kanno](https://nocruft.com/), [Obra](https://fsck.com/), [Kris](https://github.com/KristopherKubicki) und [Erik](https://thinks.lol/)) sowie auf zahlreichen Best Practices aus den ber√ºchtigten Ecken des Internets ‚Äì den [bad](https://news.ycombinator.com/) [places](https://twitter.com).

Das funktioniert **JETZT** hervorragend ‚Äì in zwei Wochen vielleicht gar nicht mehr oder doppelt so gut. ¬Ø\\\_(„ÉÑ)\_/¬Ø

## Los geht‚Äôs

{{< image src="llm-coding-robot.webp" alt="Juggalo-Roboter" caption="Ich finde diese AI-Bilder immer ein bisschen suspekt. Sagt Hallo zu meinem Juggalo-Coding-Roboter-Engel!" >}}

Bei der Entwicklung gibt es f√ºr mich im Wesentlichen zwei Szenarien:

- Greenfield-Code  
- modernisierten Legacy-Code

Ich zeige dir meinen Prozess f√ºr beide Wege.

## Greenfield

F√ºr Greenfield-Entwicklung hat sich Folgendes bew√§hrt: ein robuster Planungs- und Dokumentationsansatz, der die Umsetzung in kleinen Schritten erleichtert.

{{< image src="greenfield.jpg" alt="Gr√ºnes Feld" caption="Rein technisch gesehen liegt rechts ein gr√ºnes Feld. Leica Q, 14.05.2016" >}}

### Schritt 1: Idee sch√§rfen

Nutze ein Konversations-LLM, um die Idee auszuarbeiten (ich verwende ChatGPT 4o/o3):

```prompt
Ask me one question at a time so we can develop a thorough, step-by-step spec for this idea. Each question should build on my previous answers, and our end goal is to have a detailed specification I can hand off to a developer. Let‚Äôs do this iteratively and dig into every relevant detail. Remember, only one question at a time.

Here‚Äôs the idea:

<IDEA>
```

Am Ende des Brainstormings (das Gespr√§ch kommt meist von selbst zu einem Abschluss):

```prompt
Now that we‚Äôve wrapped up the brainstorming process, can you compile our findings into a comprehensive, developer-ready specification? Include all relevant requirements, architecture choices, data handling details, error handling strategies, and a testing plan so a developer can immediately begin implementation.
```

Das Ergebnis ist eine solide, klar strukturierte Spezifikation, die du direkt als `spec.md` im Repo speichern kannst.

> Diese Spezifikation kannst du vielf√§ltig nutzen. Wir setzen sie hier f√ºrs Codegen ein, ich habe sie aber auch schon verwendet, um mithilfe eines logikorientierten Modells Schwachstellen aufzudecken (immer tiefer bohren!), ein Whitepaper zu verfassen oder ein Gesch√§ftsmodell abzuleiten. Stecke sie in ein Recherche-LLM und du erh√§ltst ein 10 000 W√∂rter starkes Begleitdokument.

### Schritt 2: Planung

Nimm die Spezifikation und f√ºttere damit ein ordentliches logikorientiertes Modell (`o1*`, `o3*`, `r1`):

_(TDD-Prompt)_

```prompt
Draft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. Review the results and make sure that the steps are small enough to be implemented safely with strong testing, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.

From here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step in a test-driven manner. Prioritize best practices, incremental progress, and early testing, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.

Make sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.

<SPEC>
```

_(Non-TDD-Prompt)_

```prompt
Draft a detailed, step-by-step blueprint for building this project. Then, once you have a solid plan, break it down into small, iterative chunks that build on each other. Look at these chunks and then go another round to break it into small steps. review the results and make sure that the steps are small enough to be implemented safely, but big enough to move the project forward. Iterate until you feel that the steps are right sized for this project.

From here you should have the foundation to provide a series of prompts for a code-generation LLM that will implement each step. Prioritize best practices, and incremental progress, ensuring no big jumps in complexity at any stage. Make sure that each prompt builds on the previous prompts, and ends with wiring things together. There should be no hanging or orphaned code that isn't integrated into a previous step.

Make sure and separate each prompt section. Use markdown. Each prompt should be tagged as text using code tags. The goal is to output prompts, but context, etc is important as well.

<SPEC>
```

Das Modell liefert einen Prompt-Fahrplan, den du mit Aider, Cursor & Co. abarbeiten kannst. Ich speichere ihn als `prompt_plan.md` im Repo.

Danach lasse ich mir eine `todo.md` erzeugen:

```prompt
Can you make a `todo.md` that I can use as a checklist? Be thorough.
```

Speichere sie als `todo.md` im Repo. Dein Codegen-Tool kann die Punkte w√§hrend der Arbeit abhaken ‚Äì das hilft, den √úberblick zu behalten.

#### Yay. Plan!

Jetzt hast du einen soliden Plan plus Doku, um dein Projekt umzusetzen. Das Ganze dauert vielleicht **15 Minuten** ‚Äì ziemlich verr√ºckt, ehrlich gesagt.

### Schritt 3: Umsetzung

Es gibt unz√§hlige Optionen; wie gut es l√§uft, h√§ngt stark von Schritt 2 ab.

Ich habe den Workflow mit [GitHub Workspace](https://githubnext.com/projects/copilot-workspace), [Aider](https://aider.chat/), [Cursor](https://www.cursor.com/), [Claude Engineer](https://github.com/Doriandarko/claude-engineer), [sweep.dev](https://sweep.dev/), [ChatGPT](https://chatgpt.com), [claude.ai](https://claude.ai) usw. genutzt. Klappt √ºberall ganz gut.

Meine Favoriten sind **reiner** Claude und Aider:

### Claude

Im Prinzip pair-programmiere ich mit [claude.ai](https://claude.ai) und reiche die Prompts nacheinander ein. Das Hin und Her nervt manchmal, funktioniert aber.

Ich k√ºmmere mich ums Grundger√ºst (Boilerplate, Tooling usw.), damit Claude nicht reflexhaft React-Code ausspuckt.

Wenn etwas h√§ngt, nehme ich [repomix](https://github.com/yamadashy/repomix) zur Hilfe (mehr dazu sp√§ter).

Workflow:

- Repo aufsetzen (`uv init`, `cargo init`, ‚Ä¶)  
- Prompt bei Claude einwerfen  
- Code von claude.ai kopieren und in der IDE speichern  
- Code ausf√ºhren, Tests laufen lassen  
- ‚Ä¶  
- l√§uft alles ‚Üí n√§chster Prompt  
- l√§uft es nicht ‚Üí repomix nutzen, um den Code an Claude zu schicken und zu debuggen  
- und immer so weiter ‚ú©‚ÇäÀö.‚ãÜ‚òæ‚ãÜ‚Å∫‚Çä‚úß

### Aider

[Aider](https://aider.chat/) ist spa√üig und etwas eigen. Passt super zu den Ergebnissen aus Schritt 2 ‚Äì mit wenig Aufwand kommst du sehr weit.

Tests sind mit Aider besonders angenehm, weil du dich nahezu ohne manuelles Eingreifen zur√ºcklehnen kannst: Aider f√ºhrt die Test-Suite aus und debuggt selbst.

Workflow:

- Repo aufsetzen  
- Aider starten  
- Prompt einf√ºgen  
- Aider beim Tanzen zusehen ‚ô™‚îè(„ÉªoÔΩ•)‚îõ‚ô™  
- Aider f√ºhrt Tests aus oder du startest die App  
- bei Erfolg ‚Üí n√§chster Prompt  
- bei Fehlern ‚Üí Q&A mit Aider, bis es passt  
- und immer so weiter ‚ú©‚ÇäÀö.‚ãÜ‚òæ‚ãÜ‚Å∫‚Çä‚úß

> Nebenbei: Aider betreibt hervorragendes Benchmarking neuer Modelle f√ºr Codegen in seinen [LLM-Leaderboards](https://aider.chat/docs/leaderboards/). Eine super Ressource, um zu sehen, wie effektiv neue Modelle sind.

### Ergebnisse

Ich habe mit diesem Workflow unz√§hlige Dinge gebaut: Skripte, Expo-Apps, Rust-CLI-Tools und mehr ‚Äì √ºber verschiedenste Sprachen und Kontexte hinweg.

Wenn du ein kleines oder gro√ües Projekt aufschiebst, probier‚Äôs aus ‚Äì du wirst staunen, wie schnell du vorankommst.

Meine Hack-To-Do-Liste ist leer, weil ich alles umgesetzt habe. Mir fallen st√§ndig neue Ideen ein, die ich dann mal eben nebenbei ‚Äì etwa beim Filmschauen ‚Äì umsetze. Zum ersten Mal seit Jahren befasse ich mich wieder mit neuen Sprachen und Tools. Das erweitert meinen Programmier-Horizont enorm.

## Non-Greenfield: Inkrementell iterieren

Manchmal ist kein Greenfield m√∂glich, sondern man muss auf einer bestehenden Codebasis weiterarbeiten.

{{< image src="brownfield.jpg" alt="Brownfield-Gel√§nde" caption="Das ist kein Greenfield. Ein Foto aus der Kamera meines Gro√üvaters ‚Äì irgendwo in Uganda in den 60ern" >}}

Hier nutze ich eine leicht andere Methode: Planung pro Aufgabe, nicht f√ºrs ganze Projekt.

### Kontext holen

Alle, die tief in AI-Dev stecken, haben ihr eigenes Tool daf√ºr ‚Äì du brauchst etwas, das deinen Quellcode z√ºgig ins LLM einspeist.

Ich verwende aktuell [repomix](https://github.com/yamadashy/repomix) √ºber eine Task-Sammlung in `~/.config/mise/config.toml`.

```shell
LLM:clean_bundles           Generate LLM bundle output file using repomix
LLM:copy_buffer_bundle      Copy generated LLM bundle from output.txt to system clipboard for external use
LLM:generate_code_review    Generate code review output from repository content stored in output.txt using LLM generation
LLM:generate_github_issues  Generate GitHub issues from repository content stored in output.txt using LLM generation
LLM:generate_issue_prompts  Generate issue prompts from repository content stored in output.txt using LLM generation
LLM:generate_missing_tests  Generate missing tests for code in repository content stored in output.txt using LLM generation
LLM:generate_readme         Generate README.md from repository content stored in output.txt using LLM generation
```

Ich erzeuge eine `output.txt`, die den Code-Kontext enth√§lt. Wenn sie zu gro√ü wird, passe ich die Ignore-Pattern an.

> Ein gro√üer Vorteil von `mise` ist, dass Tasks im `.mise.toml` des Arbeitsverzeichnisses √ºberschrieben werden k√∂nnen. Ich kann ein anderes Tool nutzen, um den Code zu packen ‚Äì solange es eine `output.txt` erzeugt, funktionieren meine LLM-Tasks. Das ist praktisch, wenn Codebasen sehr unterschiedlich sind.

Danach f√ºttere ich `output.txt` an den [LLM](https://github.com/simonw/LLM)-Befehl, um verschiedene Transformationen zu erzeugen und als Markdown abzulegen.

Im Kern l√§uft z. B. `cat output.txt | LLM -t readme-gen > README.md` oder  
`cat output.txt | LLM -m claude-3.5-sonnet -t code-review-gen > code-review.md`. Nicht kompliziert ‚Äì der `LLM`-Befehl √ºbernimmt die eigentliche Arbeit (Modelle, Keys, Prompt-Templates).

Beispiel-Workflow f√ºr fehlende Tests:

#### Claude

- Ins Projektverzeichnis wechseln  
- `mise run LLM:generate_missing_tests` ausf√ºhren  
- die erzeugte Datei `missing-tests.md` ansehen  
- kompletten Code-Kontext kopieren: `mise run LLM:copy_buffer_bundle`  
- Kontext + ersten Punkt ‚Äûfehlende Tests‚Äú in Claude einf√ºgen  
- generierten Code in der IDE √ºbernehmen  
- Tests ausf√ºhren  
- und immer so weiter ‚ú©‚ÇäÀö.‚ãÜ‚òæ‚ãÜ‚Å∫‚Çä‚úß

#### Aider

- Ins Projektverzeichnis wechseln (immer auf einem neuen Branch f√ºr Aider-Arbeit)  
- Aider starten  
- `mise run LLM:generate_missing_tests` ausf√ºhren  
- `missing-tests.md` ansehen  
- ersten Punkt ‚Äûfehlende Tests‚Äú in Aider einf√ºgen  
- Aider beim Tanzen zusehen ‚ô™‚îè(„ÉªoÔΩ•)‚îõ‚ô™  
- Tests ausf√ºhren  
- und immer so weiter ‚ú©‚ÇäÀö.‚ãÜ‚òæ‚ãÜ‚Å∫‚Çä‚úß

So l√§sst sich eine gro√üe Codebasis St√ºck f√ºr St√ºck verbessern.

### Prompt-Magie

Diese schnellen Hacks helfen, Projekte robuster zu machen. Hier ein paar meiner Prompts:

#### Code-Review

```prompt
You are a senior developer. Your job is to do a thorough code review of this code. You should write it up and output markdown. Include line numbers, and contextual info. Your code review will be passed to another teammate, so be thorough. Think deeply before writing the code review. Review every part, and don't hallucinate.
```

#### GitHub-Issues

```prompt
You are a senior developer. Your job is to review this code, and write out the top issues that you see with the code. It could be bugs, design choices, or code cleanliness issues. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues will be given to a developer to executed on, so they should be in a format that is compatible with github issues
```

#### Fehlende Tests

```prompt
You are a senior developer. Your job is to review this code, and write out a list of missing test cases, and code tests that should exist. You should be specific, and be very good. Do Not Hallucinate. Think quietly to yourself, then act - write the issues. The issues will be given to a developer to executed on, so they should be in a format that is compatible with github issues
```

Diese Prompts sind schon etwas ‚Äûold and busted‚Äú (Boomer-Prompts). Wenn du Ideen hast, wie man sie aufpolieren kann ‚Äì sag gern Bescheid.

## Skifahren ·®í‚Üü ñ†∞·®í‚Üü ñ†∞

Wenn ich den Prozess erkl√§re, sage ich oft: ‚ÄûMan muss aggressiv den √úberblick behalten, sonst ger√§t man schnell *over my skis* ‚Äì sprich: man √ºbernimmt sich.‚Äú

Warum auch immer: Ich sage st√§ndig *over my skis*, wenn es um LLMs geht. Vielleicht, weil alles butterweich l√§uft und man pl√∂tzlich denkt: ‚ÄûWTF geht hier ab?!‚Äú ‚Äì und schon st√ºrzt man die Klippe runter.

Ein **Planungsschritt** (siehe Greenfield) hilft enorm. Tests sind ebenfalls Gold wert ‚Äì besonders beim wilden Aider-Coding.

Trotzdem gerate ich oft *over my skis*. Eine kurze Pause oder ein Spaziergang wirken Wunder.

> Wir bitten das LLM h√§ufig, v√∂llig absurde Dinge in unseren eigentlich n√ºchternen Code einzubauen. Zum Beispiel haben wir es gebeten, eine Lore-Datei anzulegen und sie dann in der Benutzeroberfl√§che zu referenzieren ‚Äì f√ºr ein Python-CLI-Tool. Pl√∂tzlich gibt es Lore, glitchige Interfaces usw. Nur um deine Cloud-Functions oder deine To-Do-Liste zu verwalten. The sky is the limit.

## Ich bin so einsam (ÔΩ°‚Ä¢ÃÅÔ∏ø‚Ä¢ÃÄÔΩ°)

Mein gr√∂√üter Kritikpunkt: Das Ganze ist meist Single-Player-Modus.

Ich habe allein, im Pair und im Team gearbeitet ‚Äì mit Leuten ist es immer besser. Diese Workflows sind im Team schwer: Bots kollidieren, Merges sind h√§sslich, Kontext ist kompliziert.

Ich w√ºnsche mir dringend, dass jemand das in ein echtes Multiplayer-Game verwandelt. Riesige Chance ‚Äì legt los!

## ‚¥µ Zeit ‚¥µ

All dieses Codegen hat meine Output-Menge extrem gesteigert. Nebenwirkung: viel ‚ÄûDowntime‚Äú, w√§hrend das LLM Tokens verbrennt.

{{< image src="apple-print-shop-printing.png" alt="Druckvorgang" caption="Ich erinnere mich daran, als w√§re es gestern gewesen" >}}

Ich nutze die Wartezeit inzwischen so:

- Ich starte den Brainstorming-Prozess f√ºr ein anderes Projekt  
- Ich h√∂re Schallplatten  
- Ich spiele [Cookie Clicker](https://orteil.dashnet.org/cookieclicker/)  
- Ich quatsche mit Freund*innen und Robotern

So produktiv war ich beim Coden noch nie.

## Haterade ‚ï≠‚à©‚ïÆ( ‚Ä¢ÃÄ_‚Ä¢ÃÅ )‚ï≠‚à©‚ïÆ

Viele Freund*innen sagen: ‚ÄûFuck LLMs. Die k√∂nnen doch nix.‚Äú Skepsis ist okay. Gr√ºnde zum Ablehnen gibt es genug. Meine gr√∂√üte Sorge: Stromverbrauch und Umweltbelastung. Aber ‚Ä¶ der Code muss flie√üen. Right? *seufz*

Mein Rat lautet nicht, deine Meinung zu √§ndern, sondern lediglich Ethan Mollicks Buch [**Co-Intelligence: Living and Working with AI**](https://www.penguinrandomhouse.com/books/741805/co-intelligence-by-ethan-mollick/) zu lesen.

Es zeigt die Vorteile ohne Tech-Bro-Geschwafel. Hat schon viele gute, nuancierte Gespr√§che ausgel√∂st ‚Äì sehr empfehlenswert.

Bist du skeptisch, aber doch interessiert? Melde dich gern, vielleicht bauen wir zusammen was.

_Danke an [Derek](https://derek.broox.com), [Kanno](https://nocruft.com/), [Obra](https://fsck.com) und [Erik](https://thinks.lol/) f√ºrs Gegenlesen dieses Beitrags und die Vorschl√§ge. Ich wei√ü das zu sch√§tzen._