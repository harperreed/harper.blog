The translation of the Harper.Blog excerpt proceeded through a structured, multi-stage workflow designed to preserve formatting and deliver natural-sounding Japanese. First, an “o3” model received the original English text plus a detailed system prompt that insisted on preserving markdown and structure, skipping block quotes and code, and favoring fluid, idiomatic Japanese wherever possible. That initial pass produced a rough Japanese draft faithful to the source but still displaying signs of literalness and occasional nuance loss—especially around phrases conveying effort and temporal subtleties.

Next, an editing agent applied a second system prompt to that draft. This step focused on fluency and cultural appropriateness: correcting grammatical slips, smoothing awkward sentence rhythms, and ensuring that idioms felt native. The edited output showed clearer phrasing and better overall cohesion, though a few technical terms and some of the author’s original emphases still risked being over-simplified or under-represented.

To polish the result further, the team ran four critique loops. In the first loop, reviewers flagged “high-severity” issues such as missing the “time/effort” nuance in the phrase “I have recently spent some time,” and spots where the translation either trimmed or altered the original meaning. Subsequent loops deepened the analysis, with reviewers cataloging accuracy gaps, potential omissions, and stylistic mismatches. They called out, for example, cases where the Japanese sounded too formal compared to the casual blog voice, or where a colloquial turn of phrase needed a livelier equivalent.

Across these critique rounds, the translation was refined iteratively: nuanced temporal expressions were restored; technical labels were either accurately translated or explicitly left in English with quotation marks; and markdown headers, links, and other formatting markers remained untouched throughout. Each pass tightened fidelity to the source while boosting readability for Japanese speakers.

Overall, the workflow proved highly effective. Starting from a single automated pass, the process layered human-style edits and targeted critiques to remedy the initial model’s shortcomings. By the end of the fourth loop, the text not only mirrored the original’s structure and content but also captured its friendly, conversational tone. While the total token count (23,029) reflects a heavy resource investment, the four critique loops ensured a robust quality outcome—demonstrating that combining AI translation with systematic human review yields accurate, culturally tuned results without sacrificing the author’s intent.